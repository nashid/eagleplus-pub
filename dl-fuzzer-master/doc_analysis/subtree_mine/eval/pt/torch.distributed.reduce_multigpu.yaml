constraints:
  tensor_list:
    descp: Input and output GPU tensors of the collective. The function operates in-place.
      You also need to make sure that `len(tensor_list)` is the same for all the distributed
      processes calling this function.
    doc_dtype:
    - List[Tensor]
    structure:
    - list
    tensor_t:
    - torch.tensor
title: torch.distributed.reduce_multigpu
