constraints:
  '**kwargs':
    descp: ''
    normalized_descp: []
  beta_initializer:
    default: zeros
    descp: Initializer for the beta weight.
    doc_dtype: "str or Initializer, default \u2018zeros\u2019"
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the beta weight
    normalized_docdtype: D_TYPE or Initializer, default zeros
  center:
    default: 'True'
    descp: If True, add offset of beta to normalized tensor. If False, beta is ignored.
    doc_dtype: bool, default True
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL, add offset of beta to normalized D_STRUCTURE
    - If CONSTANT_BOOL, beta is ignored
    normalized_docdtype: D_TYPE, default CONSTANT_BOOL
  epsilon:
    default: 1e-05
    descp: Small float added to variance to avoid dividing by zero.
    doc_dtype: float, default 1e-5
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - Small D_TYPE added to variance to avoid dividing by zero
    normalized_docdtype: D_TYPE, default CONSTANT_FLOAT
  gamma_initializer:
    default: ones
    descp: Initializer for the gamma weight.
    doc_dtype: "str or Initializer, default \u2018ones\u2019"
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the gamma weight
    normalized_docdtype: D_TYPE or Initializer, default ones
  in_channels:
    default: '0'
    descp: Number of channels (feature maps) in input data. If not specified, initialization
      will be deferred to the first time forward is called and in_channels will be
      inferred from the shape of input data.
    doc_dtype: int, default 0
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - Number of channels BSTR in input data
    - If not specified, initialization will be deferred to the first time forward
      is called and in_channels will be inferred from the shape of input data
    normalized_docdtype: D_TYPE, default CONSTANT_NUM
  momentum:
    default: '0.9'
    descp: Momentum for the moving average.
    doc_dtype: float, default 0.9
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - Momentum for the moving average
    normalized_docdtype: D_TYPE, default CONSTANT_FLOAT
  num_devices:
    default: None
    descp: ''
    doc_dtype: int, default number of visible GPUs
    normalized_default: DEFAULT None
    normalized_descp: []
    normalized_docdtype: D_TYPE, default number of visible GPUs
  running_mean_initializer:
    default: zeros
    descp: Initializer for the running mean.
    doc_dtype: "str or Initializer, default \u2018zeros\u2019"
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the running mean
    normalized_docdtype: D_TYPE or Initializer, default zeros
  running_variance_initializer:
    default: ones
    descp: Initializer for the running variance.
    doc_dtype: "str or Initializer, default \u2018ones\u2019"
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the running variance
    normalized_docdtype: D_TYPE or Initializer, default ones
  scale:
    default: 'True'
    descp: If True, multiply by gamma. If False, gamma is not used. When the next
      layer is linear (also e.g. nn.relu), this can be disabled since the scaling
      will be done by the next layer.
    doc_dtype: bool, default True
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL, multiply by gamma
    - If CONSTANT_BOOL, gamma is not used
    - When the next layer is linear also e g
    - nn relu , this can be disabled since the scaling will be done by the next layer
    normalized_docdtype: D_TYPE, default CONSTANT_BOOL
  use_global_stats:
    default: 'False'
    descp: If True, use global moving statistics instead of local batch-norm. This
      will force change batch-norm into a scale shift operator. If False, use local
      batch-norm.
    doc_dtype: bool, default False
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL, use global moving statistics instead of local batch norm
    - This will force change batch norm into a PARAM shift operator
    - If CONSTANT_BOOL, use local batch norm
    normalized_docdtype: D_TYPE, default CONSTANT_BOOL
inputs:
  optional:
  - in_channels
  - num_devices
  - momentum
  - epsilon
  - center
  - scale
  - use_global_stats
  - beta_initializer
  - gamma_initializer
  - running_mean_initializer
  - running_variance_initializer
  required:
  - '**kwargs'
link: https://mxnet.apache.org/versions/1.6.0/api/python/docs/api/gluon/contrib/index.html#mxnet.gluon.contrib.nn.SyncBatchNorm
package: mxnet
target: SyncBatchNorm
title: mxnet.gluon.contrib.nn.SyncBatchNorm
version: 1.6.0
