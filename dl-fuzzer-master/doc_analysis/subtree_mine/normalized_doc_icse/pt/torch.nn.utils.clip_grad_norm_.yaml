constraints:
  max_norm:
    descp: max norm of the gradients
    doc_dtype: float or int
    normalized_descp:
    - max norm of the gradients
    normalized_docdtype: ONE_WORD D_TYPE
  norm_type:
    default: '2'
    descp: type of the used p-norm. Can be `'inf'` for infinity norm.
    doc_dtype: float or int
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - type of the used p norm
    - Can be QSTR for infinity norm
    normalized_docdtype: ONE_WORD D_TYPE
  parameters:
    descp: an iterable of Tensors or a single Tensor that will have gradients normalized
    doc_dtype: Iterable[Tensor] or Tensor
    normalized_descp:
    - an D_STRUCTURE of D_STRUCTURE or a single D_STRUCTURE that will have gradients
      normalized
    normalized_docdtype: D_STRUCTURE BSTR or D_STRUCTURE
inputs:
  optional:
  - norm_type
  required:
  - parameters
  - max_norm
link: https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_
package: torch
target: clip_grad_norm_
title: torch.nn.utils.clip_grad_norm_
version: 1.5.0
