{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from parse_utils import *\n",
    "from yaml_file_cls import yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parser:\n",
    "    def __init__(self, fname, content):\n",
    "        self.data = yaml_file(title=fname, api_name=fname.split('.')[-1], url='', package='scikit-learn', version='0.24.X')\n",
    "        self.content = content\n",
    "        self.fname = fname\n",
    "    \n",
    "    def parse_sig(self):\n",
    "        def process_sig(parsed_sig):\n",
    "            ret = {}\n",
    "            for key in parsed_sig:\n",
    "                if key=='' or key.isspace() or key=='...':\n",
    "                    continue\n",
    "                else:\n",
    "                    ret[key] = parsed_sig[key]\n",
    "            return ret\n",
    "        m = re.match('(.*?)\\s=\\s([\\w_.]+)\\((.*?)\\)$', self.content[0])\n",
    "        if m:\n",
    "            assert(fname == m.group(1))\n",
    "            assert(fname.split('.')[-1] == m.group(2))\n",
    "            sig = m.group(3)\n",
    "            parsed_sig = parse_input(sig)\n",
    "            parsed_sig = process_sig(parsed_sig)\n",
    "            if self.data.init_input(parsed_sig):\n",
    "                return True\n",
    "        \n",
    "        raise GoToTheNextOne('' , self.fname, '[Sig] Fail to parse signature', save=True)\n",
    "    \n",
    "    def get_sect(self, sect):\n",
    "        rule = r'\\n\\s*{}\\n\\s*---+\\n(.*?)(\\n\\s*(\\w+|See [aA]lso)\\n\\s*---+\\n|$)'.format(sect)\n",
    "        m = re.search(rule, ''.join(self.content), flags=re.DOTALL)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        raise GoToTheNextOne('' , self.fname, '[{}] Fail to parse {} section'.format(sect, sect), save=True)\n",
    "        \n",
    "    def descp_pre_process(self,descp):\n",
    "        return re.sub(r'\\.\\.\\s+version(changed|added)::.*?(\\n\\s+\\n|$)', '\\n \\n', descp, flags=re.DOTALL)\n",
    "    \n",
    "    def descp_post_process(self,descp):\n",
    "        return re.sub(r'\\s*\\n\\s*', ' ', descp.lstrip())\n",
    "    \n",
    "    def parse_descp(self, raw_descp):\n",
    "        ret = {}\n",
    "        raw_descp = self.descp_pre_process(raw_descp)\n",
    "#         print(raw_descp)\n",
    "        for a in re.split(r'\\n\\s+\\n', raw_descp):\n",
    "            if not a:\n",
    "                continue\n",
    "            m = re.match(r'^\\s*([\\w_\\*]+)\\s*:(.*?)$', a, flags=re.DOTALL)\n",
    "            if not m:\n",
    "                return self.parse_descp2(raw_descp)\n",
    "                \n",
    "            varname = m.group(1)\n",
    "            descp = self.descp_post_process(m.group(2))\n",
    "            ret[varname] = descp\n",
    "            \n",
    "        if not ret:\n",
    "            raise GoToTheNextOne('' , self.fname, '[Descp] Fail to parse descp section (empty return)', save=True)\n",
    "        \n",
    "        for arg in ret:\n",
    "            self.data.update_constraint(arg, ret[arg], allow_inconsistent_when_kwargs=False, ignore_star=False)\n",
    "        \n",
    "    def parse_descp2(self, raw_descp):\n",
    "        # match the descp by the args\n",
    "        # cannot detect inconsistencies, but can solve the itmes inside param descp\n",
    "        ret = {}\n",
    "        # raw_descp = descp_pre_process(raw_descp)\n",
    "        arg_list = list(self.data.data['constraints'].keys())\n",
    "        non_space_seg = []\n",
    "        for seg in re.split(r'(^|\\n\\s+\\n)\\s+({})\\s+:(.*?)'.format(get_bigrex(arg_list, boundary=True, escape=True)), raw_descp, flags=re.DOTALL):\n",
    "            if seg and not seg.isspace():\n",
    "                non_space_seg.append(seg)\n",
    "        assert(len(non_space_seg) == 2*len(arg_list))\n",
    "#         print(non_space_seg)\n",
    "        for i in range(0, len(non_space_seg), 2):\n",
    "            varname = non_space_seg[i]\n",
    "            descp = self.descp_post_process(non_space_seg[i+1])\n",
    "            ret[varname] = descp\n",
    "        if not ret:\n",
    "            raise GoToTheNextOne('' , self.fname, '[Descp] Fail to parse descp section (empty return)', save=True)\n",
    "#         for arg in ret:\n",
    "#             self.data.update_constraint(arg, ret[arg], allow_inconsistent_when_kwargs=False, ignore_star=False)\n",
    "        for arg in ret:\n",
    "            self.data.update_constraint(arg, ret[arg], allow_inconsistent_when_kwargs=False, ignore_star=False)\n",
    "        \n",
    "        \n",
    "    def parse(self, folder):\n",
    "        self.parse_sig()\n",
    "        p.parse_descp(p.get_sect('Parameters'))\n",
    "        self.data.save_file(folder, filename = self.fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/Users/danning/Desktop/deepflaw/exp2/code/dl-fuzzer/doc_analysis/collect_doc/scikitlearn/raw/'\n",
    "dst_path = '/Users/danning/Desktop/deepflaw/exp2/code/dl-fuzzer/doc_analysis/collect_doc/scikitlearn/parsed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "del_file(dst_path)\n",
    "for fname in get_file_list(src_path):\n",
    "    try:\n",
    "        p = parser(fname, content = read_file(os.path.join(src_path, fname)))\n",
    "        p.parse(dst_path)\n",
    "#         descp_dict = p.parse_descp(p.get_sect('Parameters'))\n",
    "#         prettyprint(descp_dict)\n",
    "#         print()\n",
    "    except GoToTheNextOne as gttno:\n",
    "        if gttno.save:\n",
    "            # continue\n",
    "            print(fname+': '+gttno.msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'X': 'array-like The data to fit. Can be for example a list, or an array.',\n",
      "  'cv': 'int, cross-validation generator or an iterable, optional Determines '\n",
      "        'the cross-validation splitting strategy. Possible inputs for cv are: '\n",
      "        '- None, to use the default 5-fold cross validation, - integer, to '\n",
      "        'specify the number of folds in a `(Stratified)KFold`, - :term:`CV '\n",
      "        'splitter`, - An iterable yielding (train, test) splits as arrays of '\n",
      "        'indices. For integer/None inputs, if the estimator is a classifier '\n",
      "        'and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is '\n",
      "        'used. In all other cases, :class:`KFold` is used. Refer :ref:`User '\n",
      "        'Guide <cross_validation>` for the various cross-validation strategies '\n",
      "        'that can be used here.',\n",
      "  'error_score': \"'raise' or numeric Value to assign to the score if an error \"\n",
      "                 \"occurs in estimator fitting. If set to 'raise', the error is \"\n",
      "                 'raised. If a numeric value is given, FitFailedWarning is '\n",
      "                 'raised. This parameter does not affect the refit step, which '\n",
      "                 'will always raise the error.',\n",
      "  'estimator': \"estimator object implementing 'fit' The object to use to fit \"\n",
      "               'the data.',\n",
      "  'fit_params': 'dict, optional Parameters to pass to the fit method of the '\n",
      "                'estimator.',\n",
      "  'groups': 'array-like, with shape (n_samples,), optional Group labels for '\n",
      "            'the samples used while splitting the dataset into train/test set. '\n",
      "            'Only used in conjunction with a \"Group\" :term:`cv` instance '\n",
      "            '(e.g., :class:`GroupKFold`).',\n",
      "  'n_jobs': 'int or None, optional (default=None) The number of CPUs to use to '\n",
      "            'do the computation. ``None`` means 1 unless in a '\n",
      "            ':obj:`joblib.parallel_backend` context. ``-1`` means using all '\n",
      "            'processors. See :term:`Glossary <n_jobs>` for more details.',\n",
      "  'pre_dispatch': 'int, or string, optional Controls the number of jobs that '\n",
      "                  'get dispatched during parallel execution. Reducing this '\n",
      "                  'number can be useful to avoid an explosion of memory '\n",
      "                  'consumption when more jobs get dispatched than CPUs can '\n",
      "                  'process. This parameter can be: - None, in which case all '\n",
      "                  'the jobs are immediately created and spawned. Use this for '\n",
      "                  'lightweight and fast-running jobs, to avoid delays due to '\n",
      "                  'on-demand spawning of the jobs - An int, giving the exact '\n",
      "                  'number of total jobs that are spawned - A string, giving an '\n",
      "                  \"expression as a function of n_jobs, as in '2*n_jobs'\",\n",
      "  'scoring': 'string, callable or None, optional, default: None A string (see '\n",
      "             'model evaluation documentation) or a scorer callable object / '\n",
      "             'function with signature ``scorer(estimator, X, y)`` which should '\n",
      "             'return only a single value. Similar to :func:`cross_validate` '\n",
      "             \"but only a single metric is permitted. If None, the estimator's \"\n",
      "             'default scorer (if available) is used.',\n",
      "  'verbose': 'integer, optional The verbosity level.',\n",
      "  'y': 'array-like, optional, default: None The target variable to try to '\n",
      "       'predict in the case of supervised learning.'}\n"
     ]
    }
   ],
   "source": [
    "fname = 'sklearn.model_selection.cross_val_score'\n",
    "p = parser(fname, content = read_file(os.path.join(src_path, fname)))\n",
    "# print(p.data.data)\n",
    "# p.parse(dst_path)\n",
    "p.parse_sig()\n",
    "s= p.get_sect('Parameters')\n",
    "prettyprint(p.parse_descp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrex(sep, boundary=True, escape=True):\n",
    "    if boundary:\n",
    "        s1 = r'\\b%s\\b'\n",
    "        s2 = r'\\b|\\b'\n",
    "    else:\n",
    "        s1 = r'%s'\n",
    "        s2 = r'|'\n",
    "\n",
    "\n",
    "    if escape:\n",
    "        return s1 % s2.join(map(re.escape, sep))\n",
    "    else:\n",
    "        return s1 % s2.join(sep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bigrex(args, boundary=True, escape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = re.split(r'(^|\\n\\s+\\n)\\s+({})\\s+:(.*?)'.format(get_bigrex(args, boundary=True, escape=True)), raw_descp, flags=re.DOTALL)\n",
    "cnt = 0\n",
    "for line in m:\n",
    "    if line and not line.isspace():\n",
    "        cnt+=1\n",
    "        print(line)\n",
    "        print('**********************')\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['estimator', 'X', 'y', 'groups', 'scoring', 'cv', 'n_jobs', 'verbose', 'fit_params', 'pre_dispatch', 'error_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in re.split(r'\\n\\s+\\n', s):\n",
    "    m = re.match(r'^\\s*([\\w_]+)\\s*:(.*?)$', a, flags=re.DOTALL)\n",
    "\n",
    "    varname = m.group(1)\n",
    "    descp = m.group(2)\n",
    "    descp = re.sub(r'\\s*\\n\\s*', ' ', descp.lstrip())\n",
    "    print(varname)\n",
    "    print(descp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\n\\s+\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'((\\n\\s+\\n|^)\\s*([\\w_]+)\\s*:(.*?))', s, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docterfuzz",
   "language": "python",
   "name": "docterfuzz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
