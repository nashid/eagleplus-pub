constraints:
  chunk_sizes:
    default: None
    descp: sizes of chunks to be placed on each device. It should match `devices`
      in length and sum to `tensor.size(dim)`. If not specified, the tensor will be
      divided into equal chunks.
    doc_dtype: Iterable[int], optional
    dtype:
    - int
    - numeric
    - string
    - torch.bool
    ndim:
    - '0'
    - '1'
    range:
    - '[0,1]'
    - '[0,inf)'
    shape:
    - '[&devices]'
    - '[&dim]'
    - '[1]'
    - '[c]'
    - '[int]'
    structure:
    - list
    tensor_t:
    - torch.tensor
  devices:
    descp: iterable of ints, specifying among which devices the tensor should be scattered.
    doc_dtype: Iterable[int]
    dtype:
    - int
    - numeric
    - string
    - torch.bool
    - torch.dtype
    ndim:
    - '0'
    - '1'
    - '3'
    range:
    - '[0,1]'
    - '[0,inf)'
    shape:
    - '[1]'
    - '[b,p,m]'
    - '[c]'
    - '[int]'
    structure:
    - list
    tensor_t:
    - torch.tensor
  dim:
    default: '0'
    descp: A dimension along which to chunk the tensor.
    doc_dtype: int, optional
    dtype:
    - int
    - numeric
    - torch.bool
    ndim:
    - '0'
    - '1'
    range:
    - '[0,1]'
    - '[0,inf)'
    tensor_t:
    - torch.tensor
  streams:
    default: None
    descp: ''
  tensor:
    descp: tensor to scatter.
    doc_dtype: Tensor
    dtype:
    - int
    - string
    ndim:
    - '1'
    range:
    - '[0,inf)'
    shape:
    - '[c]'
    tensor_t:
    - torch.tensor
inputs:
  optional:
  - chunk_sizes
  - dim
  - streams
  required:
  - tensor
  - devices
link: https://pytorch.org/docs/stable/cuda.html#torch.cuda.comm.scatter
package: torch
target: scatter
title: torch.cuda.comm.scatter
version: 1.5.0
