API,Param,ratio,pt1.5_parsed,pt1.6_parsed
torch.autograd.functional.hvp.yaml,create_graph,0.9975186104218362,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.","If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.  Defaults to `False`."
torch.allclose.yaml,equal_nan,0.9147286821705426,"if `True`, then two `NaN` s will be compared as equal. Default: `False`","if `True`, then two `NaN` s will be considered equal. Default: `False`"
torch.autograd.functional.vjp.yaml,v,0.9372549019607843,The vector for which the vector Jacobian product is computed. Must be the same size as the output of `func`. This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,The vector for which the vector Jacobian product is computed.  Must be the same size as the output of `func`. This argument is optional when the output of `func` contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.
torch.autograd.functional.vjp.yaml,create_graph,0.9975186104218362,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.","If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.  Defaults to `False`."
torch.autograd.functional.jacobian.yaml,create_graph,0.9973753280839895,"If `True`, the Jacobian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.","If `True`, the Jacobian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.  Defaults to `False`."
torch.autograd.functional.jvp.yaml,v,0.9425742574257425,The vector for which the Jacobian vector product is computed. Must be the same size as the input of `func`. This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,The vector for which the Jacobian vector product is computed. Must be the same size as the input of `func`. This argument is optional when the input to `func` contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.
torch.autograd.functional.jvp.yaml,create_graph,0.9975186104218362,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.","If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.  Defaults to `False`."
torch.load.yaml,f,0.9166666666666666,"a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string or os.PathLike object containing a file name"
