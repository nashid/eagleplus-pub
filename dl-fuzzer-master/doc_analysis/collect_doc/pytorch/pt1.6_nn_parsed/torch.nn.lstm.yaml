constraints:
  '**kwargs':
    descp: ''
  '*args':
    descp: ''
  batch_first:
    default: None
    descp: 'If `True`, then the input and output tensors are provided as (batch, seq,
      feature). Default: `False`'
  bias:
    default: None
    descp: 'If `False`, then the layer does not use bias weights b_ih and b_hh. Default:
      `True`'
  bidirectional:
    default: None
    descp: 'If `True`, becomes a bidirectional LSTM. Default: `False`'
  dropout:
    default: None
    descp: 'If non-zero, introduces a Dropout layer on the outputs of each LSTM layer
      except the last layer, with dropout probability equal to `dropout`. Default:
      0'
  hidden_size:
    default: None
    descp: The number of features in the hidden state h
  input_size:
    default: None
    descp: The number of expected features in the input x
  num_layers:
    default: None
    descp: 'Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking
      two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs
      of the first LSTM and computing the final results. Default: 1'
inputs:
  optional:
  - input_size
  - hidden_size
  - num_layers
  - bias
  - batch_first
  - dropout
  - bidirectional
  required:
  - '*args'
  - '**kwargs'
link: https://pytorch.org/docs/1.6.0/generated/torch.nn.LSTM.html#torch.nn.LSTM#torch.nn.LSTM
package: torch
target: LSTM
title: torch.nn.LSTM
version: 1.6.0
