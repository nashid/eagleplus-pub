constraints:
  allow_unused:
    default: 'False'
    descp: If `False`, specifying inputs that were not used when computing outputs
      (and therefore their grad is always zero) is an error. Defaults to `False`.
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL specifying PARAM that were not used when computing PARAM BSTR
      is an error
    - Defaults to CONSTANT_BOOL
    normalized_docdtype: D_TYPE optional
  create_graph:
    default: 'False'
    descp: 'If `True`, graph of the derivative will be constructed, allowing to compute
      higher order derivative products. Default: `False`.'
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL graph of the derivative will be constructed allowing to compute
      higher order derivative products
    - Default CONSTANT_BOOL
    normalized_docdtype: D_TYPE optional
  grad_outputs:
    default: None
    descp: 'The "vector" in the Jacobian-vector product. Usually gradients w.r.t.
      each output. None values can be specified for scalar Tensors or ones that don''t
      require grad. If a None value would be acceptable for all grad_tensors, then
      this argument is optional. Default: None.'
    doc_dtype: sequence of Tensor
    normalized_default: DEFAULT None
    normalized_descp:
    - The QSTR in the Jacobian vector product
    - Usually gradients w r t
    - each output
    - None values can be specified for scalar D_STRUCTURE or ones that don t require
      grad
    - If a None value would be acceptable for all grad_tensors then this argument
      is optional
    - Default None
    normalized_docdtype: D_STRUCTURE of D_STRUCTURE
  inputs:
    descp: Inputs w.r.t. which the gradient will be returned (and not accumulated
      into `.grad`).
    doc_dtype: sequence of Tensor
    normalized_descp:
    - Inputs w r t
    - which the gradient will be returned and not accumulated into grad
    normalized_docdtype: D_STRUCTURE of D_STRUCTURE
  only_inputs:
    default: 'True'
    descp: ''
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp: []
  outputs:
    descp: outputs of the differentiated function.
    doc_dtype: sequence of Tensor
    normalized_descp:
    - outputs of the differentiated function
    normalized_docdtype: D_STRUCTURE of D_STRUCTURE
  retain_graph:
    default: None
    descp: If `False`, the graph used to compute the grad will be freed. Note that
      in nearly all cases setting this option to `True` is not needed and often can
      be worked around in a much more efficient way. Defaults to the value of `create_graph`.
    doc_dtype: bool, optional
    normalized_default: DEFAULT None
    normalized_descp:
    - If CONSTANT_BOOL the graph used to compute the grad will be freed
    - Note that in nearly all cases setting this option to CONSTANT_BOOL is not needed
      and often can be worked around in a much more efficient way
    - Defaults to the value of PARAM
    normalized_docdtype: D_TYPE optional
inputs:
  optional:
  - grad_outputs
  - retain_graph
  - create_graph
  - only_inputs
  - allow_unused
  required:
  - outputs
  - inputs
link: https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad
package: torch
target: grad
title: torch.autograd.grad
version: 1.5.0
