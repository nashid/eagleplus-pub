constraints:
  X: {descp: '{ndarray, sparse matrix, LinearOperator} of shape         (n_samples,
      n_features) Training data'}
  alpha: {descp: 'float or array-like of shape (n_targets,) Regularization strength;
      must be a positive float. Regularization improves the conditioning of the problem
      and reduces the variance of the estimates. Larger values specify stronger regularization.
      Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression
      or LinearSVC. If an array is passed, penalties are assumed to be specific to
      the targets. Hence they must correspond in number.'}
  check_input: {default: 'True', descp: 'bool, default=True If False, the input arrays
      X and y will not be checked. '}
  max_iter: {default: None, descp: 'int, default=None Maximum number of iterations
      for conjugate gradient solver. For the ''sparse_cg'' and ''lsqr'' solvers, the
      default value is determined by scipy.sparse.linalg. For ''sag'' and saga solver,
      the default value is 1000.'}
  random_state: {default: None, descp: 'int, RandomState instance, default=None The
      seed of the pseudo random number generator to use when shuffling the data.  If
      int, random_state is the seed used by the random number generator; If RandomState
      instance, random_state is the random number generator; If None, the random number
      generator is the RandomState instance used by `np.random`. Used when ``solver``
      == ''sag''.'}
  return_intercept: {default: 'False', descp: 'bool, default=False If True and if
      X is sparse, the method also returns the intercept, and the solver is automatically
      changed to ''sag''. This is only a temporary fix for fitting the intercept with
      sparse data. For dense data, use sklearn.linear_model._preprocess_data before
      your regression.'}
  return_n_iter: {default: 'False', descp: 'bool, default=False If True, the method
      also returns `n_iter`, the actual number of iteration performed by the solver.'}
  sample_weight: {default: None, descp: 'float or array-like of shape (n_samples,),
      default=None Individual weights for each sample. If given a float, every sample
      will have the same weight. If sample_weight is not None and solver=''auto'',
      the solver will be set to ''cholesky''.'}
  solver: {default: auto, descp: '{''auto'', ''svd'', ''cholesky'', ''lsqr'', ''sparse_cg'',
      ''sag'', ''saga''},         default=''auto'' Solver to use in the computational
      routines: - ''auto'' chooses the solver automatically based on the type of data.
      - ''svd'' uses a Singular Value Decomposition of X to compute the Ridge coefficients.
      More stable for singular matrices than ''cholesky''. - ''cholesky'' uses the
      standard scipy.linalg.solve function to obtain a closed-form solution via a
      Cholesky decomposition of dot(X.T, X) - ''sparse_cg'' uses the conjugate gradient
      solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver
      is more appropriate than ''cholesky'' for large-scale data (possibility to set
      `tol` and `max_iter`). - ''lsqr'' uses the dedicated regularized least-squares
      routine scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative procedure.
      - ''sag'' uses a Stochastic Average Gradient descent, and ''saga'' uses its
      improved, unbiased version named SAGA. Both methods also use an iterative procedure,
      and are often faster than other solvers when both n_samples and n_features are
      large. Note that ''sag'' and ''saga'' fast convergence is only guaranteed on
      features with approximately the same scale. You can preprocess the data with
      a scaler from sklearn.preprocessing. All last five solvers support both dense
      and sparse data. However, only ''sag'' and ''sparse_cg'' supports sparse input
      when`fit_intercept` is True.'}
  tol: {default: '0.001', descp: 'float, default=1e-3 Precision of the solution.'}
  verbose: {default: '0', descp: 'int, default=0 Verbosity level. Setting verbose
      > 0 will display additional information depending on the solver used.'}
  y: {descp: 'ndarray of shape (n_samples,) or (n_samples, n_targets) Target values'}
inputs:
  optional: [sample_weight, solver, max_iter, tol, verbose, random_state, return_n_iter,
    return_intercept, check_input]
  required: [X, y, alpha]
link: ''
package: sklearn
target: ridge_regression
title: sklearn.linear_model.ridge_regression
version: 0.24.2
