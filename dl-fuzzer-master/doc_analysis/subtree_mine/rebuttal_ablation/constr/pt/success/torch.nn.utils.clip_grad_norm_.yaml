constraints:
  max_norm:
    descp: max norm of the gradients
    doc_dtype: float or int
    dtype:
    - int
    - torch.float32
  norm_type:
    default: '2'
    descp: type of the used p-norm. Can be `'inf'` for infinity norm.
    doc_dtype: float or int
    dtype:
    - int
    - torch.bool
    - torch.dtype
    - torch.float32
    ndim:
    - '0'
    range:
    - '[0,inf)'
  parameters:
    descp: an iterable of Tensors or a single Tensor that will have gradients normalized
    doc_dtype: Iterable[Tensor] or Tensor
    dtype:
    - int
    - numeric
    - string
    - torch.bool
    - torch.dtype
    ndim:
    - '0'
    - '1'
    - '3'
    range:
    - '[0,1]'
    - '[0,inf)'
    shape:
    - '[1]'
    - '[b,p,m]'
    - '[c]'
    - '[iterable]'
    - '[tensor]'
    structure:
    - list
    tensor_t:
    - torch.tensor
inputs:
  optional:
  - norm_type
  required:
  - parameters
  - max_norm
link: https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_
package: torch
target: clip_grad_norm_
title: torch.nn.utils.clip_grad_norm_
version: 1.5.0
