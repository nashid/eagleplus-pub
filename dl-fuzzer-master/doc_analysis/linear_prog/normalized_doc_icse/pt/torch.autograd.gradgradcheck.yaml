constraints:
  atol:
    default: 1e-05
    descp: absolute tolerance
    doc_dtype: float, optional
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - absolute tolerance
    normalized_docdtype: D_TYPE optional
  eps:
    default: 1e-06
    descp: perturbation for finite differences
    doc_dtype: float, optional
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - perturbation for finite differences
    normalized_docdtype: D_TYPE optional
  func:
    descp: a Python function that takes Tensor inputs and returns a Tensor or a tuple
      of Tensors
    doc_dtype: function
    normalized_descp:
    - a Python function that takes D_STRUCTURE PARAM and returns a D_STRUCTURE of
      D_STRUCTURE
    normalized_docdtype: ONE_WORD function
  gen_non_contig_grad_outputs:
    default: 'False'
    descp: if `grad_outputs` is `None` and `gen_non_contig_grad_outputs` is `True`,
      the randomly generated gradient outputs are made to be noncontiguous
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if PARAM is QSTR is CONSTANT_BOOL the randomly generated gradient outputs are
      made to be noncontiguous
    normalized_docdtype: D_TYPE optional
  grad_outputs:
    default: None
    descp: The gradients with respect to the function's outputs.
    doc_dtype: tuple of Tensor or Tensor, optional
    normalized_default: DEFAULT None
    normalized_descp:
    - The gradients with respect to the function outputs
    normalized_docdtype: D_STRUCTURE of D_STRUCTURE optional
  inputs:
    descp: inputs to the function
    doc_dtype: tuple of Tensor or Tensor
    normalized_descp:
    - inputs to the function
    normalized_docdtype: D_STRUCTURE of D_STRUCTURE
  nondet_tol:
    default: '0.0'
    descp: tolerance for non-determinism. When running identical inputs through the
      differentiation, the results must either match exactly (default, 0.0) or be
      within this tolerance. Note that a small amount of nondeterminism in the gradient
      will lead to larger inaccuracies in the second derivative.
    doc_dtype: float, optional
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - tolerance for non determinism
    - When running identical PARAM through the differentiation the results must either
      match exactly BSTR or be within this tolerance
    - Note that a small amount of nondeterminism in the gradient will lead to larger
      inaccuracies in the second derivative
    normalized_docdtype: D_TYPE optional
  raise_exception:
    default: 'True'
    descp: indicating whether to raise an exception if the check fails. The exception
      gives more information about the exact nature of the failure. This is helpful
      when debugging gradchecks.
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - indicating whether to raise an exception if the check fails
    - The exception gives more information about the exact nature of the failure
    - This is helpful when debugging gradchecks
    normalized_docdtype: D_TYPE optional
  rtol:
    default: '0.001'
    descp: relative tolerance
    doc_dtype: float, optional
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - relative tolerance
    normalized_docdtype: D_TYPE optional
inputs:
  optional:
  - grad_outputs
  - eps
  - atol
  - rtol
  - gen_non_contig_grad_outputs
  - raise_exception
  - nondet_tol
  required:
  - func
  - inputs
link: https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck
package: torch
target: gradgradcheck
title: torch.autograd.gradgradcheck
version: 1.5.0
