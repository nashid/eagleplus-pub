,API,Arg,Descp,Normalized_descp,dtype,shape,ndim,range,enum,structure
0,torch.onnx.export,f,a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file.,A binary Protobuf will be written to this file,,,,,,
1,torch.nn.init.eye_,tensor,a 2-dimensional torch.Tensor,a CONSTANT_NUM dimensional D_STRUCTURE,,,CONSTANT_NUM,,,D_STRUCTURE
2,torch.rot90,dims,DD: a list or tuple,a D_STRUCTURE,,,,,,D_STRUCTURE
3,torch.chain_matmul,*matrices,a sequence of 2 or more 2-D tensors whose product is to be determined.,a D_STRUCTURE of CONSTANT_NUM or more CONSTANT_NUM D D_STRUCTURE whose product is to be determined,,,1,,,D_STRUCTURE
4,torch.zeros,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
5,torch.rand,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
6,torch.utils.cpp_extension.load_inline,functions,"A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",A D_STRUCTURE of function names for which to generate function bindings,string,,,,,D_STRUCTURE
7,torch.quantization.convert,mapping,"a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",a D_STRUCTURE that maps from D_TYPE PARAM type to quantized PARAM type can be overwrritten to allow swapping user defined Modules,,,,,,D_STRUCTURE
8,torch.pinverse,rcond,A floating point value to determine the cutoff for small singular values. Default: 1e-15,A D_TYPE value to determine the cutoff for small singular values,D_TYPE,,0,,,
9,torch.nn.SyncBatchNorm,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module tracks the running mean and variance and when set to CONSTANT_BOOL this module does not track such statistics and always uses batch statistics in both training and eval modes,D_TYPE,,0,,,
10,torch.nn.BatchNorm2d,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module tracks the running mean and variance and when set to CONSTANT_BOOL this module does not track such statistics and always uses batch statistics in both training and eval modes,D_TYPE,,0,,,
11,torch.nn.Softmax,dim,A dimension along which Softmax will be computed (so every slice along dim will sum to 1).,A dimension along which Softmax will be computed BSTR,int,,,,,
12,torch.onnx.export,f,a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file.,a file like object BSTR or a D_TYPE containing a file name,,,,,,
13,torch.jit.save,f,A file-like object (has to implement write and flush) or a string containing a file name.,A file like object BSTR or a D_TYPE containing a file name,,,,,,
14,torch.load,f,"a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",a file like object has to implement read meth QSTR meth QSTR and meth QSTR or a D_TYPE containing a file name,,,,,,
15,torch.quantization.quantize_qat,run_fn,"a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",a function for evaluating the prepared PARAM can be a function that simply runs the prepared PARAM or a training loop,,,,,,
16,torch.load,map_location,"a function, `torch.device`, string or a dict specifying how to remap storage locations",a function torch device D_TYPE or a D_STRUCTURE specifying how to remap storage locations,,,,,,
17,torch.nn.MultiMarginLoss,weight,"a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.",a manual rescaling weight given to each class,numeric,,,,,
18,torch.nn.BCEWithLogitsLoss,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,a PARAM of positive examples,,,,,,
19,torch.nn.MaxPool1d,dilation,a parameter that controls the stride of elements in the window,a parameter that controls the PARAM of elements in the window,,,,,,
20,torch.nn.Fold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,a parameter that controls the PARAM of elements within the neighborhood,,,,,,
21,torch.poisson,generator,a pseudorandom number generator for sampling,a pseudorandom number generator for sampling,,,,,,
22,torch.autograd.functional.vjp,func,a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,a Python function that takes D_STRUCTURE PARAM and returns a D_STRUCTURE of D_STRUCTURE,,,,,,
23,torch.autograd.functional.hvp,func,a Python function that takes Tensor inputs and returns a Tensor with a single element.,a Python function that takes D_STRUCTURE PARAM and returns a D_STRUCTURE with a single element,,,,,,
24,torch.jit.load,map_location,A simplified version of `map_location` in `torch.save` used to dynamically remap storages to an alternative set of devices.,A simplified version of QSTR in torch save used to dynamically remap storages to an alternative set of devices,,,,,,
25,torch.nn.LPPool1d,kernel_size,"a single int, the size of the window",a single D_TYPE the size of the window,D_TYPE;int,,0,"[0,inf)",,
26,torch.sparse.addmm,mat1,a sparse matrix to be multiplied,a sparse matrix to be multiplied,numeric,,,,,
27,torch.nn.LayerNorm,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,numeric,,,,,
28,torch.nn.InstanceNorm1d,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,numeric,,,,,
29,torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",additional size added to one side of each dimension in the output shape,int,,,"[0,inf)",,
30,torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",additional size added to one side of each dimension in the output shape,int,,,"[0,inf)",,
31,torch.nn.ConvTranspose2d,output_padding,Additional size added to one side of each dimension in the output shape. Default: 0,Additional size added to one side of each dimension in the output shape,int,,,"[0,inf)",,
32,torch.cuda.comm.reduce_add,inputs,an iterable of tensors to add.,an D_STRUCTURE of D_STRUCTURE to add,,,,,,D_STRUCTURE
33,torch.nn.init.zeros_,tensor,an n-dimensional torch.Tensor,an n dimensional D_STRUCTURE,,,,,,D_STRUCTURE
34,torch.cartesian_prod,*tensors,any number of 1 dimensional tensors.,any number of CONSTANT_NUM dimensional D_STRUCTURE,int,,,"[0,inf)",,D_STRUCTURE
35,torch.rot90,dims,axis to rotate,axis to rotate,int,,,,,
36,torch.nn.utils.prune.custom_from_mask,mask,binary mask to be applied to the parameter.,binary mask to be applied to the parameter,,,,,,
37,torch.load,**pickle_load_args,"(Python 3 only) optional keyword arguments passed over to `pickle_module.load()` and `pickle_module.Unpickler()`, e.g., `errors=...`.",BSTR and PARAM Unpickler e g errors,,,,,,
38,torch.nn.functional.affine_grid,size,"the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",BSTR Example torch Size BSTR,,,,,,
39,torch.nn.functional.nll_loss,input,"(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",BSTR where C number of classes or BSTR where K REXPR in the case of K dimensional loss,,BSTR,,,,
40,torch.nn.functional.cross_entropy,target,"(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",BSTR where each value is CONSTANT_NUM REXPR BSTR REXPR CONSTANT_NUM or BSTR where K REXPR for K dimensional loss,,BSTR,,,,D_STRUCTURE
41,torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",By default fork_rng operates on all devices but will emit a warning if your machine has a lot of devices since this function will run very slowly in that case,,,,,,D_STRUCTURE
42,torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
43,torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
44,torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
45,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
46,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
47,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
48,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
49,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
50,torch.nn.BatchNorm2d,num_features,"C  from an expected input of size (N, C, H, W) ",C from an expected input of size BSTR,,,,,,
51,torch.nn.FractionalMaxPool2d,kernel_size,"the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)",Can be a single number k BSTR,,,0,,,
52,torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
53,torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
54,torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
55,torch.nn.quantized.functional.avg_pool2d,kernel_size,"size of the pooling region. Can be a single number or a tuple (kH, kW)",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
56,torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
57,torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
58,torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
59,torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,,,0;1,,,D_STRUCTURE
60,torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",Can be a single number or a one element D_STRUCTURE BSTR,,[1],0;1,,,D_STRUCTURE
61,torch.zeros,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,Can be a variable number of arguments or a collection like a D_STRUCTURE,int,,0;1,"[0,inf)",,D_STRUCTURE
62,torch.rand,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,Can be a variable number of arguments or a collection like a D_STRUCTURE,int,,0;1,,,D_STRUCTURE
63,torch.nn.ReLU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,,,,,,
64,torch.nn.ReLU6,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,,,,,,
65,torch.nn.Hardtanh,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,,,,,,
66,torch.quantization.convert,inplace,"carry out model transformations in-place, the original module is mutated",carry out model transformations in place the original PARAM is mutated,,,,,,
67,torch.quantization.quantize,inplace,"carry out model transformations in-place, the original module is mutated",carry out PARAM transformations in place the original module is mutated,,,,,,
68,torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",Check if the same inputs run through traced code produce the same outputs,,,,,,
69,torch.ger,vec2,1-D input vector,CONSTANT_NUM D PARAM D_STRUCTURE,,,CONSTANT_NUM,,,D_STRUCTURE
70,torch.svd,some,controls the shape of returned U and V,controls the shape of returned U and V,,,,,,
71,torch.ifft,normalized,controls whether to return normalized results. Default: `False`,controls whether to return normalized results,bool,,0,,,
72,torch.quantization.quantize,mapping,correspondence between original module types and quantized counterparts,correspondence between original module types and quantized counterparts,,,,,,
73,torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",CPU RNG state is always forked,,,,,,
74,torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",CUDA devices for which to fork the RNG,,,,,,
75,torch.cuda.comm.reduce_add,inputs,DD: Iterable[Tensor],D_STRUCTURE BSTR,,,,,,D_STRUCTURE
76,torch.distributed.new_group,ranks,DD: list[int],D_STRUCTURE BSTR,D_TYPE,,,,,D_STRUCTURE
77,torch.utils.checkpoint.checkpoint,*args,tuple containing inputs to the `function`,D_STRUCTURE containing inputs to the PARAM,,,,,,D_STRUCTURE
78,torch.random.fork_rng,devices,DD: iterable of CUDA IDs,D_STRUCTURE of CUDA IDs,int,,,"[0,inf)",,D_STRUCTURE
79,torch.autograd.grad,inputs,DD: sequence of Tensor,D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
80,torch.autograd.functional.hessian,inputs,DD: tuple of Tensors or Tensor,D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
81,torch.onnx.export,output_names,"DD: list of strings, default empty list",D_STRUCTURE of D_TYPE default empty D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
82,torch.jit.load,_extra_files,DD: dictionary of filename to content,D_STRUCTURE of filename to content,string,,,,,D_STRUCTURE
83,torch.nn.parallel.data_parallel,device_ids,DD: list of python:int or torch.device,D_STRUCTURE of python D_TYPE or torch device,D_TYPE,,,,,D_STRUCTURE
84,torch.distributed.new_group,ranks,List of ranks of group members.,D_STRUCTURE of ranks of group members,,,,,,D_STRUCTURE
85,torch.nn.functional.binary_cross_entropy_with_logits,target,Tensor of the same shape as input,D_STRUCTURE of the same shape as PARAM,,&PARAM,,,,D_STRUCTURE
86,torch.addmv,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
87,torch.bernoulli,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
88,torch.atan2,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
89,torch.normal,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
90,torch.eye,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
91,torch.mm,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
92,torch.ones,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
93,torch.remainder,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
94,torch.div,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
95,torch.addmm,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
96,torch.var,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
97,torch.histc,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
98,torch.multinomial,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
99,torch.randperm,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
100,torch.neg,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
101,torch.matmul,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
102,torch.nn.BCEWithLogitsLoss,pos_weight,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
103,torch.nn.functional.normalize,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
104,torch.mv,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
105,torch.diag,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
106,torch.pow,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
107,torch.nn.MultiMarginLoss,weight,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
108,torch.reciprocal,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
109,torch.conj,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
110,torch.erf,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
111,torch.acos,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
112,torch.lu,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
113,torch.min,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
114,torch.svd,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
115,torch.max,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
116,torch.cummax,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
117,torch.eig,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
118,torch.lt,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,,,,,D_STRUCTURE
119,torch.remainder,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,,,,,D_STRUCTURE
120,torch.cuda.comm.broadcast_coalesced,tensors,tensors to broadcast.,D_STRUCTURE to broadcast,,,,,,D_STRUCTURE
121,torch.distributed.recv,tensor,Tensor to fill with received data.,D_STRUCTURE to fill with received data,,,,,,D_STRUCTURE
122,torch.distributed.send,tensor,Tensor to send.,D_STRUCTURE to send,,,,,,D_STRUCTURE
123,torch.distributed.isend,tensor,Tensor to send.,D_STRUCTURE to send,,,,,,D_STRUCTURE
124,torch.norm,dim,"DD: int, 2-tuple of python:ints, 2-list of python:ints, optional",D_TYPE CONSTANT_NUM D_STRUCTURE of python D_TYPE CONSTANT_NUM D_STRUCTURE of python D_TYPE optional,D_TYPE,[CONSTANT_NUM],,,,D_STRUCTURE
125,torch.nn.ReplicationPad3d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
126,torch.nn.ReflectionPad2d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
127,torch.nn.ReplicationPad1d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
128,torch.onnx.export,aten,"DD: bool, default False",D_TYPE default CONSTANT_BOOL,D_TYPE,,,,,
129,torch.cumprod,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
130,torch.empty_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
131,torch.linspace,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
132,torch.ones,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
133,torch.triu_indices,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
134,torch.sparse.sum,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
135,torch.randperm,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
136,torch.zeros_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
137,torch.nn.functional.softmax,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
138,torch.hann_window,periodic,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
139,torch.nn.MultiLabelMarginLoss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
140,torch.allclose,equal_nan,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
141,torch.autograd.grad,retain_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
142,torch.autograd.grad,allow_unused,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
143,torch.ifft,normalized,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
144,torch.jit.trace,check_trace,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
145,torch.svd,some,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
146,torch.nn.PoissonNLLLoss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
147,torch.nn.functional.embedding,sparse,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
148,torch.nn.utils.rnn.pack_sequence,enforce_sorted,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
149,torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
150,torch.matrix_rank,symmetric,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
151,torch.autograd.functional.vjp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
152,torch.nn.NLLLoss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
153,torch.nn.Embedding,sparse,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
154,torch.nn.SoftMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
155,torch.nn.HingeEmbeddingLoss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
156,torch.nn.quantized.functional.interpolate,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
157,torch.norm,keepdim,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
158,torch.nn.MSELoss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
159,torch.nn.SoftMarginLoss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
160,torch.nn.functional.nll_loss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
161,torch.nn.Upsample,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
162,torch.hub.list,force_reload,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
163,torch.autograd.functional.vhp,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
164,torch.multinomial,replacement,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
165,torch.svd,compute_uv,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
166,torch.nn.PairwiseDistance,keepdim,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
167,torch.ones_like,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
168,torch.combinations,with_replacement,"DD: boolean, optional",D_TYPE optional,D_TYPE,,,,,
169,torch.nn.PoissonNLLLoss,eps,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
170,torch.nn.utils.rnn.pad_sequence,padding_value,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
171,torch.nn.functional.poisson_nll_loss,eps,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
172,torch.nn.Embedding,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
173,torch.nn.EmbeddingBag,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
174,torch.cuda.comm.gather,destination,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
175,torch.kthvalue,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
176,torch.distributed.send,tag,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
177,torch.nn.utils.spectral_norm,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
178,torch.distributed.gather,dst,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
179,torch.nn.ConvTranspose3d,groups,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
180,torch.distributed.recv,src,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
181,torch.lobpcg,niter,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
182,torch.topk,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
183,torch.eye,m,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
184,torch.cat,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
185,torch.repeat_interleave,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
186,torch.lobpcg,n,"DD: integer, optional",D_TYPE optional,D_TYPE,,,,,
187,torch.nn.utils.remove_spectral_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,
188,torch.nn.Upsample,mode,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,
189,torch.nn.Conv2d,padding_mode,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
190,torch.nn.SmoothL1Loss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
191,torch.nn.MultiLabelSoftMarginLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
192,torch.nn.functional.binary_cross_entropy_with_logits,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
193,torch.nn.MSELoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
194,torch.nn.functional.kl_div,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
195,torch.utils.checkpoint.checkpoint,preserve_rng_state,"DD: bool, optional, default=True",D_TYPE optional default CONSTANT_BOOL,D_TYPE,,,,,
196,torch.nn.MaxUnpool2d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
197,torch.nn.Fold,kernel_size,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
198,torch.nn.MaxUnpool3d,padding,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
199,torch.nn.MaxUnpool3d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
200,torch.nn.UpsamplingNearest2d,scale_factor,"DD: float or Tuple[float, float], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,0;1,,,D_STRUCTURE
201,torch.nn.UpsamplingBilinear2d,size,"DD: int or Tuple[int, int], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,0;1,,,D_STRUCTURE
202,torch.nn.UpsamplingNearest2d,size,"DD: int or Tuple[int, int], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,0;1,,,D_STRUCTURE
203,torch.nn.Upsample,scale_factor,"DD: float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional",D_TYPE or D_STRUCTURE BSTR or D_STRUCTURE BSTR or D_STRUCTURE BSTR optional,D_TYPE,,0;1,,,D_STRUCTURE
204,torch.nn.Upsample,size,"DD: int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional",D_TYPE or D_STRUCTURE BSTR or D_STRUCTURE BSTR or D_STRUCTURE BSTR optional,D_TYPE,,0;1,,,D_STRUCTURE
205,torch.var,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,0;1,,,D_STRUCTURE
206,torch.std,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,0;1,,,D_STRUCTURE
207,torch.roll,shifts,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,0;1,,,D_STRUCTURE
208,torch.tensordot,dims,DD: int or tuple of two lists of python:integers,D_TYPE or D_STRUCTURE of two D_STRUCTURE of python D_TYPE,D_TYPE,,0;1,,,D_STRUCTURE
209,torch.nn.Fold,dilation,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
210,torch.nn.ConvTranspose2d,output_padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
211,torch.nn.Fold,padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
212,torch.nn.ConvTranspose3d,padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
213,torch.nn.Conv3d,stride,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
214,torch.nn.Conv1d,dilation,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,0;1,,,D_STRUCTURE
215,torch.nn.DataParallel,output_device,DD: int or torch.device,D_TYPE or torch device,D_TYPE,,,,,
216,torch.jit.load,map_location,DD: string or torch.device,D_TYPE or torch device,D_TYPE,,,,,
217,torch.utils.data.random_split,dataset,Dataset to be split,Dataset to be split,,,,,,
218,torch.nn.SyncBatchNorm,process_group,synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world,Default behavior is synchronization across the whole world,,,,,,
219,torch.nn.SyncBatchNorm,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
220,torch.nn.BatchNorm2d,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
221,torch.nn.ReLU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
222,torch.nn.ReLU6,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
223,torch.nn.Hardtanh,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
224,torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",Default CONSTANT_BOOL,bool,,0,,,
225,torch.ifft,normalized,controls whether to return normalized results. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
226,torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
227,torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
228,torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
229,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
230,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
231,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
232,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
233,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
234,torch.nn.PairwiseDistance,keepdim,Determines whether or not to keep the vector dimension. Default: False,Default CONSTANT_BOOL,bool,,0,,,
235,torch.nn.functional.avg_pool3d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
236,torch.unique,return_inverse,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
237,torch.nn.ReLU,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
238,torch.allclose,equal_nan,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
239,torch.nn.MaxPool3d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
240,torch.nn.ReLU6,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
241,torch.nn.quantized.functional.avg_pool2d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
242,torch.var_mean,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
243,torch.autograd.grad,allow_unused,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
244,torch.nn.functional.dropout,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
245,torch.ifft,normalized,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
246,torch.nn.MaxPool3d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
247,torch.nn.functional.embedding,sparse,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
248,torch.combinations,with_replacement,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
249,torch.nn.MaxPool1d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
250,torch.matrix_rank,symmetric,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
251,torch.autograd.functional.vjp,strict,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
252,torch.var,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
253,torch.nn.Embedding,sparse,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
254,torch.nn.Hardtanh,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
255,torch.onnx.export,aten,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
256,torch.quantization.quantize,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
257,torch.prod,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
258,torch.quantization.convert,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
259,torch.nn.AvgPool2d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
260,torch.norm,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
261,torch.nn.AdaptiveMaxPool2d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
262,torch.hub.list,force_reload,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
263,torch.autograd.functional.vhp,create_graph,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
264,torch.multinomial,replacement,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
265,torch.nn.PairwiseDistance,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
266,torch.ones_like,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
267,torch.nn.AdaptiveMaxPool3d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
268,torch.nn.CTCLoss,zero_infinity,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
269,torch.nn.RReLU,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
270,torch.eye,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
271,torch.autograd.functional.jacobian,strict,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
272,torch.distributed.all_gather,async_op,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
273,torch.nn.utils.rnn.pack_padded_sequence,batch_first,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
274,torch.autograd.functional.vjp,create_graph,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
275,torch.sum,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
276,torch.nn.MaxPool2d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
277,torch.argsort,descending,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
278,torch.sparse_coo_tensor,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
279,torch.autograd.functional.vhp,strict,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
280,torch.kthvalue,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
281,torch.min,keepdim,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
282,torch.nn.functional.avg_pool1d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
283,torch.nn.functional.dropout3d,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
284,torch.nn.functional.poisson_nll_loss,full,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
285,torch.autograd.functional.hessian,strict,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
286,torch.autograd.functional.hvp,strict,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
287,torch.onnx.export,verbose,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
288,torch.nn.CELU,inplace,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
289,torch.autograd.functional.hessian,create_graph,DF: False,DEFAULT CONSTANT_BOOL,bool,,0,,,
290,torch.std,unbiased,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
291,torch.nn.RNNCell,bias,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
292,torch.hann_window,periodic,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
293,torch.nn.functional.avg_pool2d,count_include_pad,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
294,torch.var,unbiased,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
295,torch.nn.SyncBatchNorm,track_running_stats,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
296,torch.unique,sorted,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
297,torch.std_mean,unbiased,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
298,torch.nn.BatchNorm2d,track_running_stats,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
299,torch.jit.trace,check_trace,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
300,torch.svd,some,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
301,torch.std2,unbiased,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
302,torch.nn.utils.rnn.pack_sequence,enforce_sorted,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
303,torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
304,torch.nn.AvgPool3d,count_include_pad,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
305,torch.svd,compute_uv,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
306,torch.hub.load_state_dict_from_url,progress,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
307,torch.nn.BatchNorm3d,affine,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
308,torch.hamming_window,periodic,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
309,torch.utils.cpp_extension.load,is_python_module,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
310,torch.nn.BatchNorm1d,affine,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
311,torch.nn.ConvTranspose3d,bias,DF: True,DEFAULT CONSTANT_BOOL,bool,,0,,,
312,torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
313,torch.nn.RNNCell,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
314,torch.nn.GRU,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
315,torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",Default CONSTANT_BOOL,bool,,0,,,
316,torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
317,torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",Default CONSTANT_BOOL,bool,,0,,,
318,torch.allclose,equal_nan,"if `True`, then two `NaN` s will be compared as equal. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
319,torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
320,torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
321,torch.ones_like,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,bool,,0,,,
322,torch.nn.functional.dropout,inplace,"If set to `True`, will do this operation in-place. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
323,torch.matrix_rank,symmetric,indicates whether `input` is symmetric. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
324,torch.nn.functional.avg_pool2d,count_include_pad,"when True, will include the zero-padding in the averaging calculation. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
325,torch.nn.quantized.functional.avg_pool2d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
326,torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
327,torch.pinverse,rcond,A floating point value to determine the cutoff for small singular values. Default: 1e-15,Default CONSTANT_FLOAT,int,,0,,,
328,torch.nn.LayerNorm,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_FLOAT,int,,0,,,
329,torch.nn.InstanceNorm1d,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_FLOAT,int,,0,,,
330,torch.nn.init.uniform_,a,DF: 0.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
331,torch.nn.LocalResponseNorm,alpha,DF: 0.0001,DEFAULT CONSTANT_FLOAT,float,,0,,,
332,torch.autograd.gradcheck,rtol,DF: 0.001,DEFAULT CONSTANT_FLOAT,float,,0,,,
333,torch.nn.TransformerEncoderLayer,dropout,DF: 0.1,DEFAULT CONSTANT_FLOAT,float,,0,,,
334,torch.nn.TransformerDecoderLayer,dropout,DF: 0.1,DEFAULT CONSTANT_FLOAT,float,,0,,,
335,torch.nn.InstanceNorm1d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,float,,0,,,
336,torch.nn.BatchNorm3d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,float,,0,,,
337,torch.nn.RReLU,lower,DF: 0.125,DEFAULT CONSTANT_FLOAT,float,,0,,,
338,torch.nn.Softshrink,lambd,DF: 0.5,DEFAULT CONSTANT_FLOAT,float,,0,,,
339,torch.nn.LocalResponseNorm,beta,DF: 0.75,DEFAULT CONSTANT_FLOAT,float,,0,,,
340,torch.nn.quantized.functional.conv3d,scale,DF: 1.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
341,torch.nn.LayerNorm,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,float,,0,,,
342,torch.nn.InstanceNorm1d,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,float,,0,,,
343,torch.nn.InstanceNorm3d,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,float,,0,,,
344,torch.nn.PoissonNLLLoss,eps,DF: 1e-08,DEFAULT CONSTANT_FLOAT,float,,0,,,
345,torch.nn.functional.poisson_nll_loss,eps,DF: 1e-08,DEFAULT CONSTANT_FLOAT,float,,0,,,
346,torch.pinverse,rcond,DF: 1e-15,DEFAULT CONSTANT_FLOAT,float,,0,,,
347,torch.nn.Embedding,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
348,torch.nn.EmbeddingBag,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
349,torch.nn.functional.embedding,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
350,torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,DF: 4.0,DEFAULT CONSTANT_FLOAT,float,,0,,,
351,torch.nn.LocalResponseNorm,beta,exponent. Default: 0.75,Default CONSTANT_FLOAT,float,,0,,,
352,torch.nn.PoissonNLLLoss,eps,Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8,Default CONSTANT_FLOAT,float,,0,,,
353,torch.nn.functional.poisson_nll_loss,eps,Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8,Default CONSTANT_FLOAT,float,,0,,,
354,torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",Default CONSTANT_NUM,int,,0,,,
355,torch.nn.ConvTranspose3d,padding,`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0,Default CONSTANT_NUM,int,,0,,,
356,torch.nn.Fold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,Default CONSTANT_NUM,int,,0,,,
357,torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",Default CONSTANT_NUM,int,,0,,,
358,torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",Default CONSTANT_NUM,int,,0,,,
359,torch.nn.ConvTranspose2d,output_padding,Additional size added to one side of each dimension in the output shape. Default: 0,Default CONSTANT_NUM,int,,0,,,
360,torch.trapz,dim,DF: -1,DEFAULT CONSTANT_NUM,int,,0,,,
361,torch.nn.functional.gumbel_softmax,dim,DF: -1,DEFAULT CONSTANT_NUM,int,,0,,,
362,torch.nn.functional.nll_loss,ignore_index,DF: -100,DEFAULT CONSTANT_NUM,int,,0,,,
363,torch.nn.functional.pad,value,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
364,torch.distributed.send,tag,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
365,torch.nn.functional.conv_transpose3d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
366,torch.nn.ConvTranspose2d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
367,torch.nn.utils.rnn.pad_sequence,padding_value,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
368,torch.arange,start,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
369,torch.distributed.gather,dst,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
370,torch.nn.functional.conv_transpose1d,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
371,torch.nn.Fold,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
372,torch.nn.ConvTranspose3d,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
373,torch.nn.functional.conv_transpose2d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
374,torch.nn.MaxUnpool3d,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
375,torch.chunk,dim,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
376,torch.cat,dim,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
377,torch.nn.MaxUnpool2d,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
378,torch.triu,diagonal,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
379,torch.as_strided,storage_offset,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
380,torch.histc,min,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
381,torch.nn.functional.avg_pool1d,padding,DF: 0,DEFAULT CONSTANT_NUM,int,,0,,,
382,torch.nn.Fold,dilation,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
383,torch.nn.quantized.functional.conv2d,dilation,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
384,torch.addmm,alpha,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
385,torch.nn.init.orthogonal_,gain,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
386,torch.nn.ConvTranspose3d,groups,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
387,torch.addbmm,beta,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
388,torch.baddbmm,beta,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
389,torch.nn.MaxPool1d,dilation,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
390,torch.nn.quantized.functional.conv3d,stride,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
391,torch.nn.functional.conv1d,stride,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
392,torch.nn.Conv3d,stride,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
393,torch.nn.quantized.functional.conv3d,groups,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
394,torch.nn.init.dirac_,groups,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
395,torch.nn.Conv1d,dilation,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
396,torch.addr,beta,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
397,torch.nn.ConvTranspose2d,groups,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
398,torch.addcdiv,value,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
399,torch.nn.functional.conv_transpose2d,dilation,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
400,torch.nn.Unfold,stride,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
401,torch.nn.CosineSimilarity,dim,DF: 1,DEFAULT CONSTANT_NUM,int,,0,,,
402,torch.logspace,steps,DF: 100,DEFAULT CONSTANT_NUM,int,,0,,,
403,torch.linspace,steps,DF: 100,DEFAULT CONSTANT_NUM,int,,0,,,
404,torch.tensordot,dims,DF: 2,DEFAULT CONSTANT_NUM,int,,0,,,
405,torch.pca_lowrank,niter,DF: 2,DEFAULT CONSTANT_NUM,int,,0,,,
406,torch.nn.Transformer,num_decoder_layers,DF: 6,DEFAULT CONSTANT_NUM,int,,0,,,
407,torch.nn.functional.pad,value,fill value for `'constant'` padding. Default: `0`,Default CONSTANT_NUM,int,,0,,,
408,torch.nn.GRU,dropout,"If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0",Default CONSTANT_NUM,int,,0,,,
409,torch.nn.Fold,padding,implicit zero padding to be added on both sides of input. Default: 0,Default CONSTANT_NUM,int,,0,,,
410,torch.nn.ConvTranspose3d,groups,Number of blocked connections from input channels to output channels. Default: 1,Default CONSTANT_NUM,int,,0,,,
411,torch.logspace,steps,number of points to sample between `start` and `end`. Default: `100`.,Default CONSTANT_NUM,int,,0,,,
412,torch.nn.Conv1d,dilation,Spacing between kernel elements. Default: 1,Default CONSTANT_NUM,int,,0,,,
413,torch.nn.quantized.functional.conv3d,groups,"split input into groups, in _channels  should be divisible by the number of groups. Default: 1",Default CONSTANT_NUM,int,,0,,,
414,torch.nn.Conv3d,stride,Stride of the convolution. Default: 1,Default CONSTANT_NUM,int,,0,,,
415,torch.nn.Embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,int,,0,,,
416,torch.nn.EmbeddingBag,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,int,,0,,,
417,torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",Default CONSTANT_NUM,int,,0,,,
418,torch.arange,start,the starting value for the set of points. Default: `0`.,Default CONSTANT_NUM,int,,0,,,
419,torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",Default CONSTANT_NUM,int,,0,,,
420,torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",Default CONSTANT_NUM,int,,0,,,
421,torch.nn.utils.rnn.pad_sequence,padding_value,value for padded elements. Default: 0.,Default CONSTANT_NUM,int,,0,,,
422,torch.nn.RReLU,lower,lower bound of the uniform distribution. Default:  1/8,Default CONSTANT_NUM CONSTANT_NUM,,,,,,
423,torch.randperm,dtype,DF: torch.int64,DEFAULT D_TYPE,torch.dtype,,,,,
424,torch.triu_indices,dtype,DF: torch.long,DEFAULT D_TYPE,torch.dtype,,,,,
425,torch.randperm,dtype,the desired data type of returned tensor. Default: `torch.int64`.,Default D_TYPE,torch.dtype,,,,,
426,torch.cuda.set_rng_state,device,DF: cuda,DEFAULT DF_STR,string,,,,,
427,torch.jit.load,_extra_files,DF: ExtraFilesMap{},DEFAULT DF_STR,string,,,,,
428,torch.jit.save,_extra_files,DF: ExtraFilesMap{},DEFAULT DF_STR,string,,,,,
429,torch.nn.SmoothL1Loss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
430,torch.nn.MultiLabelSoftMarginLoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
431,torch.nn.functional.binary_cross_entropy_with_logits,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
432,torch.nn.MSELoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
433,torch.nn.functional.kl_div,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
434,torch.nn.TripletMarginLoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
435,torch.nn.CrossEntropyLoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
436,torch.nn.KLDivLoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
437,torch.nn.NLLLoss,reduction,DF: mean,DEFAULT DF_STR,string,,,,,
438,torch.nn.Upsample,mode,DF: nearest,DEFAULT DF_STR,string,,,,,
439,torch.nn.Transformer,activation,DF: relu,DEFAULT DF_STR,string,,,,,
440,torch.nn.utils.remove_spectral_norm,name,DF: weight,DEFAULT DF_STR,string,,,,,
441,torch.nn.utils.spectral_norm,name,DF: weight,DEFAULT DF_STR,string,,,,,
442,torch.nn.utils.weight_norm,name,DF: weight,DEFAULT DF_STR,string,,,,,
443,torch.nn.Conv2d,padding_mode,DF: zeros,DEFAULT DF_STR,string,,,,,
444,torch.sparse.sum,dtype,the desired data type of returned Tensor. Default: dtype of `input`.,Default dtype of PARAM,torch.dtype,,,,,
445,torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if None uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,
446,torch.triu_indices,dtype,"the desired data type of returned tensor. Default: if `None`, `torch.long`.",Default if QSTR D_TYPE,,,,,,
447,torch.empty_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,,,,,,
448,torch.zeros_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,,,,,,
449,torch.linspace,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,,,,,,
450,torch.ones,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,,,,,,
451,torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,
452,torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,
453,torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,
454,torch.hub.list,force_reload,whether to discard the existing cache and force a fresh download. Default is False.,Default is CONSTANT_BOOL,bool,,0,,,
455,torch.hamming_window,device,DF: None,DEFAULT None,,,,,,
456,torch.nn.UpsamplingBilinear2d,size,DF: None,DEFAULT None,,,,,,
457,torch.addmv,out,DF: None,DEFAULT None,,,,,,
458,torch.bernoulli,out,DF: None,DEFAULT None,,,,,,
459,torch.cuda.comm.gather,destination,DF: None,DEFAULT None,,,,,,
460,torch.atan2,out,DF: None,DEFAULT None,,,,,,
461,torch.normal,out,DF: None,DEFAULT None,,,,,,
462,torch.lu,out,DF: None,DEFAULT None,,,,,,
463,torch.min,out,DF: None,DEFAULT None,,,,,,
464,torch.nn.MultiLabelMarginLoss,size_average,DF: None,DEFAULT None,,,,,,
465,torch.eye,out,DF: None,DEFAULT None,,,,,,
466,torch.svd,out,DF: None,DEFAULT None,,,,,,
467,torch.kthvalue,dim,DF: None,DEFAULT None,,,,,,
468,torch.bincount,weights,DF: None,DEFAULT None,,,,,,
469,torch.nn.MaxUnpool2d,stride,DF: None,DEFAULT None,,,,,,
470,torch.mm,out,DF: None,DEFAULT None,,,,,,
471,torch.nn.functional.avg_pool1d,stride,DF: None,DEFAULT None,,,,,,
472,torch.sparse_coo_tensor,device,DF: None,DEFAULT None,,,,,,
473,torch.randperm,device,DF: None,DEFAULT None,,,,,,
474,torch.ones,out,DF: None,DEFAULT None,,,,,,
475,torch.remainder,out,DF: None,DEFAULT None,,,,,,
476,torch.lobpcg,n,DF: None,DEFAULT None,,,,,,
477,torch.nn.Softmax,dim,DF: None,DEFAULT None,,,,,,
478,torch.cumprod,dtype,DF: None,DEFAULT None,,,,,,
479,torch.cuda.memory_stats,device,DF: None,DEFAULT None,,,,,,
480,torch.max,out,DF: None,DEFAULT None,,,,,,
481,torch.autograd.grad,retain_graph,DF: None,DEFAULT None,,,,,,
482,torch.empty_like,dtype,DF: None,DEFAULT None,,,,,,
483,torch.nn.DataParallel,output_device,DF: None,DEFAULT None,,,,,,
484,torch.div,out,DF: None,DEFAULT None,,,,,,
485,torch.nn.utils.spectral_norm,dim,DF: None,DEFAULT None,,,,,,
486,torch.cummax,out,DF: None,DEFAULT None,,,,,,
487,torch.jit.load,map_location,DF: None,DEFAULT None,,,,,,
488,torch.addmm,out,DF: None,DEFAULT None,,,,,,
489,torch.var,out,DF: None,DEFAULT None,,,,,,
490,torch.histc,out,DF: None,DEFAULT None,,,,,,
491,torch.nn.Upsample,scale_factor,DF: None,DEFAULT None,,,,,,
492,torch.load,map_location,DF: None,DEFAULT None,,,,,,
493,torch.nn.GRU,bias,DF: None,DEFAULT None,,,,,,
494,torch.distributed.new_group,ranks,DF: None,DEFAULT None,,,,,,
495,torch.eig,out,DF: None,DEFAULT None,,,,,,
496,torch.linspace,dtype,DF: None,DEFAULT None,,,,,,
497,torch.empty_strided,device,DF: None,DEFAULT None,,,,,,
498,torch.norm,dim,DF: None,DEFAULT None,,,,,,
499,torch.nn.Upsample,size,DF: None,DEFAULT None,,,,,,
500,torch.nn.PoissonNLLLoss,size_average,DF: None,DEFAULT None,,,,,,
501,torch.ones,dtype,DF: None,DEFAULT None,,,,,,
502,torch.multinomial,out,DF: None,DEFAULT None,,,,,,
503,torch.nn.SyncBatchNorm,process_group,DF: None,DEFAULT None,,,,,,
504,torch.sparse.sum,dtype,DF: None,DEFAULT None,,,,,,
505,torch.utils.checkpoint.checkpoint,preserve_rng_state,DF: None,DEFAULT None,,,,,,
506,torch.randperm,out,DF: None,DEFAULT None,,,,,,
507,torch.neg,out,DF: None,DEFAULT None,,,,,,
508,torch.matmul,out,DF: None,DEFAULT None,,,,,,
509,torch.nn.TransformerDecoder,norm,DF: None,DEFAULT None,,,,,,
510,torch.poisson,generator,DF: None,DEFAULT None,,,,,,
511,torch.nn.RNN,hidden_size,DF: None,DEFAULT None,,,,,,
512,torch.nn.BCEWithLogitsLoss,pos_weight,DF: None,DEFAULT None,,,,,,
513,torch.nn.NLLLoss,size_average,DF: None,DEFAULT None,,,,,,
514,torch.nn.parallel.data_parallel,device_ids,DF: None,DEFAULT None,,,,,,
515,torch.quantization.quantize,mapping,DF: None,DEFAULT None,,,,,,
516,torch.onnx.export,output_names,DF: None,DEFAULT None,,,,,,
517,torch.nn.functional.normalize,out,DF: None,DEFAULT None,,,,,,
518,torch.nn.UpsamplingNearest2d,size,DF: None,DEFAULT None,,,,,,
519,torch.mv,out,DF: None,DEFAULT None,,,,,,
520,torch.nn.GRU,dropout,DF: None,DEFAULT None,,,,,,
521,torch.nn.functional.avg_pool2d,divisor_override,DF: None,DEFAULT None,,,,,,
522,torch.diag,out,DF: None,DEFAULT None,,,,,,
523,torch.quantization.convert,mapping,DF: None,DEFAULT None,,,,,,
524,torch.nn.MaxPool3d,stride,DF: None,DEFAULT None,,,,,,
525,torch.nn.functional.avg_pool3d,divisor_override,DF: None,DEFAULT None,,,,,,
526,torch.nn.SoftMarginLoss,reduce,DF: None,DEFAULT None,,,,,,
527,torch.nn.HingeEmbeddingLoss,size_average,DF: None,DEFAULT None,,,,,,
528,torch.pow,out,DF: None,DEFAULT None,,,,,,
529,torch.random.fork_rng,devices,DF: None,DEFAULT None,,,,,,
530,torch.nn.quantized.functional.interpolate,align_corners,DF: None,DEFAULT None,,,,,,
531,torch.set_printoptions,sci_mode,DF: None,DEFAULT None,,,,,,
532,torch.distributed.recv,src,DF: None,DEFAULT None,,,,,,
533,torch.nn.MSELoss,reduce,DF: None,DEFAULT None,,,,,,
534,torch.nn.SoftMarginLoss,size_average,DF: None,DEFAULT None,,,,,,
535,torch.nn.MultiMarginLoss,weight,DF: None,DEFAULT None,,,,,,
536,torch.reciprocal,out,DF: None,DEFAULT None,,,,,,
537,torch.nn.functional.nll_loss,reduce,DF: None,DEFAULT None,,,,,,
538,torch.lobpcg,niter,DF: None,DEFAULT None,,,,,,
539,torch.nn.Upsample,align_corners,DF: None,DEFAULT None,,,,,,
540,torch.topk,dim,DF: None,DEFAULT None,,,,,,
541,torch.zeros_like,dtype,DF: None,DEFAULT None,,,,,,
542,torch.eye,m,DF: None,DEFAULT None,,,,,,
543,torch.conj,out,DF: None,DEFAULT None,,,,,,
544,torch.erf,out,DF: None,DEFAULT None,,,,,,
545,torch.nn.UpsamplingNearest2d,scale_factor,DF: None,DEFAULT None,,,,,,
546,torch.nn.MaxUnpool3d,stride,DF: None,DEFAULT None,,,,,,
547,torch.repeat_interleave,dim,DF: None,DEFAULT None,,,,,,
548,torch.utils.cpp_extension.load_inline,functions,DF: None,DEFAULT None,,,,,,
549,torch.nn.functional.avg_pool2d,stride,DF: None,DEFAULT None,,,,,,
550,torch.nn.GRU,hidden_size,DF: None,DEFAULT None,,,,,,
551,torch.nn.functional.softmax,dtype,DF: None,DEFAULT None,,,,,,
552,torch.acos,out,DF: None,DEFAULT None,,,,,,
553,torch.logical_and,out,DF: None,DEFAULT None,,,,,,
554,torch.lobpcg,method,DF: None,DEFAULT None,,,,,,
555,torch.round,out,DF: None,DEFAULT None,,,,,,
556,torch.multinomial,generator,DF: None,DEFAULT None,,,,,,
557,torch.symeig,out,DF: None,DEFAULT None,,,,,,
558,torch.nn.MultiLabelMarginLoss,reduce,DF: None,DEFAULT None,,,,,,
559,torch.empty_like,device,DF: None,DEFAULT None,,,,,,
560,torch.nn.BCELoss,reduce,DF: None,DEFAULT None,,,,,,
561,torch.utils.cpp_extension.load,extra_cflags,DF: None,DEFAULT None,,,,,,
562,torch.nn.MaxUnpool1d,stride,DF: None,DEFAULT None,,,,,,
563,torch.mode,out,DF: None,DEFAULT None,,,,,,
564,torch.lu,get_infos,DF: None,DEFAULT None,,,,,,
565,torch.nn.functional.embedding,padding_idx,DF: None,DEFAULT None,,,,,,
566,torch.gt,out,DF: None,DEFAULT None,,,,,,
567,torch.nn.TripletMarginLoss,reduce,DF: None,DEFAULT None,,,,,,
568,torch.nn.LSTM,input_size,DF: None,DEFAULT None,,,,,,
569,torch.rand_like,dtype,DF: None,DEFAULT None,,,,,,
570,torch.ones,device,DF: None,DEFAULT None,,,,,,
571,torch.logsumexp,out,DF: None,DEFAULT None,,,,,,
572,torch.quantization.propagate_qconfig_,qconfig_dict,DF: None,DEFAULT None,,,,,,
573,torch.nn.MaxPool2d,stride,DF: None,DEFAULT None,,,,,,
574,torch.log1p,out,DF: None,DEFAULT None,,,,,,
575,torch.nn.functional.interpolate,align_corners,DF: None,DEFAULT None,,,,,,
576,torch.nn.functional.adaptive_max_pool3d,return_indices,DF: None,DEFAULT None,,,,,,
577,torch.clamp,out,DF: None,DEFAULT None,,,,,,
578,torch.nn.RNN,input_size,DF: None,DEFAULT None,,,,,,
579,torch.empty_like,layout,DF: None,DEFAULT None,,,,,,
580,torch.eye,dtype,DF: None,DEFAULT None,,,,,,
581,torch.logical_not,out,DF: None,DEFAULT None,,,,,,
582,torch.nn.functional.binary_cross_entropy_with_logits,weight,DF: None,DEFAULT None,,,,,,
583,torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,DF: None,DEFAULT None,,,,,,
584,torch.sin,out,DF: None,DEFAULT None,,,,,,
585,torch.autograd.gradgradcheck,grad_outputs,DF: None,DEFAULT None,,,,,,
586,torch.nn.L1Loss,size_average,DF: None,DEFAULT None,,,,,,
587,torch.logical_or,out,DF: None,DEFAULT None,,,,,,
588,torch.bartlett_window,dtype,DF: None,DEFAULT None,,,,,,
589,torch.log,out,DF: None,DEFAULT None,,,,,,
590,torch.nn.quantized.functional.avg_pool2d,stride,DF: None,DEFAULT None,,,,,,
591,torch.nn.functional.avg_pool2d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",Default None,,,,,,
592,torch.nn.functional.avg_pool3d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",Default None,,,,,,
593,torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",Default None,,,,,,
594,torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",Default None,,,,,,
595,torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Default PARAM,,,,,,
596,torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",Default PARAM,,,,,,
597,torch.nn.Conv2d,padding_mode,"`'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`",Default QSTR,,,,,QSTR,
598,torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",Default QSTR,,,,,QSTR,
599,torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,,
600,torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,QSTR,
601,torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,QSTR,
602,torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,QSTR,
603,torch.nn.Upsample,mode,"the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`",Default QSTR,,,,,QSTR,
604,torch.cuda.set_rng_state,device,"The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",Default QSTR i e torch device QSTR the current CUDA device,,,,,QSTR,
605,torch.quantization.fuse_modules,fuser_func,DF: <functionfuse_known_modules>,DEFAULT REXPR,,,,,,
606,torch.distributed.barrier,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
607,torch.distributed.get_rank,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
608,torch.distributed.broadcast_multigpu,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
609,torch.distributed.send,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
610,torch.distributed.all_gather,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
611,torch.distributed.get_world_size,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
612,torch.distributed.reduce,group,DF: <objectobject>,DEFAULT REXPR,,,,,,
613,torch.full,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,,
614,torch.lobpcg,n,"if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns.",Default value for n is k If X is specifed the value of n BSTR must be the number of X columns,D_TYPE,,,,,
615,torch.nn.MaxPool3d,stride,the stride of the window. Default value is `kernel_size`,Default value is PARAM,,,,,,
616,torch.autograd.grad,allow_unused,"If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
617,torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
618,torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
619,torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",Defaults to the value of PARAM,,,,,,
620,torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
621,torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
622,torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
623,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
624,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
625,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
626,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
627,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
628,torch.distributed.gather,dst,Destination rank (default is 0),Destination rank BSTR,,,,,,
629,torch.nn.PairwiseDistance,keepdim,Determines whether or not to keep the vector dimension. Default: False,Determines whether or not to keep the D_STRUCTURE dimension,bool,,0,,,
630,torch.nn.DataParallel,output_device,device location of output (default: device_ids[0]),device location of output default PARAM BSTR,,,,,,
631,torch.chunk,dim,dimension along which to split the tensor,dimension along which to split the D_STRUCTURE,int,,0,,,
632,torch.nn.utils.spectral_norm,dim,"dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",dimension corresponding to number of outputs the default is CONSTANT_NUM except for modules that are instances of ConvTranspose CONSTANT_NUM CONSTANT_NUM CONSTANT_NUM d when it is CONSTANT_NUM,int,,0,,,
633,torch.can_cast,from,DD: dpython:type,dpython type,torch.dtype,,,,,
634,torch.set_printoptions,sci_mode,"Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by _Formatter",Enable BSTR scientific notation,,,,,,
635,torch.nn.functional.pad,value,fill value for `'constant'` padding. Default: `0`,fill value for QSTR padding,,,,,,
636,torch.nn.functional.conv_transpose2d,weight,"filters of shape (in _channels ,  out _channels/groups , kH , kW) ",filters of shape BSTR,,BSTR,,,,
637,torch.nn.functional.cosine_similarity,x1,First input.,First input,,,,,,
638,torch.set_grad_enabled,mode,"Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients.",Flag whether to enable grad CONSTANT_BOOL or disable CONSTANT_BOOL,bool,,0,,,
639,torch.quantization.fuse_modules,fuser_func,"Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",For example fuser_func BSTR returns the D_STRUCTURE BSTR Defaults to torch quantization fuse_known_modules,,,,,,
640,torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",For infinite iteration but until convergence criteria is met use CONSTANT_NUM,,,,,,
641,torch.quantization.fuse_modules,fuser_func,"Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",Function that takes in a D_STRUCTURE of modules and outputs a D_STRUCTURE of fused modules of the same length,,,,,,
642,torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",Geometrically we consider the pixels of the PARAM and output as squares rather than points,,,,,,
643,torch.nn.parallel.data_parallel,device_ids,GPU ids on which to replicate module,GPU ids on which to replicate PARAM,int,,,"[0,inf)",,
644,torch.nn.Upsample,scale_factor,multiplier for spatial size. Has to match input size if it is a tuple.,Has to match input PARAM if it is a D_STRUCTURE,,,,,,
645,torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,,[CONSTANT_NUM],,,,D_STRUCTURE
646,torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,,[CONSTANT_NUM],,,,D_STRUCTURE
647,torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,,[CONSTANT_NUM],,,,D_STRUCTURE
648,torch.utils.cpp_extension.load_inline,functions,"A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",If a D_STRUCTURE is given it should map function names to docstrings BSTR,,,,,,D_STRUCTURE
649,torch.ones_like,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,,,,,,
650,torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,bool,,0,,,
651,torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL both the output and result will be computed in a differentiable way,bool,,0,,,
652,torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",if CONSTANT_BOOL checks that the input contains PARAM sorted by length in a decreasing order,bool,,0,,,
653,torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",If CONSTANT_BOOL gradient w r t,bool,,0,,,
654,torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",If CONSTANT_BOOL gradient w r t,bool,,0,,,
655,torch.hann_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL return a symmetric window,bool,,0,,,
656,torch.hann_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL returns a window to be used as periodic function,bool,,0,,,
657,torch.autograd.grad,allow_unused,"If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`.",If CONSTANT_BOOL specifying PARAM that were not used when computing PARAM BSTR is an error,bool,,0,,,
658,torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",if CONSTANT_BOOL the corner pixels of the input and output D_STRUCTURE are aligned and thus preserving the values at those pixels,bool,,0,,,
659,torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",If CONSTANT_BOOL the graph used to compute the grad will be freed,bool,,0,,,
660,torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",if CONSTANT_BOOL the PARAM is expected to contain sequences sorted by length in a decreasing order,bool,,0,,,
661,torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",If CONSTANT_BOOL the PARAM will get sorted unconditionally,bool,,0,,,
662,torch.nn.RNNCell,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",If CONSTANT_BOOL then the layer does not use bias weights b_ih and b_hh,bool,,0,,,
663,torch.nn.GRU,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",If CONSTANT_BOOL then the layer does not use bias weights b_ih and b_hh,bool,,0,,,
664,torch.allclose,equal_nan,"if `True`, then two `NaN` s will be compared as equal. Default: `False`",if CONSTANT_BOOL then two QSTR will be compared as equal,bool,,0,,,
665,torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",If CONSTANT_BOOL this condition is not checked,bool,,0,,,
666,torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the vjp for said PARAM which is the expected mathematical value,bool,,0,,,
667,torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",if CONSTANT_BOOL will return the indices along with the outputs,bool,,0,,,
668,torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",if CONSTANT_BOOL will return the indices along with the outputs,bool,,0,,,
669,torch.nn.MaxPool3d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later",if CONSTANT_BOOL will return the max indices along with the outputs,bool,,0,,,
670,torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE it represents the absolute number of parameters to prune,D_TYPE,,0,"[0,inf)",,
671,torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE should be between CONSTANT_FLOAT and CONSTANT_FLOAT and represent the fraction of parameters to prune,D_TYPE,,0,,,
672,torch.nn.MultiMarginLoss,weight,"a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.",If given it has to be a D_STRUCTURE of size C Otherwise it is treated as if having all ones,,C,,,,D_STRUCTURE
673,torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,,,,
674,torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,,,,
675,torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,,,,
676,torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If it is an D_TYPE D_STRUCTURE norm will be calculated if it is CONSTANT_NUM D_STRUCTURE of D_TYPE matrix norm will be calculated,D_TYPE,[CONSTANT_NUM],0;1,,,D_STRUCTURE
677,torch.nn.GRU,dropout,"If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0",If non zero introduces a Dropout layer on the outputs of each GRU layer except the last layer with dropout probability equal to QSTR,,,,,,
678,torch.set_printoptions,sci_mode,"Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by _Formatter",If None BSTR is specified the value is defined by Formatter,,,,,,
679,torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",If PARAM is CONSTANT_BOOL then the elements in the D_STRUCTURE are D_STRUCTURE IntTensor,int,,,,,D_STRUCTURE
680,torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",If PARAM is CONSTANT_BOOL then the elements in the D_STRUCTURE are D_STRUCTURE IntTensor and IntTensor,int,,,,,D_STRUCTURE
681,torch.nn.functional.normalize,out,"the output tensor. If `out` is used, this operation won't be differentiable.",If QSTR is used this operation won t be differentiable,,,,,,
682,torch.argmax,dim,"the dimension to reduce. If `None`, the argmax of the flattened input is returned.",If QSTR the argmax of the flattened PARAM is returned,,,,,,
683,torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the center points of their corner pixels preserving the values at the corner pixels,bool,,0,,,
684,torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the corner points of their corner pixels and the interpolation uses edge value padding for out of boundary values making this operation independent of PARAM PARAM when PARAM is kept the same,bool,,0,,,
685,torch.nn.functional.dropout,inplace,"If set to `True`, will do this operation in-place. Default: `False`",If set to CONSTANT_BOOL will do this operation in place,bool,,0,,,
686,torch.roll,shifts,"The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",If shifts is a D_STRUCTURE PARAM must be a D_STRUCTURE of the same size and each dimension will be rolled by the corresponding value,,,,,,D_STRUCTURE
687,torch.nn.functional.avg_pool2d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",if specified it will be used as divisor otherwise size of the pooling region will be used,,,,,,
688,torch.nn.functional.avg_pool3d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",if specified it will be used as divisor otherwise size of the pooling region will be used,,,,,,
689,torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",If specified the PARAM D_STRUCTURE is casted to QSTR before the operation is performed,,,,,,
690,torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",If specified the PARAM D_STRUCTURE is casted to QSTR before the operation is performed,,,,,,
691,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,bool,,0,,,
692,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,bool,,0,,,
693,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,bool,,0,,,
694,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,bool,,0,,,
695,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,bool,,0,,,
696,torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If the PARAM D_STRUCTURE has more than two dimensions the D_STRUCTURE norm will be applied to last dimension,,,,,,
697,torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If the value is None matrix norm will be calculated when the PARAM D_STRUCTURE only has two dimensions D_STRUCTURE norm will be calculated when the PARAM D_STRUCTURE only has one dimension,,,,,,
698,torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",If using aten mode all the ops original exported by the functions in symbolic_opset REXPR REXPR are exported as ATen ops,,,,,,
699,torch.lobpcg,n,"if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns.",if X is not specified then n specifies the size of the generated random approximation of eigenvectors,D_TYPE,,,,,
700,torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",If you explicitly specify devices this warning will be suppressed,,,,,,
701,torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,Ignored if PARAM QSTR and PARAM QSTR,,,,,,
702,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
703,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
704,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
705,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
706,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
707,torch.nn.Fold,padding,implicit zero padding to be added on both sides of input. Default: 0,implicit zero padding to be added on both sides of input,,,,,,
708,torch.matrix_rank,symmetric,indicates whether `input` is symmetric. Default: `False`,indicates whether PARAM is symmetric,bool,,0,,,
709,torch.distributed.all_reduce,tensor,Input and output of the collective. The function operates in-place.,Input and output of the collective,,,,,,
710,torch.nn.functional.avg_pool2d,input,"input tensor (minibatch , in _channels , iH , iW) ",input D_STRUCTURE BSTR,,BSTR,,,,D_STRUCTURE
711,torch.nn.functional.avg_pool3d,input,"input tensor (minibatch , in _channels , iT  times iH , iW) ",input D_STRUCTURE BSTR,,BSTR,,,,D_STRUCTURE
712,torch.nn.functional.grid_sample,input,"input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",input of shape BSTR,,BSTR,,,,
713,torch.autograd.functional.hessian,inputs,inputs to the function `func`.,inputs to the function PARAM,,,,,,
714,torch.autograd.grad,inputs,Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).,Inputs w r t,,,,,,
715,torch.nn.MaxUnpool2d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,,,,,,
716,torch.nn.MaxUnpool3d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,,,,,,
717,torch.utils.data.random_split,lengths,lengths of splits to be produced,lengths of splits to be produced,int,,1,"[0,inf)",,
718,torch.nn.functional.ctc_loss,target_lengths,(N) . Lengths of the targets,Lengths of the PARAM,int,,1,"[0,inf)",,
719,torch.clamp,min,lower-bound of the range to be clamped to,lower bound of the range to be clamped to,numeric,,,,,
720,torch.nn.RReLU,lower,lower bound of the uniform distribution. Default:  1/8,lower bound of the uniform distribution,numeric,,,,,
721,torch.mv,input,matrix to be multiplied,matrix to be multiplied,numeric,,,,,
722,torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",maximum number of iterations,int,,0,"[0,inf)",,
723,torch.nn.utils.prune.random_unstructured,module,module containing the tensor to prune,module containing the D_STRUCTURE to prune,,,,,,D_STRUCTURE
724,torch.addbmm,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,,,,,,
725,torch.baddbmm,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,,,,,,
726,torch.addmm,alpha,multiplier for mat1 @ mat2  ( alpha ),multiplier for PARAM PARAM BSTR,,,,,,
727,torch.nn.UpsamplingNearest2d,scale_factor,multiplier for spatial size.,multiplier for spatial PARAM,,,,,,
728,torch.nn.Upsample,scale_factor,multiplier for spatial size. Has to match input size if it is a tuple.,multiplier for spatial PARAM,,,,,,
729,torch.nn.BCEWithLogitsLoss,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,Must be a D_STRUCTURE with length equal to the number of classes,int,,,"[0,inf)",,D_STRUCTURE
730,torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,Must be executable in a shell process,,,,,,
731,torch.nn.utils.remove_spectral_norm,name,name of weight parameter,name of weight parameter,string,,0,,,
732,torch.onnx.export,output_names,"names to assign to the output nodes of the graph, in order",names to assign to the output nodes of the graph in order,string,,1,,,
733,torch.nn.utils.prune.random_unstructured,module,DD: nn.Module,nn Module,,,,,,
734,torch.nn.quantized.functional.conv3d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,non quantized bias D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
735,torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,,,,,,
736,torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,,,,,,
737,torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,,,,,,
738,torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,,,,,,
739,torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
740,torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
741,torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
742,torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
743,torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
744,torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",Note that in nearly all cases setting this option to CONSTANT_BOOL is not needed and often can be worked around in a much more efficient way,,,,,,
745,torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,,,,,,
746,torch.nn.ConvTranspose3d,groups,Number of blocked connections from input channels to output channels. Default: 1,Number of blocked connections from input channels to output channels,int,,0,"[0,inf)",,
747,torch.nn.ConvTranspose1d,out_channels,Number of channels produced by the convolution,Number of channels produced by the convolution,int,,0,"[0,inf)",,
748,torch.nn.ConvTranspose2d,out_channels,Number of channels produced by the convolution,Number of channels produced by the convolution,int,,0,"[0,inf)",,
749,torch.tensordot,dims,number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively,number of dimensions to contract or explicit D_STRUCTURE of dimensions for QSTR respectively,int,,0,"[0,inf)",,
750,torch.nn.AdaptiveLogSoftmaxWithLoss,in_features,Number of features in the input tensor,Number of features in the input D_STRUCTURE,int,,0,"[0,inf)",,
751,torch.nn.init.dirac_,groups,number of groups in the conv layer (default: 1),number of groups in the conv layer default CONSTANT_NUM,int,,0,"[0,inf)",,
752,torch.logspace,steps,number of points to sample between `start` and `end`. Default: `100`.,number of points to sample between PARAM and PARAM,int,,0,"[0,inf)",,
753,torch.addmm,alpha,"DD: Number, optional",Number optional,numeric,,,,,
754,torch.addbmm,beta,"DD: Number, optional",Number optional,numeric,,,,,
755,torch.baddbmm,beta,"DD: Number, optional",Number optional,numeric,,,,,
756,torch.utils.checkpoint.checkpoint,preserve_rng_state,Omit stashing and restoring the RNG state during each checkpoint.,Omit stashing and restoring the RNG state during each checkpoint,,,,,,
757,torch.nn.functional.ctc_loss,target_lengths,(N) . Lengths of the targets,ONE_WORD BSTR,,BSTR,,,,
758,torch.cuda.comm.broadcast_coalesced,tensors,DD: sequence,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
759,torch.utils.data.random_split,lengths,DD: sequence,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
760,torch.sparse.mm,mat1,DD: SparseTensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
761,torch.sparse.addmm,mat1,DD: SparseTensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
762,torch.where,x,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
763,torch.add,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
764,torch.nn.utils.prune.custom_from_mask,mask,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
765,torch.bitwise_not,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
766,torch.lu_unpack,LU_pivots,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
767,torch.where,y,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
768,torch.digamma,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
769,torch.ge,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
770,torch.bincount,weights,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
771,torch.reshape,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
772,torch.std_mean,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
773,torch.atan2,other,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
774,torch.unique,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
775,torch.matrix_rank,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
776,torch.distributed.recv,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
777,torch.ormqr,input3,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
778,torch.nn.functional.cosine_similarity,x1,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
779,torch.nn.functional.log_softmax,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
780,torch.logical_or,other,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
781,torch.mean,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
782,torch.dist,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
783,torch.gather,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
784,torch.normal,std,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
785,torch.solve,A,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
786,torch.ne,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
787,torch.distributed.all_reduce,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
788,torch.is_complex,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
789,torch.sinh,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
790,torch.addmv,vec,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
791,torch.normal,std2,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
792,torch.distributed.send,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
793,torch.mode,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
794,torch.ger,vec2,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
795,torch.rot90,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
796,torch.round,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
797,torch.log1p,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
798,torch.renorm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
799,torch.gt,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
800,torch.acos,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
801,torch.distributed.isend,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
802,torch.nn.functional.cross_entropy,target,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
803,torch.log10,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
804,torch.rfft,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
805,torch.cummin,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
806,torch.fmod,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
807,torch.addr,vec1,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
808,torch.poisson,input*,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
809,torch.ormqr,input2,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
810,torch.randn_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
811,torch.median,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
812,torch.diagflat,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
813,torch.cummax,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
814,torch.imag,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
815,torch.bmm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
816,torch.distributed.scatter,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
817,torch.nn.functional.grid_sample,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
818,torch.empty_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
819,torch.sum,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
820,torch.cholesky_solve,input2,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
821,torch.mv,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
822,torch.orgqr,input2,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
823,torch.chain_matmul,*matrices,DD: Tensors...,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
824,torch.unique,return_inverse,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
825,torch.std,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
826,torch.var,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
827,torch.unique,sorted,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
828,torch.std_mean,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
829,torch.var_mean,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
830,torch.std2,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
831,torch.var,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
832,torch.prod,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
833,torch.set_grad_enabled,mode,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
834,torch.renorm,p,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,,
835,torch.pinverse,rcond,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,,
836,torch.renorm,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
837,torch.nn.AdaptiveLogSoftmaxWithLoss,in_features,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
838,torch.logspace,steps,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
839,torch.nn.ConvTranspose1d,out_channels,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
840,torch.nn.Softmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
841,torch.nn.ConvTranspose2d,out_channels,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
842,torch.distributed.broadcast_multigpu,src,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
843,torch.trapz,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
844,torch.transpose,dim1,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
845,torch.matrix_power,n,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
846,torch.stft,n_fft,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
847,torch.chunk,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
848,torch.cumsum,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
849,torch.argmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
850,torch.nn.utils.prune.random_structured,amount,DD: int or float,ONE_WORD D_TYPE,D_TYPE,,,,,
851,torch.zeros,*size,DD: int...,ONE_WORD D_TYPE,D_TYPE,,,,,
852,torch.rand,*size,DD: int...,ONE_WORD D_TYPE,D_TYPE,,,,,
853,torch.index_select,index,DD: LongTensor,ONE_WORD D_TYPE,D_TYPE,,,,,
854,torch.gather,index,DD: LongTensor,ONE_WORD D_TYPE,D_TYPE,,,,,
855,torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,DD: str,ONE_WORD D_TYPE,D_TYPE,,,,,
856,torch.nn.utils.prune.ln_structured,name,DD: str,ONE_WORD D_TYPE,D_TYPE,,,,,
857,torch.hub.load_state_dict_from_url,url,DD: string,ONE_WORD D_TYPE,D_TYPE,,,,,
858,torch.set_rng_state,new_state,DD: torch.ByteTensor,ONE_WORD D_TYPE,D_TYPE,,,,,
859,torch.utils.data.random_split,dataset,DD: Dataset,ONE_WORD Dataset,,,,,,
860,torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",ONE_WORD DEPRECATED,,,,,,
861,torch.nn.LocalResponseNorm,beta,exponent. Default: 0.75,ONE_WORD exponent,,,,,,
862,torch.autograd.functional.hvp,func,DD: function,ONE_WORD function,,,,,,
863,torch.autograd.functional.vjp,func,DD: function,ONE_WORD function,,,,,,
864,torch.nn.functional.log_softmax,input,input,ONE_WORD input,,,,,,
865,torch.nn.parallel.data_parallel,module,DD: Module,ONE_WORD Module,,,,,,
866,torch.clamp,max,DD: Number,ONE_WORD Number,numeric,,,,,
867,torch.arange,start,DD: Number,ONE_WORD Number,numeric,,,,,
868,torch.clamp,min,DD: Number,ONE_WORD Number,numeric,,,,,
869,torch.arange,end,DD: Number,ONE_WORD Number,numeric,,,,,
870,torch.div,other,DD: Number,ONE_WORD Number,numeric,,,,,
871,torch.hub.load,**kwargs,DD: optional,ONE_WORD optional,,,,,,
872,torch.hub.load,*args,DD: optional,ONE_WORD optional,,,,,,
873,torch.nn.init.dirac_,groups,DD: optional,ONE_WORD optional,,,,,,
874,torch.nn.Conv2d,padding_mode,"`'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`",ONE_WORD QSTR,,,,,QSTR,
875,torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,ONE_WORD QSTR,,,,,QSTR,
876,torch.svd,compute_uv,option whether to compute U and V or not,option whether to compute U and V or not,bool,,0,,,
877,torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",optional output D_STRUCTURE,,,,,,D_STRUCTURE
878,torch.nn.init.orthogonal_,gain,optional scaling factor,optional scaling factor,,,,,,
879,torch.bincount,weights,"optional, weight for each value in the input tensor. Should be of same size as input tensor.",optional weight for each value in the PARAM D_STRUCTURE,numeric,,,,,
880,torch.distributed.scatter,tensor,Output tensor.,Output D_STRUCTURE,,,,,,D_STRUCTURE
881,torch.cuda.comm.gather,destination,"output device (-1 means CPU, default: current device)",output device CONSTANT_NUM means CPU default current device,,,,,,
882,torch.nn.UpsamplingBilinear2d,size,output spatial sizes,output spatial sizes,int,,,"[0,inf)",,
883,torch.nn.Upsample,size,output spatial sizes,output spatial sizes,int,,,"[0,inf)",,
884,torch.nn.UpsamplingNearest2d,size,output spatial sizes,output spatial sizes,int,,,"[0,inf)",,
885,torch.nn.MaxUnpool3d,padding,Padding that was added to the input,Padding that was added to the input,,,,,,
886,torch.nn.ConvTranspose3d,padding,`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0,PARAM BSTR padding zero padding will be added to both sides of each dimension in the input,,,,,,
887,torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",PARAM BSTR padding zero padding will be added to both sides of each dimension in the PARAM,,,,,,
888,torch.cholesky_solve,input2,"input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",PARAM matrix u of size BSTR where is zero of more batch dimensions composed of PARAM or lower triangular Cholesky factor,numeric,BSTR,,,,
889,torch.solve,A,"input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",PARAM square matrix of size BSTR where is zero or more batch dimensions,numeric,BSTR,,,,
890,torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",PARAM will be a sparse D_STRUCTURE,,,,,,
891,torch.nn.utils.prune.ln_structured,name,parameter name within `module` on which pruning will act.,parameter name within PARAM on which pruning will act,string,,0,,,
892,torch.quantization.quantize_qat,run_args,positional arguments for run_fn,positional arguments for PARAM,,,,,,
893,torch.quantization.quantize,run_args,positional arguments for run_fn,positional arguments for PARAM,,,,,,
894,torch.distributed.barrier,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
895,torch.distributed.get_rank,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
896,torch.distributed.broadcast_multigpu,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
897,torch.distributed.send,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
898,torch.distributed.all_gather,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
899,torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",QSTR matrix will be a sparse D_STRUCTURE,,,,,,
900,torch.nn.functional.kl_div,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,QSTR no reduction will be applied QSTR the sum of the output will be divided by the batchsize QSTR the output will be summed QSTR the output will be divided by the number of elements in the output Default QSTR,,,,,QSTR,
901,torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,QSTR,
902,torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,QSTR,
903,torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,QSTR,
904,torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,QSTR,
905,torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,
906,torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,
907,torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,
908,torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,
909,torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",quantity of parameters to prune,int,,0,"[0,inf)",,
910,torch.nn.quantized.functional.conv3d,input,"quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",quantized input D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
911,torch.nn.quantized.functional.conv2d,input,"quantized input tensor of shape (minibatch , in _channels , iH , iW) ",quantized input D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
912,torch.cuda.memory_stats,device,"selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",Returns statistics for the current device given by current_device if QSTR is QSTR BSTR,,,,,,
913,torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",See Notes for more details regarding sparse gradients,,,,,,
914,torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",See Notes under torch nn Embedding for more details regarding sparse gradients,,,,,,
915,torch.cuda.memory_stats,device,"selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",selected device,,,,,,
916,torch.bincount,weights,"optional, weight for each value in the input tensor. Should be of same size as input tensor.",Should be of same size as PARAM D_STRUCTURE,,,,,,
917,torch.nn.Linear,in_features,size of each input sample,size of each input sample,int,,,"[0,inf)",,
918,torch.nn.Bilinear,in2_features,size of each second input sample,size of each second input sample,int,,,"[0,inf)",,
919,torch.stft,n_fft,size of Fourier transform,size of Fourier transform,int,,,"[0,inf)",,
920,torch.nn.quantized.functional.avg_pool2d,kernel_size,"size of the pooling region. Can be a single number or a tuple (kH, kW)",size of the pooling region,int,,,"[0,inf)",,
921,torch.nn.PoissonNLLLoss,eps,Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8,Small value to avoid evaluation of log BSTR when PARAM CONSTANT_BOOL,numeric,,,,,
922,torch.nn.functional.poisson_nll_loss,eps,Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8,Small value to avoid evaluation of log BSTR when PARAM CONSTANT_BOOL,numeric,,,,,
923,torch.distributed.broadcast_multigpu,src,Source rank.,Source rank,,,,,,
924,torch.distributed.recv,src,Source rank. Will receive from any process if unspecified.,Source rank,,,,,,
925,torch.nn.Conv1d,dilation,Spacing between kernel elements. Default: 1,Spacing between kernel elements,,,,,,
926,torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
927,torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
928,torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
929,torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
930,torch.nn.functional.kl_div,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,Specifies the reduction to apply to the output QSTR QSTR QSTR QSTR,,,,,QSTR,
931,torch.nn.quantized.functional.conv3d,groups,"split input into groups, in _channels  should be divisible by the number of groups. Default: 1",split PARAM into groups in channels should be divisible by the number of groups,,,,,,
932,torch.nn.Conv3d,stride,Stride of the convolution. Default: 1,Stride of the convolution,int,,,"[0,inf)",,
933,torch.nn.MaxUnpool2d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,Stride of the max pooling window,int,,,"[0,inf)",,
934,torch.nn.MaxUnpool3d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,Stride of the max pooling window,int,,,"[0,inf)",,
935,torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",stride of the pooling operation,int,,,"[0,inf)",,
936,torch.nn.SyncBatchNorm,process_group,synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world,synchronization of stats happen within each process group individually,,,,,,
937,torch.distributed.send,tag,Tag to match send with remote recv,Tag to match send with remote recv,,,,,,
938,torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,The compiler executable name to check e g,string,,,,,
939,torch.index_select,index,the 1-D tensor containing the indices to index,the CONSTANT_NUM D D_STRUCTURE containing the indices to index,int,,CONSTANT_NUM,,,D_STRUCTURE
940,torch.hub.load,*args,the corresponding args for callable model.,the corresponding args for callable PARAM,,,,,,
941,torch.hub.load,**kwargs,the corresponding kwargs for callable model.,the corresponding kwargs for callable PARAM,,,,,,
942,torch.normal,std,the tensor of per-element standard deviations,the D_STRUCTURE of per element standard deviations,numeric,,,,,D_STRUCTURE
943,torch.normal2,std,the tensor of per-element standard deviations,the D_STRUCTURE of per element standard deviations,numeric,,,,,D_STRUCTURE
944,torch.min,out,"the tuple of two output tensors (min, min_indices)",the D_STRUCTURE of two output D_STRUCTURE BSTR,,,,,,D_STRUCTURE
945,torch.lt,other,the tensor or value to compare,the D_STRUCTURE or value to compare,,,,,,D_STRUCTURE
946,torch.reshape,input,the tensor to be reshaped,the D_STRUCTURE to be reshaped,,,,,,D_STRUCTURE
947,torch.ge,input,the tensor to compare,the D_STRUCTURE to compare,,,,,,D_STRUCTURE
948,torch.ne,input,the tensor to compare,the D_STRUCTURE to compare,,,,,,D_STRUCTURE
949,torch.gt,input,the tensor to compare,the D_STRUCTURE to compare,,,,,,D_STRUCTURE
950,torch.logical_or,other,the tensor to compute OR with,the D_STRUCTURE to compute OR with,,,,,,D_STRUCTURE
951,torch.digamma,input,the tensor to compute the digamma function on,the D_STRUCTURE to compute the digamma function on,,,,,,D_STRUCTURE
952,torch.nn.quantized.functional.conv3d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,The D_STRUCTURE type must be D_TYPE,D_TYPE,,,,,D_STRUCTURE
953,torch.set_default_tensor_type,t,the floating point tensor type or its name,the D_TYPE D_STRUCTURE type or its name,D_TYPE,,,,,D_STRUCTURE
954,torch.randperm,dtype,the desired data type of returned tensor. Default: `torch.int64`.,the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
955,torch.sparse.sum,dtype,the desired data type of returned Tensor. Default: dtype of `input`.,the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
956,torch.triu_indices,dtype,"the desired data type of returned tensor. Default: if `None`, `torch.long`.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
957,torch.empty_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
958,torch.zeros_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
959,torch.linspace,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
960,torch.ones,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
961,torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
962,torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
963,torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,
964,torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,
965,torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,
966,torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,
967,torch.full,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,,
968,torch.set_rng_state,new_state,The desired state,The desired state,,,,,,
969,torch.cuda.set_rng_state,device,"The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",The device to set the RNG state,,,,,,
970,torch.trapz,dim,"The dimension along which to integrate.By default, use the last dimension.",The dimension along which to integrate By default use the last dimension,int,,0,,,
971,torch.repeat_interleave,dim,"The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",The dimension along which to repeat values By default use the flattened PARAM D_STRUCTURE and return a flat outputarray,int,,0,,,
972,torch.var,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,int,,0;1,,,
973,torch.std,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,int,,0;1,,,
974,torch.cat,dim,the dimension over which the tensors are concatenated,the dimension over which the D_STRUCTURE are concatenated,int,,0,,,
975,torch.cumsum,dim,the dimension to do the operation over,the dimension to do the operation over,int,,0,,,
976,torch.kthvalue,dim,the dimension to find the kth value along,the dimension to find the kth value along,int,,0,,,
977,torch.argmax,dim,"the dimension to reduce. If `None`, the argmax of the flattened input is returned.",the dimension to reduce,int,,0,,,
978,torch.renorm,dim,the dimension to slice over to get the sub-tensors,the dimension to slice over to get the sub D_STRUCTURE,int,,0,,,
979,torch.topk,dim,the dimension to sort along,the dimension to sort along,int,,0,,,
980,torch.fmod,input,the dividend,the dividend,,,,,,
981,torch.remainder,other,the divisor that may be either a number or a Tensor of the same shape as the dividend,the divisor that may be either a number or a D_STRUCTURE of the same shape as the dividend,int,,,,,D_STRUCTURE
982,torch.nn.TransformerEncoderLayer,dropout,the dropout value (default=0.1).,the dropout value default CONSTANT_FLOAT,float,,0,,,
983,torch.nn.TransformerDecoderLayer,dropout,the dropout value (default=0.1).,the dropout value default CONSTANT_FLOAT,float,,0,,,
984,torch.arange,end,the ending value for the set of points,the ending value for the set of points,,,,,,
985,torch.jit.load,_extra_files,The extra filenames given in the map would be loaded and their content would be stored in the provided map.,The extra filenames given in the map would be loaded and their content would be stored in the provided map,,,,,,
986,torch.bmm,input,the first batch of matrices to be multiplied,the first batch of matrices to be multiplied,numeric,,,,,
987,torch.add,input,the first input tensor,the first input D_STRUCTURE,,,,,,D_STRUCTURE
988,torch.sparse.mm,mat1,the first sparse matrix to be multiplied,the first sparse matrix to be multiplied,numeric,,,,,
989,torch.addr,vec1,the first vector of the outer product,the first D_STRUCTURE of the outer product,,,,,,D_STRUCTURE
990,torch.distributed.all_reduce,tensor,Input and output of the collective. The function operates in-place.,The function operates in place,,,,,,
991,torch.gather,index,the indices of elements to gather,the indices of elements to gather,int,,1,,,
992,torch.matrix_rank,input,the input 2-D tensor,the input CONSTANT_NUM D D_STRUCTURE,,,CONSTANT_NUM,,,D_STRUCTURE
993,torch.unique,input,the input tensor,the input D_STRUCTURE,,,,,,D_STRUCTURE
994,torch.bitwise_not,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
995,torch.std_mean,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
996,torch.mean,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
997,torch.dist,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
998,torch.sinh,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
999,torch.mode,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1000,torch.rot90,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1001,torch.round,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1002,torch.log1p,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1003,torch.renorm,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1004,torch.acos,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1005,torch.log10,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1006,torch.cummin,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1007,torch.median,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1008,torch.diagflat,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1009,torch.cummax,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1010,torch.imag,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1011,torch.sum,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1012,torch.poisson,input*,the input tensor containing the rates of the Poisson distribution,the input D_STRUCTURE containing the rates of the Poisson distribution,,,,"[0,1]",,D_STRUCTURE
1013,torch.rfft,input,the input tensor of at least `signal_ndim` dimensions,the input D_STRUCTURE of at least PARAM dimensions,,,&PARAM,,,D_STRUCTURE
1014,torch.nn.TransformerDecoder,norm,the layer normalization component (optional).,the layer normalization component BSTR,,,,,,
1015,torch.ormqr,input3,the matrix to be multiplied.,the matrix to be multiplied,numeric,,,,,
1016,torch.nn.parallel.data_parallel,module,the module to evaluate in parallel,the module to evaluate in parallel,,,,,,
1017,torch.eye,m,the number of columns with default being `n`,the number of columns with default being QSTR,int,,0,"[0,inf)",,
1018,torch.nn.TransformerEncoderLayer,d_model,the number of expected features in the input (required).,the number of expected features in the input BSTR,int,,0,"[0,inf)",,
1019,torch.nn.RNN,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,int,,0,"[0,inf)",,
1020,torch.nn.GRUCell,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,int,,0,"[0,inf)",,
1021,torch.nn.GRU,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,int,,0,"[0,inf)",,
1022,torch.nn.TransformerDecoderLayer,nhead,the number of heads in the multiheadattention models (required).,the number of heads in the multiheadattention models BSTR,int,,0,"[0,inf)",,
1023,torch.roll,shifts,"The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",The number of places by which the elements of the D_STRUCTURE are shifted,int,,0,"[0,inf)",,
1024,torch.nn.TransformerDecoder,num_layers,the number of sub-decoder-layers in the decoder (required).,the number of sub decoder layers in the decoder BSTR,int,,0,"[0,inf)",,
1025,torch.nn.Transformer,num_decoder_layers,the number of sub-decoder-layers in the decoder (default=6).,the number of sub decoder layers in the decoder default CONSTANT_NUM,int,,0,"[0,inf)",,
1026,torch.div,other,the number to be divided to each element of `input`,the number to be divided to each element of PARAM,int,,0,"[0,inf)",,
1027,torch.can_cast,from,The original `torch.dtype`.,The original D_TYPE,D_TYPE,,,,,
1028,torch.addmv,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1029,torch.bernoulli,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1030,torch.atan2,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1031,torch.normal,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1032,torch.eye,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1033,torch.mm,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1034,torch.ones,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1035,torch.remainder,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1036,torch.div,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1037,torch.addmm,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1038,torch.var,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1039,torch.histc,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1040,torch.multinomial,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1041,torch.randperm,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1042,torch.neg,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1043,torch.matmul,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1044,torch.mv,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1045,torch.diag,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1046,torch.pow,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1047,torch.reciprocal,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1048,torch.conj,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1049,torch.erf,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1050,torch.acos,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1051,torch.nn.functional.normalize,out,"the output tensor. If `out` is used, this operation won't be differentiable.",the output D_STRUCTURE,,,,,,D_STRUCTURE
1052,torch.eig,out,the output tensors,the output D_STRUCTURE,,,,,,D_STRUCTURE
1053,torch.svd,out,the output tuple of tensors,the output D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
1054,torch.nn.Embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,,,,,,
1055,torch.nn.EmbeddingBag,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,,,,,,
1056,torch.lu_unpack,LU_pivots,the packed LU factorization pivots,the packed LU factorization pivots,,,,,,
1057,torch.renorm,p,the power for the norm computation,the power for the norm computation,int,,,,,
1058,torch.matrix_power,n,the power to raise the matrix to,the power to raise the matrix to,int,,,,,
1059,torch.distributed.barrier,group,The process group to work on,The process group to work on,,,,,,
1060,torch.distributed.get_rank,group,The process group to work on,The process group to work on,,,,,,
1061,torch.distributed.broadcast_multigpu,group,The process group to work on,The process group to work on,,,,,,
1062,torch.distributed.send,group,The process group to work on,The process group to work on,,,,,,
1063,torch.distributed.all_gather,group,The process group to work on,The process group to work on,,,,,,
1064,torch.is_complex,input,the PyTorch tensor to test,the PyTorch D_STRUCTURE to test,,,,,,D_STRUCTURE
1065,torch.max,out,"the result tuple of two output tensors (max, max_indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,,,,,D_STRUCTURE
1066,torch.cummax,out,"the result tuple of two output tensors (values, indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,,,,,D_STRUCTURE
1067,torch.transpose,dim1,the second dimension to be transposed,the second dimension to be transposed,int,,0,0,,
1068,torch.atan2,other,the second input tensor,the second PARAM D_STRUCTURE,,,,,,D_STRUCTURE
1069,torch.randn_like,input,the size of `input` will determine size of the output tensor.,the size of QSTR will determine size of the output D_STRUCTURE,int,,,"[0,inf)",,
1070,torch.empty_like,input,the size of `input` will determine size of the output tensor.,the size of QSTR will determine size of the output D_STRUCTURE,int,,,"[0,inf)",,
1071,torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",the size of the padding,int,,,"[0,inf)",,
1072,torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",the size of the padding,int,,,"[0,inf)",,
1073,torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",the size of the padding,int,,,"[0,inf)",,
1074,torch.nn.Fold,kernel_size,the size of the sliding blocks,the size of the sliding blocks,int,,,"[0,inf)",,
1075,torch.nn.MaxPool2d,kernel_size,the size of the window to take a max over,the size of the window to take a max over,int,,,"[0,inf)",,
1076,torch.nn.MaxPool1d,kernel_size,the size of the window to take a max over,the size of the window to take a max over,int,,,"[0,inf)",,
1077,torch.nn.FractionalMaxPool2d,kernel_size,"the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)",the size of the window to take a max over,int,,,"[0,inf)",,
1078,torch.gather,input,the source tensor,the source D_STRUCTURE,,,,,,D_STRUCTURE
1079,torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",the spacing between kernel elements,,,,,,
1080,torch.arange,start,the starting value for the set of points. Default: `0`.,the starting value for the set of points,,,,,,
1081,torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",the stride of the convolving kernel,int,,,"[0,inf)",,
1082,torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",the stride of the convolving kernel,int,,,"[0,inf)",,
1083,torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",the stride of the window,int,,,"[0,inf)",,
1084,torch.nn.MaxPool3d,stride,the stride of the window. Default value is `kernel_size`,the stride of the window,int,,,"[0,inf)",,
1085,torch.nn.functional.affine_grid,size,"the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",the target output image size,int,,1,"[0,inf)",,
1086,torch.nn.AdaptiveMaxPool1d,output_size,the target output size H,the target output size H,,[H],,"[0,inf)",,
1087,torch.ormqr,input2,the tau from `torch.geqrf()`.,the tau from torch geqrf,,,,,,
1088,torch.orgqr,input2,the tau from `torch.geqrf()`.,the tau from torch geqrf,,,,,,
1089,torch.nn.Upsample,mode,"the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`",the upsampling algorithm one of QSTR,,,,,,
1090,torch.nn.init.constant_,val,the value to fill the tensor with,the value to fill the D_STRUCTURE with,,,,,,
1091,torch.set_grad_enabled,mode,"Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients.",This can be used to conditionally enable gradients,,,,,,
1092,torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",This is useful for preventing data type overflows,,,,,,
1093,torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",This is useful for preventing data type overflows,,,,,,
1094,torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",This only has an effect when PARAM is QSTR,,,,,,
1095,torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",This only has effect when PARAM is QSTR,,,,,,
1096,torch.hamming_window,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1097,torch.sparse_coo_tensor,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1098,torch.randperm,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1099,torch.empty_strided,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1100,torch.cuda.set_rng_state,device,"DD: torch.device or int, optional",torch device or D_TYPE optional,D_TYPE,,,,,
1101,torch.cuda.memory_stats,device,"DD: torch.device or int, optional",torch device or D_TYPE optional,D_TYPE,,,,,
1102,torch.poisson,generator,"DD: `torch.Generator`, optional",torch Generator optional,,,,,,
1103,torch.full,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,,
1104,torch.nn.functional.affine_grid,size,DD: torch.Size,torch Size,int,,1,"[0,inf)",,
1105,torch.full,layout,DF: torch.strided,torch strided,,,,,,
1106,torch.rand,layout,DF: torch.strided,torch strided,,,,,,
1107,torch.linspace,layout,DF: torch.strided,torch strided,,,,,,
1108,torch.set_default_tensor_type,t,DD: type or string,type or D_TYPE,D_TYPE;torch.dtype,,,,,
1109,torch.clamp,max,upper-bound of the range to be clamped to,upper bound of the range to be clamped to,,,,,,
1110,torch.hub.load_state_dict_from_url,url,URL of the object to download,URL of the object to download,string,,,,,
1111,torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",use PARAM export the PARAM in aten mode,,,,,,
1112,torch.nn.MaxPool3d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later",Useful for torch nn MaxUnpool3d later,,,,,,
1113,torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",Useful to pass to nn MaxUnpool2d,,,,,,
1114,torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",Useful to pass to nn MaxUnpool3d,,,,,,
1115,torch.nn.utils.rnn.pad_sequence,padding_value,value for padded elements. Default: 0.,value for padded elements,,,,,,
1116,torch.where,y,values selected at indices where `condition` is `False`,values selected at indices where PARAM is CONSTANT_BOOL,,,,,,
1117,torch.where,x,values selected at indices where `condition` is `True`,values selected at indices where PARAM is CONSTANT_BOOL,,,,,,
1118,torch.addmv,vec,vector to be multiplied,D_STRUCTURE to be multiplied,,,1,,,D_STRUCTURE
1119,torch.nn.AvgPool3d,count_include_pad,"when True, will include the zero-padding in the averaging calculation",when CONSTANT_BOOL will include the zero PARAM in the averaging calculation,bool,,0,,,
1120,torch.nn.functional.avg_pool2d,count_include_pad,"when True, will include the zero-padding in the averaging calculation. Default: `True`",when CONSTANT_BOOL will include the zero PARAM in the averaging calculation,bool,,0,,,
1121,torch.nn.functional.avg_pool3d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor in the formula to compute the output shape,bool,,0,,,
1122,torch.nn.quantized.functional.avg_pool2d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`",when CONSTANT_BOOL will use ceil instead of floor in the formula to compute the output shape,bool,,0,,,
1123,torch.nn.MaxPool3d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,bool,,0,,,
1124,torch.nn.MaxPool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,bool,,0,,,
1125,torch.nn.AvgPool2d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,bool,,0,,,
1126,torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1127,torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1128,torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1129,torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",When reached the iteration process is hard stopped and the current approximation of eigenpairs is returned,,,,,,
1130,torch.var_mean,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1131,torch.var,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1132,torch.prod,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1133,torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,whether the output D_STRUCTURE have PARAM retained or not,bool,,0,,,
1134,torch.combinations,with_replacement,whether to allow duplication in combination,whether to allow duplication in combination,bool,,0,,,
1135,torch.unique,return_inverse,Whether to also return the indices for where elements in the original input ended up in the returned unique list.,Whether to also return the indices for where elements in the original PARAM ended up in the returned unique D_STRUCTURE,bool,,0,,,
1136,torch.hub.list,force_reload,whether to discard the existing cache and force a fresh download. Default is False.,whether to discard the existing cache and force a fresh download,bool,,0,,,
1137,torch.multinomial,replacement,whether to draw with replacement or not,whether to draw with replacement or not,bool,,0,,,
1138,torch.unique,sorted,Whether to sort the unique elements in ascending order before returning as output.,Whether to sort the unique elements in ascending order before returning as output,bool,,0,,,
1139,torch.std,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,bool,,0,,,
1140,torch.var,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,bool,,0,,,
1141,torch.std_mean,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,bool,,0,,,
1142,torch.std2,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,bool,,0,,,
1143,torch.autograd.grad,inputs,Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).,which the gradient will be returned and not accumulated into grad,bool,,0,,,
1144,torch.distributed.recv,src,Source rank. Will receive from any process if unspecified.,Will receive from any process if unspecified,,,,,,
1145,torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",You might want to disable this if for example your network contains non deterministic ops or if you are sure that the network is correct despite a checker failure,,,,,,
1146,torch.jit.save,_extra_files,Map from filename to contents which will be stored as part of 'f'.,Map from filename to contents which will be stored as part of QSTR,,,,,,
1147,torch.einsum,*operands,The operands to compute the Einstein sum of.,The operands to compute the Einstein sum of,,,,,,
1148,torch.nn.init.uniform_,a,the lower bound of the uniform distribution,the lower bound of the uniform distribution,numeric,,,,,
1149,torch.tensordot,a,Left tensor to contract,Left D_STRUCTURE to contract,,,,,,D_STRUCTURE
1150,torch.nn.Transformer,activation,"the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).",the activation function of encoder decoder intermediate layer relu or gelu default relu,,,,,,
1151,torch.nn.BatchNorm1d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module has learnable affine parameters,D_TYPE,,0,,,
1152,torch.nn.BatchNorm1d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1153,torch.nn.BatchNorm3d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module has learnable affine parameters,D_TYPE,,0,,,
1154,torch.nn.BatchNorm3d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1155,torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
1156,torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",Geometrically we consider the pixels of the PARAM and output as squares rather than points,,,,,,
1157,torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the center points of their corner pixels preserving the values at the corner pixels,bool,,0,,,
1158,torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the corner points of their corner pixels and the interpolation uses edge value padding for out of boundary values making this operation independent of PARAM PARAM when PARAM is kept the same,bool,,0,,,
1159,torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",This only has an effect when PARAM is QSTR,,,,,,
1160,torch.nn.LocalResponseNorm,alpha,multiplicative factor. Default: 0.0001,Default CONSTANT_NUM,bool,,0,,,
1161,torch.nn.LocalResponseNorm,alpha,multiplicative factor. Default: 0.0001,multiplicative factor,numeric,,,,,
1162,torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE it represents the absolute number of parameters to prune,D_TYPE,,0,"[0,inf)",,
1163,torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE should be between CONSTANT_NUM and represent the fraction of parameters to prune,D_TYPE,,0,"[0,inf)",,
1164,torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",quantity of parameters to prune,int,,0,"[0,inf)",,
1165,torch.distributed.all_gather,async_op,Whether this op should be an async op,Whether this op should be an async op,bool,,0,,,
1166,torch.quantize_per_channel,axis,dimension on which apply per-channel quantization,dimension on which apply per channel quantization,int,,0,,,
1167,torch.tensordot,b,Right tensor to contract,Right D_STRUCTURE to contract,,,,,,D_STRUCTURE
1168,torch.nn.utils.rnn.pack_padded_sequence,batch_first,"if `True`, the input is expected in `B x T x *` format.",if CONSTANT_BOOL the PARAM is expected in B x T x format,bool,,0,,,
1169,torch.addr,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,numeric,,,,,
1170,torch.nn.ConvTranspose3d,bias,"If `True`, adds a learnable bias to the output. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1171,torch.nn.ConvTranspose3d,bias,"If `True`, adds a learnable bias to the output. Default: `True`",If CONSTANT_BOOL adds a learnable bias to the output,bool,,0,,,
1172,torch.nn.quantized.functional.conv2d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,non quantized bias D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
1173,torch.nn.quantized.functional.conv2d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,The D_STRUCTURE type must be D_TYPE,D_TYPE,,,,,D_STRUCTURE
1174,torch.nn.functional.avg_pool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
1175,torch.nn.functional.avg_pool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape. Default: `False`",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,bool,,0,,,
1176,torch.chunk,chunks,number of chunks to return,number of chunks to return,int,,0,"[0,inf)",,
1177,torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1178,torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL the Hessian will be computed in a differentiable manner,bool,,0,,,
1179,torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,,,,,,
1180,torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1181,torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL both the output and result will be computed in a differentiable way,bool,,0,,,
1182,torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,,,,,,
1183,torch.argsort,descending,controls the sorting order (ascending or descending),controls the sorting order BSTR,,,,,,
1184,torch.empty_like,device,"the desired device of returned tensor. Default: if `None`, defaults to the device of `input`.",Default if QSTR defaults to the device of PARAM,,,,,,
1185,torch.empty_like,device,"the desired device of returned tensor. Default: if `None`, defaults to the device of `input`.",the desired device of returned D_STRUCTURE,,,,,,
1186,torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,
1187,torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,
1188,torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",device will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,
1189,torch.triu,diagonal,the diagonal to consider,the diagonal to consider,,,,,,
1190,torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",Can be a single number or a D_STRUCTURE BSTR,int,,0;1,,,D_STRUCTURE
1191,torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",Default CONSTANT_NUM,bool,,0,,,
1192,torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",the spacing between kernel elements,,,,,,
1193,torch.nn.CosineSimilarity,dim,Dimension where cosine similarity is computed. Default: 1,Default CONSTANT_NUM,bool,,0,,,
1194,torch.nn.CosineSimilarity,dim,Dimension where cosine similarity is computed. Default: 1,Dimension where cosine similarity is computed,int,,0,,,
1195,torch.nn.functional.gumbel_softmax,dim,A dimension along which softmax will be computed. Default: -1.,A dimension along which softmax will be computed,int,,0,,,
1196,torch.nn.functional.gumbel_softmax,dim,A dimension along which softmax will be computed. Default: -1.,Default CONSTANT_NUM,bool,,0,,,
1197,torch.sum,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,int,,0;1,,,
1198,torch.unsqueeze,dim,the index at which to insert the singleton dimension,the index at which to insert the singleton dimension,int,,0,,,
1199,torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,value used as an exponent to compute sizes of the clusters. Default: 4.0,Default CONSTANT_NUM,bool,,0,,,
1200,torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,value used as an exponent to compute sizes of the clusters. Default: 4.0,value used as an exponent to compute sizes of the clusters,,,,,,
1201,torch.utils.dlpack.from_dlpack,dlpack,a PyCapsule object with the dltensor,a PyCapsule object with the dltensor,,,,,,
1202,torch.distributed.send,dst,Destination rank.,Destination rank,int,,0,,,
1203,torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",Default if QSTR uses a global default see torch set_default_tensor_type,,,,,,
1204,torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",Only D_TYPE types are supported,,,,,,
1205,torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
1206,torch.eye,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,,,,,,
1207,torch.eye,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
1208,torch.rand_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,,,,,,
1209,torch.rand_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,torch.dtype,,,,,
1210,torch.nn.InstanceNorm3d,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,,,0,,,
1211,torch.nn.InstanceNorm3d,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_NUM,bool,,0,,,
1212,torch.utils.cpp_extension.load,extra_cflags,optional list of compiler flags to forward to the build.,optional D_STRUCTURE of compiler flags to forward to the build,,,,,,
1213,torch.nn.functional.poisson_nll_loss,full,"whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .",Default CONSTANT_BOOL PARAM log BSTR,bool,,0,,,
1214,torch.nn.functional.poisson_nll_loss,full,"whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .",whether to compute full loss i e to add the Stirling approximation term,bool,,0,,,
1215,torch.utils.checkpoint.checkpoint_sequential,functions,A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,A torch nn Sequential or the D_STRUCTURE of modules or functions BSTR to run sequentially,,,,,,D_STRUCTURE
1216,torch.multinomial,generator,a pseudorandom number generator for sampling,a pseudorandom number generator for sampling,,,,,,
1217,torch.lu,get_infos,"if set to `True`, returns an info IntTensor. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
1218,torch.lu,get_infos,"if set to `True`, returns an info IntTensor. Default: `False`",if set to CONSTANT_BOOL returns an info IntTensor,bool,,0,,,
1219,torch.autograd.gradgradcheck,grad_outputs,The gradients with respect to the function's outputs.,The gradients with respect to the function outputs,numeric,,,,,
1220,torch.distributed.get_world_size,group,The process group to work on,The process group to work on,,,,,,
1221,torch.distributed.reduce,group,The process group to work on,The process group to work on,,,,,,
1222,torch.nn.ConvTranspose2d,groups,Number of blocked connections from input channels to output channels. Default: 1,Default CONSTANT_NUM,bool,,0,,,
1223,torch.nn.ConvTranspose2d,groups,Number of blocked connections from input channels to output channels. Default: 1,Number of blocked connections from input channels to output channels,int,,0,"[0,inf)",,
1224,torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",Default CONSTANT_NUM,bool,,0,,,
1225,torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",Specifies a PARAM value that is ignored and does not contribute to the PARAM gradient,,,,,,
1226,torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",When PARAM is CONSTANT_BOOL the loss is averaged over non ignored targets,,,,,,
1227,torch.nn.CELU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,,,,,,
1228,torch.nn.CELU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
1229,torch.nn.functional.dropout3d,inplace,"If set to `True`, will do this operation in-place. Default: `False`",Default CONSTANT_BOOL,bool,,0,,,
1230,torch.nn.functional.dropout3d,inplace,"If set to `True`, will do this operation in-place. Default: `False`",If set to CONSTANT_BOOL will do this operation in place,bool,,0,,,
1231,torch.nn.RReLU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,,,,,,
1232,torch.nn.RReLU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
1233,torch.abs,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1234,torch.addcdiv,input,the tensor to be added,the D_STRUCTURE to be added,,,,,,D_STRUCTURE
1235,torch.addcmul,input,the tensor to be added,the D_STRUCTURE to be added,,,,,,D_STRUCTURE
1236,torch.clamp,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1237,torch.div,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1238,torch.ifft,input,the input tensor of at least `signal_ndim` `+ 1` dimensions,the input D_STRUCTURE of at least PARAM CONSTANT_NUM dimensions,,,PARAM +CONSTANT_NUM,,,D_STRUCTURE
1239,torch.le,input,the tensor to compare,the D_STRUCTURE to compare,,,,,,D_STRUCTURE
1240,torch.logical_and,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1241,torch.masked_select,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1242,torch.mm,input,the first matrix to be multiplied,the first matrix to be multiplied,numeric,,,,,
1243,torch.narrow,input,the tensor to narrow,the D_STRUCTURE to narrow,,,,,,D_STRUCTURE
1244,torch.neg,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1245,torch.nn.functional.avg_pool1d,input,"input tensor of shape (minibatch , in _channels , iW) ",input D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
1246,torch.nn.functional.conv_transpose2d,input,"input tensor of shape (minibatch , in _channels , iH , iW) ",input D_STRUCTURE of shape BSTR,,BSTR,,,,D_STRUCTURE
1247,torch.nonzero,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1248,torch.norm,input,the input tensor,the input D_STRUCTURE,,,,,,D_STRUCTURE
1249,torch.numel,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1250,torch.pinverse,input,"The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",The input D_STRUCTURE of size BSTR where is zero or more batch dimensions,,BSTR,,,,D_STRUCTURE
1251,torch.rand_like,input,the size of `input` will determine size of the output tensor.,the size of input will determine size of the output D_STRUCTURE,int,,,"[0,inf)",,
1252,torch.remainder,input,the dividend,the dividend,,,,,,
1253,torch.repeat_interleave,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1254,torch.sigmoid,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1255,torch.square,input,the input tensor.,the input D_STRUCTURE,,,,,,D_STRUCTURE
1256,torch.nn.LSTM,input_size,The number of expected features in the input x,The number of expected features in the input x,int,,0,"[0,inf)",,
1257,torch.nn.RNN,input_size,The number of expected features in the input x,The number of expected features in the input x,int,,0,"[0,inf)",,
1258,torch.autograd.gradcheck,inputs,inputs to the function,inputs to the function,,,,,,
1259,torch.utils.cpp_extension.load,is_python_module,"If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library.",If CONSTANT_BOOL BSTR imports the produced shared library as a Python module,bool,,0,,,
1260,torch.utils.cpp_extension.load,is_python_module,"If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library.",If CONSTANT_BOOL loads it into the process as a plain dynamic library,bool,,0,,,
1261,torch.kthvalue,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1262,torch.min,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1263,torch.sum,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,bool,,0,,,
1264,torch.nn.AvgPool1d,kernel_size,the size of the window,the size of the window,int,,,"[0,inf)",,
1265,torch.nn.Conv1d,kernel_size,Size of the convolving kernel,Size of the convolving kernel,int,,,"[0,inf)",,
1266,torch.nn.Softshrink,lambd,the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5,Default CONSTANT_NUM,bool,,0,,,
1267,torch.nn.Softshrink,lambd,the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5,the lambda BSTR value for the Softshrink formulation,,,,,,
1268,torch.empty_like,layout,"the desired layout of returned tensor. Default: if `None`, defaults to the layout of `input`.",Default if QSTR defaults to the layout of PARAM,,,,,,
1269,torch.empty_like,layout,"the desired layout of returned tensor. Default: if `None`, defaults to the layout of `input`.",the desired layout of returned D_STRUCTURE,,,,,,
1270,torch.linspace,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,,
1271,torch.linspace,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,,
1272,torch.rand,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,,
1273,torch.rand,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,,
1274,torch.jit.save,m,A `ScriptModule` to save.,A QSTR to save,,,,,,
1275,torch.quantization.swap_module,mapping,a dictionary that maps from nn module to nnq module,a D_STRUCTURE that maps from nn module to nnq module,,,,,,D_STRUCTURE
1276,torch.sparse.addmm,mat,a dense matrix to be added,a dense matrix to be added,numeric,,,,,
1277,torch.addmm,mat1,the first matrix to be multiplied,the first matrix to be multiplied,numeric,,,,,
1278,torch.nn.utils.clip_grad_norm_,max_norm,max norm of the gradients,max norm of the gradients,numeric,,,,,
1279,torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",Default is QSTR,,,,,,
1280,torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",See the description of the function above,,,,,,
1281,torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",select LOBPCG method,,,,,,
1282,torch.histc,min,lower end of the range (inclusive),lower end of the range BSTR,,,,,,
1283,torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,Can be set to QSTR for cumulative moving average i e,,,,,,
1284,torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,Default CONSTANT_NUM,bool,,0,,,
1285,torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,simple average,,,,,,
1286,torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,the value used for the running_mean and running_var computation,,,,,,
1287,torch.nn.InstanceNorm1d,momentum,the value used for the running_mean and running_var computation. Default: 0.1,Default CONSTANT_NUM,bool,,0,,,
1288,torch.nn.InstanceNorm1d,momentum,the value used for the running_mean and running_var computation. Default: 0.1,the value used for the running_mean and running_var computation,,,,,,
1289,torch.nn.utils.prune.remove,name,parameter name within `module` on which pruning will act.,parameter name within PARAM on which pruning will act,,,,,,
1290,torch.nn.utils.spectral_norm,name,name of weight parameter,name of weight parameter,string,,0,,,
1291,torch.nn.utils.weight_norm,name,name of weight parameter,name of weight parameter,string,,0,,,
1292,torch.cuda.set_rng_state,new_state,The desired state,The desired state,,,,,,
1293,torch.pca_lowrank,niter,"the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.",the number of subspace iterations to conduct niter must be a nonnegative D_TYPE and defaults to CONSTANT_NUM,int,,0,"[0,inf)",,
1294,torch.nn.functional.embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,bool,,0,,,
1295,torch.nn.functional.embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,,,,,,
1296,torch.is_tensor,obj,Object to test,Object to test,,,,,,
1297,torch.eq,other,the tensor or value to compare,the D_STRUCTURE or value to compare,D_TYPE,,,,,D_STRUCTURE
1298,torch.floor_divide,other,the denominator,the denominator,,,,,,
1299,torch.logical_and,other,the tensor to compute AND with,the D_STRUCTURE to compute AND with,,,,,,D_STRUCTURE
1300,torch.ne,other,the tensor or value to compare,the D_STRUCTURE or value to compare,D_TYPE,,,,,D_STRUCTURE
1301,torch.clamp,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1302,torch.gt,out,the output tensor that must be a BoolTensor,the output D_STRUCTURE that must be a D_TYPE,D_TYPE,,,,,D_STRUCTURE
1303,torch.log,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1304,torch.log1p,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1305,torch.logical_and,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1306,torch.logical_not,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1307,torch.logical_or,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1308,torch.logsumexp,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1309,torch.mode,out,"the result tuple of two output tensors (values, indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,,,,,D_STRUCTURE
1310,torch.round,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1311,torch.sin,out,the output tensor.,the output D_STRUCTURE,,,,,,D_STRUCTURE
1312,torch.symeig,out,"the output tuple of (Tensor, Tensor)",the output D_STRUCTURE of BSTR,,,,,,D_STRUCTURE
1313,torch.nn.functional.adaptive_avg_pool1d,output_size,the target output size (single integer),the target output size BSTR,numeric,,,"[0,inf)",,
1314,torch.autograd.grad,outputs,outputs of the differentiated function.,outputs of the differentiated function,,,,,,
1315,torch.distributions.kl.kl_divergence,p,A `Distribution` object.,A QSTR object,,,,,,
1316,torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,,,1,,,D_STRUCTURE
1317,torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,,,,
1318,torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",the size of the padding,int,,,"[0,inf)",,
1319,torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",Can be a single number or a D_STRUCTURE BSTR,int,,0;1,,,D_STRUCTURE
1320,torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",Default CONSTANT_NUM,bool,,0,,,
1321,torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",implicit zero paddings on both sides of the PARAM,,,,,,
1322,torch.nn.MaxUnpool2d,padding,Padding that was added to the input,padding that was added to the input,,,,,,
1323,torch.nn.functional.embedding,padding_idx,"If given, pads the output with the embedding vector at `padding_idx` (initialized to zeros) whenever it encounters the index.",If given pads the output with the embedding D_STRUCTURE at padding_idx BSTR whenever it encounters the index,,,,,,
1324,torch.hamming_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL return a symmetric window,bool,,0,,,
1325,torch.hamming_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL returns a window to be used as periodic function,bool,,0,,,
1326,torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,a PARAM of positive examples,numeric,,,,,
1327,torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,Must be a D_STRUCTURE with length equal to the number of classes,,,1,,,D_STRUCTURE
1328,torch.hub.load_state_dict_from_url,progress,whether or not to display a progress bar to stderr. Default: True,Default CONSTANT_BOOL,bool,,0,,,
1329,torch.hub.load_state_dict_from_url,progress,whether or not to display a progress bar to stderr. Default: True,whether or not to display a progress bar to stderr,bool,,0,,,
1330,torch.nn.utils.prune.global_unstructured,pruning_method,"a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has `PRUNING_TYPE='unstructured'`.",a valid pruning function from this module or a custom one implemented by the user that satisfies the implementation guidelines and has PRUNING_TYPE QSTR,,,,,,
1331,torch.quantization.propagate_qconfig_,qconfig_dict,"dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",D_STRUCTURE that maps from name or type of submodule to quantization configuration qconfig applies to all submodules of a given PARAM unless qconfig for the submodules are specified BSTR,,,,,,D_STRUCTURE
1332,torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
1333,torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1334,torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
1335,torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When reduce is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1336,torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
1337,torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1338,torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
1339,torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When reduce is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1340,torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,
1341,torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1342,torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,
1343,torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When reduce is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,
1344,torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,,
1345,torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override reduction,,,,,,
1346,torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,,
1347,torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
1348,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,Default QSTR,,,,,,
1349,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR no reduction will be applied,,,,,,
1350,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the output will be divided by the number of elements in the output,,,,,,
1351,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the output will be summed,,,,,,
1352,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the sum of the output will be divided by batchsize,,,,,,
1353,torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,Specifies the reduction to apply to the output QSTR QSTR QSTR QSTR,,,,,QSTR,
1354,torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,,
1355,torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override reduction,,,,,,
1356,torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,,
1357,torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
1358,torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,,,,,,
1359,torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override reduction,,,,,,
1360,torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,,,,,,
1361,torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,,,,,QSTR,
1362,torch.eye,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,bool,,0,,,
1363,torch.eye,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,,,,,,
1364,torch.sparse_coo_tensor,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,bool,,0,,,
1365,torch.sparse_coo_tensor,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,,,,,,
1366,torch.nn.functional.adaptive_max_pool3d,return_indices,whether to return pooling indices. Default: `False`,Default CONSTANT_BOOL,bool,,0,,,
1367,torch.nn.functional.adaptive_max_pool3d,return_indices,whether to return pooling indices. Default: `False`,whether to return pooling indices,bool,,0,,,
1368,torch.nn.MaxPool2d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later",if CONSTANT_BOOL will return the max indices along with the outputs,bool,,0,,,
1369,torch.nn.MaxPool2d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later",Useful for torch nn MaxUnpool2d later,,,,,,
1370,torch.triu_indices,row,number of rows in the 2-D matrix.,number of rows in the CONSTANT_NUM D matrix,int,,0,"[0,inf)",,
1371,torch.autograd.gradcheck,rtol,relative tolerance,relative tolerance,,,,,,
1372,torch.nn.quantized.functional.conv3d,scale,quantization scale for the output. Default: 1.0,Default CONSTANT_NUM,bool,,0,,,
1373,torch.nn.quantized.functional.conv3d,scale,quantization scale for the output. Default: 1.0,quantization scale for the output,,,,,,
1374,torch.cuda.manual_seed,seed,The desired seed.,The desired seed,,,,,,
1375,torch.rfft,signal_ndim,"the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3",the number of dimensions in each signal,int,,0,"[0,inf)",,
1376,torch.rfft,signal_ndim,"the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3",signal_ndim can only be CONSTANT_NUM,,,,,CONSTANT_NUM,
1377,torch.normal,size,a sequence of integers defining the shape of the output tensor.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,,1,,,D_STRUCTURE
1378,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,
1379,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,bool,,0,,,
1380,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,
1381,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field size_average is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,
1382,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,
1383,torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,
1384,torch.utils.cpp_extension.load,sources,A list of relative or absolute paths to C++ source files.,A D_STRUCTURE of relative or absolute paths to C source files,,,,,,D_STRUCTURE
1385,torch.distributed.broadcast,src,Source rank.,Source rank,int,,,,,
1386,torch.logspace,start,the starting value for the set of points,the starting value for the set of points,,,,,,
1387,torch.narrow,start,the starting dimension,the starting dimension,int,,0,,,
1388,torch.linspace,steps,number of points to sample between `start` and `end`. Default: `100`.,Default CONSTANT_NUM,bool,,0,,,
1389,torch.linspace,steps,number of points to sample between `start` and `end`. Default: `100`.,number of points to sample between PARAM and PARAM,int,,0,"[0,inf)",,
1390,torch.as_strided,storage_offset,the offset in the underlying storage of the output tensor,the offset in the underlying storage of the output D_STRUCTURE,,,,,,
1391,torch.cuda.stream,stream,selected stream. This manager is a no-op if it's `None`.,selected stream,,,,,,
1392,torch.cuda.stream,stream,selected stream. This manager is a no-op if it's `None`.,This manager is a no op if it QSTR,,,,,,
1393,torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1394,torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,bool,,0,,,
1395,torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the hessian for said PARAM which is the expected mathematical value,bool,,0,,,
1396,torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1397,torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,bool,,0,,,
1398,torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the hvp for said PARAM which is the expected mathematical value,bool,,0,,,
1399,torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1400,torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,bool,,0,,,
1401,torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the jacobian for said PARAM which is the expected mathematical value,bool,,0,,,
1402,torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,bool,,0,,,
1403,torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,bool,,0,,,
1404,torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the vhp for said PARAM which is the expected mathematical value,bool,,0,,,
1405,torch.nn.MaxPool2d,stride,the stride of the window. Default value is `kernel_size`,Default value is PARAM,,,,,,
1406,torch.nn.MaxPool2d,stride,the stride of the window. Default value is `kernel_size`,the stride of the window,int,,,"[0,inf)",,
1407,torch.nn.MaxUnpool1d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,,,,,,
1408,torch.nn.MaxUnpool1d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,stride of the max pooling window,int,,,"[0,inf)",,
1409,torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,int,,0;1,,,D_STRUCTURE
1410,torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Default PARAM,,,,,,
1411,torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",stride of the pooling operation,int,,,"[0,inf)",,
1412,torch.nn.Unfold,stride,the stride of the sliding blocks in the input spatial dimensions. Default: 1,Default CONSTANT_NUM,bool,,0,,,
1413,torch.nn.Unfold,stride,the stride of the sliding blocks in the input spatial dimensions. Default: 1,the stride of the sliding blocks in the input spatial dimensions,int,,,"[0,inf)",,
1414,torch.nn.functional.kl_div,target,Tensor of the same shape as input,D_STRUCTURE of the same shape as PARAM,,&input,,,,D_STRUCTURE
1415,torch.distributed.all_gather,tensor,Tensor to be broadcast from current process.,D_STRUCTURE to be broadcast from current process,,,,,,D_STRUCTURE
1416,torch.nn.init.ones_,tensor,an n-dimensional torch.Tensor,an n dimensional D_STRUCTURE,,,,,,D_STRUCTURE
1417,torch.cat,tensors,"any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.",any python D_STRUCTURE of D_STRUCTURE of the same type,,,,,,D_STRUCTURE
1418,torch.cat,tensors,"any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.",Non empty D_STRUCTURE provided must have the same shape except in the cat dimension,,,,,,D_STRUCTURE
1419,torch.nn.functional.affine_grid,theta,input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,input batch of affine matrices with shape BSTR for 3D,numeric,BSTR,,,,
1420,torch.can_cast,to,The target `torch.dtype`.,The target D_TYPE,D_TYPE,,,,,
1421,torch.hub.download_url_to_file,url,URL of the object to download,url of the object to download,,,,,,
1422,torch.addcdiv,value,multiplier for tensor1 / tensor2,multiplier for PARAM PARAM,numeric,,,,,
1423,torch.sparse_coo_tensor,values,"Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",Can be a D_STRUCTURE NumPy D_STRUCTURE scalar and other types,,,0;1,,,D_STRUCTURE
1424,torch.sparse_coo_tensor,values,"Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",Initial values for the D_STRUCTURE,,,,,,
1425,torch.onnx.export,verbose,"if specified, we will print out a debug description of the trace being exported.",if specified we will print out a debug description of the trace being exported,,,,,,
1426,torch.nn.functional.binary_cross_entropy_with_logits,weight,a manual rescaling weight if provided it's repeated to match input tensor shape,a manual rescaling weight if provided it repeated to match PARAM D_STRUCTURE shape,numeric,,,,,
1427,torch.nn.functional.conv_transpose1d,weight,"filters of shape (in _channels ,  out _channels/groups , kW) ",filters of shape BSTR,,BSTR,,,,
1428,torch.cdist,x1,input tensor of shape B  times P  times M .,input D_STRUCTURE of shape B times P times M,,"[B,P,M]",,,,D_STRUCTURE
1429,torch.nn.CTCLoss,zero_infinity,Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,Default CONSTANT_BOOL Infinite losses mainly occur when the inputs are too D_TYPE to be aligned to the targets,bool,,0,,,
1430,torch.nn.CTCLoss,zero_infinity,Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,Whether to zero infinite losses and the associated gradients,bool,,0,,,
1431,torch.clamp,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1432,torch.addcdiv,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1433,torch.numel,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1434,torch.masked_select,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1435,torch.addmm,mat1,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1436,torch.sigmoid,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1437,torch.logical_and,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1438,torch.mm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1439,torch.abs,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1440,torch.remainder,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1441,torch.square,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1442,torch.einsum,*operands,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1443,torch.norm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1444,torch.tensordot,a,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1445,torch.neg,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1446,torch.div,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1447,torch.ifft,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1448,torch.tensordot,b,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1449,torch.rand_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1450,torch.repeat_interleave,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1451,torch.narrow,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1452,torch.nonzero,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1453,torch.logical_and,other,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1454,torch.cdist,x1,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1455,torch.le,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1456,torch.distributed.all_gather,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1457,torch.addcmul,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1458,torch.sparse.addmm,mat,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1459,torch.pinverse,input,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1460,torch.sparse_coo_tensor,values,DD: array_like,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1461,torch.nn.functional.affine_grid,theta,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,D_STRUCTURE
1462,torch.cat,tensors,DD: sequence of Tensors,D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
1463,torch.autograd.grad,outputs,DD: sequence of Tensor,D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
1464,torch.autograd.gradcheck,inputs,DD: tuple of Tensor or Tensor,D_STRUCTURE of D_STRUCTURE,,,,,,D_STRUCTURE
1465,torch.autograd.gradgradcheck,grad_outputs,"DD: tuple of Tensor or Tensor, optional",D_STRUCTURE of D_STRUCTURE optional,,,,,,D_STRUCTURE
1466,torch.logical_and,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1467,torch.round,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1468,torch.symeig,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1469,torch.mode,out,"DD: tuple, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1470,torch.gt,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1471,torch.logsumexp,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1472,torch.log1p,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1473,torch.clamp,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1474,torch.logical_not,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1475,torch.nn.functional.binary_cross_entropy_with_logits,weight,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1476,torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1477,torch.sin,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1478,torch.logical_or,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1479,torch.log,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,D_STRUCTURE
1480,torch.ne,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,,,,,D_STRUCTURE
1481,torch.eq,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,,,,,D_STRUCTURE
1482,torch.floor_divide,other,DD: Tensor or Scalar,D_STRUCTURE or Scalar,,,0,,,D_STRUCTURE
1483,torch.linspace,steps,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1484,torch.nn.utils.prune.random_unstructured,amount,DD: int or float,ONE_WORD D_TYPE,D_TYPE,,,,,
1485,torch.distributed.send,dst,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1486,torch.cuda.set_rng_state,new_state,DD: torch.ByteTensor,ONE_WORD D_TYPE,D_TYPE,,,,,
1487,torch.triu_indices,row,DD: `int`,ONE_WORD D_TYPE,D_TYPE,,,,,
1488,torch.nn.utils.prune.remove,name,DD: str,ONE_WORD D_TYPE,D_TYPE,,,,,
1489,torch.narrow,start,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1490,torch.hub.download_url_to_file,url,DD: string,ONE_WORD D_TYPE,D_TYPE,,,,,
1491,torch.cuda.manual_seed,seed,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1492,torch.histc,min,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1493,torch.sum,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
1494,torch.normal,size,DD: int...,ONE_WORD D_TYPE,D_TYPE,,,,,
1495,torch.kthvalue,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
1496,torch.min,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,
1497,torch.distributed.broadcast,src,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1498,torch.unsqueeze,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1499,torch.nn.functional.gumbel_softmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1500,torch.quantize_per_channel,axis,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1501,torch.chunk,chunks,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1502,torch.rfft,signal_ndim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,
1503,torch.logspace,start,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,,
1504,torch.nn.utils.clip_grad_norm_,max_norm,DD: float or int,ONE_WORD D_TYPE,D_TYPE,,,,,
1505,torch.nn.ConstantPad3d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
1506,torch.onnx.export,verbose,"DD: bool, default False",D_TYPE default CONSTANT_BOOL,D_TYPE,,,,,
1507,torch.nn.TripletMarginLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
1508,torch.nn.utils.spectral_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,
1509,torch.lobpcg,method,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,
1510,torch.nn.CTCLoss,zero_infinity,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1511,torch.nn.functional.nll_loss,ignore_index,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1512,torch.eye,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1513,torch.autograd.functional.jacobian,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1514,torch.triu,diagonal,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1515,torch.nn.MultiLabelMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1516,torch.nn.ConvTranspose2d,groups,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1517,torch.nn.CrossEntropyLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
1518,torch.pca_lowrank,niter,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1519,torch.nn.KLDivLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
1520,torch.nn.BCELoss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1521,torch.distributed.all_gather,async_op,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1522,torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
1523,torch.lu,get_infos,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1524,torch.nn.functional.embedding,padding_idx,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1525,torch.nn.utils.rnn.pack_padded_sequence,batch_first,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1526,torch.nn.TripletMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1527,torch.rand_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
1528,torch.hub.load_state_dict_from_url,progress,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1529,torch.as_strided,storage_offset,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1530,torch.hamming_window,periodic,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1531,torch.nn.utils.weight_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,
1532,torch.autograd.functional.vjp,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1533,torch.argsort,descending,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1534,torch.sparse_coo_tensor,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1535,torch.autograd.functional.vhp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1536,torch.nn.functional.interpolate,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1537,torch.nn.NLLLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,
1538,torch.autograd.functional.hessian,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1539,torch.eye,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
1540,torch.nn.functional.embedding,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
1541,torch.nn.CosineSimilarity,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,
1542,torch.autograd.functional.hvp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1543,torch.autograd.gradcheck,rtol,"DD: float, optional",D_TYPE optional,D_TYPE,,,,,
1544,torch.nn.L1Loss,size_average,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1545,torch.bartlett_window,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,
1546,torch.nn.ConvTranspose3d,bias,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1547,torch.autograd.functional.hessian,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,,
1548,torch.nn.MaxUnpool2d,padding,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
1549,torch.nn.MaxUnpool1d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
1550,torch.nn.Conv1d,kernel_size,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,,,,D_STRUCTURE
1551,torch.sum,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,,,,D_STRUCTURE
1552,torch.nn.Unfold,stride,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,,,,D_STRUCTURE
1553,torch.distributions.kl.kl_divergence,p,DD: Distribution,ONE_WORD Distribution,,,,,,
1554,torch.can_cast,to,DD: dpython:type,dpython type,torch.dtype,,,,,
1555,torch.nn.utils.prune.global_unstructured,pruning_method,DD: function,ONE_WORD function,,,,,,
1556,torch.addr,beta,"DD: Number, optional",Number optional,numeric,,,,,
1557,torch.addcdiv,value,"DD: Number, optional",Number optional,numeric,,,,,
1558,torch.is_tensor,obj,DD: Object,ONE_WORD Object,,,,,,
1559,torch.distributed.get_world_size,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
1560,torch.distributed.reduce,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,
1561,torch.cuda.stream,stream,DD: Stream,ONE_WORD Stream,,,,,,
1562,torch.empty_like,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1563,torch.ones,device,"DD: `torch.device`, optional",torch device optional,,,,,,
1564,torch.multinomial,generator,"DD: `torch.Generator`, optional",torch Generator optional,,,,,,
1565,torch.rand,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,,
1566,torch.linspace,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,,
1567,torch.empty_like,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,,
