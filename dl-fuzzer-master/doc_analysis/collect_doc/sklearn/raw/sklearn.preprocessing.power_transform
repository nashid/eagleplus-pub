sklearn.preprocessing.power_transform = power_transform(X, method='warn', standardize=True, copy=True)
    Power transforms are a family of parametric, monotonic transformations
    that are applied to make data more Gaussian-like. This is useful for
    modeling issues related to heteroscedasticity (non-constant variance),
    or other situations where normality is desired.
    
    Currently, power_transform supports the Box-Cox transform and the
    Yeo-Johnson transform. The optimal parameter for stabilizing variance and
    minimizing skewness is estimated through maximum likelihood.
    
    Box-Cox requires input data to be strictly positive, while Yeo-Johnson
    supports both positive or negative data.
    
    By default, zero-mean, unit-variance normalization is applied to the
    transformed data.
    
    Read more in the :ref:`User Guide <preprocessing_transformer>`.
    
    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        The data to be transformed using a power transformation.
    
    method : str
        The power transform method. Available methods are:
    
        - 'yeo-johnson' [1]_, works with positive and negative values
        - 'box-cox' [2]_, only works with strictly positive values
    
        The default method will be changed from 'box-cox' to 'yeo-johnson'
        in version 0.23. To suppress the FutureWarning, explicitly set the
        parameter.
    
    standardize : boolean, default=True
        Set to True to apply zero-mean, unit-variance normalization to the
        transformed output.
    
    copy : boolean, optional, default=True
        Set to False to perform inplace computation during transformation.
    
    Returns
    -------
    X_trans : array-like, shape (n_samples, n_features)
        The transformed data.
    
    Examples
    --------
    >>> import numpy as np
    >>> from sklearn.preprocessing import power_transform
    >>> data = [[1, 2], [3, 2], [4, 5]]
    >>> print(power_transform(data, method='box-cox'))
    [[-1.332... -0.707...]
     [ 0.256... -0.707...]
     [ 1.076...  1.414...]]
    
    See also
    --------
    PowerTransformer : Equivalent transformation with the
        ``Transformer`` API (e.g. as part of a preprocessing
        :class:`sklearn.pipeline.Pipeline`).
    
    quantile_transform : Maps data to a standard normal distribution with
        the parameter `output_distribution='normal'`.
    
    Notes
    -----
    NaNs are treated as missing values: disregarded in ``fit``, and maintained
    in ``transform``.
    
    For a comparison of the different scalers, transformers, and normalizers,
    see :ref:`examples/preprocessing/plot_all_scaling.py
    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.
    
    References
    ----------
    
    .. [1] I.K. Yeo and R.A. Johnson, "A new family of power transformations to
           improve normality or symmetry." Biometrika, 87(4), pp.954-959,
           (2000).
    
    .. [2] G.E.P. Box and D.R. Cox, "An Analysis of Transformations", Journal
           of the Royal Statistical Society B, 26, 211-252 (1964).