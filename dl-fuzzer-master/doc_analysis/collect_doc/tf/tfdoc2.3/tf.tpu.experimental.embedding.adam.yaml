aliases:
- tf.compat.v1.tpu.experimental.embedding.Adam
constraints:
  beta_1:
    default: '0.9'
    descp: A float value. The exponential decay rate for the 1st moment estimates.
  beta_2:
    default: '0.999'
    descp: A float value. The exponential decay rate for the 2nd moment estimates.
  clip_weight_max:
    default: None
    descp: the maximum value to clip by; None means +infinity.
  clip_weight_min:
    default: None
    descp: the minimum value to clip by; None means -infinity.
  epsilon:
    default: 1e-07
    descp: A small constant for numerical stability.
  lazy_adam:
    default: 'True'
    descp: Use lazy Adam instead of Adam. Lazy Adam trains faster.
  learning_rate:
    default: '0.001'
    descp: The learning rate. It should be a floating point value or a callable taking
      no arguments for a dynamic learning rate.
  multiply_weight_decay_factor_by_learning_rate:
    default: None
    descp: if true, weight_decay_factor is multiplied by the current learning rate.
  slot_variable_creation_fn:
    default: None
    descp: a callable taking two parameters, a variable and a list of slot names to
      create for it. This function should return a dict with the slot names as keys
      and the created variables as values. When set to None (the default), uses the
      built-in variable creation.
  sum_inside_sqrt:
    default: 'True'
    descp: When this is true, the Adam update formula is changed from m / (sqrt(v)
      + epsilon) to m / sqrt(v + epsilon**2). This option improves the performance
      of TPU training and is not expected to harm model quality.
  use_gradient_accumulation:
    default: 'True'
    descp: Setting this to False makes embedding gradients calculation less accurate
      but faster.
  weight_decay_factor:
    default: None
    descp: amount of weight decay to apply; None means that the weights are not decayed.
inputs:
  optional:
  - learning_rate
  - beta_1
  - beta_2
  - epsilon
  - lazy_adam
  - sum_inside_sqrt
  - use_gradient_accumulation
  - clip_weight_min
  - clip_weight_max
  - weight_decay_factor
  - multiply_weight_decay_factor_by_learning_rate
  - slot_variable_creation_fn
  required: []
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/tpu/experimental/embedding/Adam
package: tensorflow
target: Adam
title: tf.tpu.experimental.embedding.Adam
version: 2.3.0
