constraints:
  _weight:
    default: None
    descp: ''
    normalized_default: DEFAULT None
    normalized_descp: []
  embedding_dim:
    descp: the size of each embedding vector
    doc_dtype: int
    normalized_descp:
    - the size of each embedding vector
    normalized_docdtype: ONE_WORD D_TYPE
  include_last_offset:
    default: 'False'
    descp: 'if `True`, `offsets` has one additional element, where the last element
      is equivalent to the size of indices. This matches the CSR format. Note: this
      option is currently only supported when `mode="sum"`.'
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if CONSTANT_BOOL QSTR has one additional element where the last element is equivalent
      to the size of indices
    - This matches the CSR format
    - Note this option is currently only supported when PARAM QSTR
    normalized_docdtype: D_TYPE optional
  max_norm:
    default: None
    descp: If given, each embedding vector with norm larger than `max_norm` is renormalized
      to have norm `max_norm`.
    doc_dtype: float, optional
    normalized_default: DEFAULT None
    normalized_descp:
    - If given each embedding vector with norm larger than QSTR is renormalized to
      have norm QSTR
    normalized_docdtype: D_TYPE optional
  mode:
    default: mean
    descp: '`"sum"`, `"mean"` or `"max"`. Specifies the way to reduce the bag. `"sum"`
      computes the weighted sum, taking `per_sample_weights` into consideration. `"mean"`
      computes the average of the values in the bag, `"max"` computes the max value
      over each bag. Default: `"mean"`'
    doc_dtype: string, optional
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - ONE_WORD QSTR
    - Specifies the way to reduce the bag
    - QSTR computes the weighted sum taking QSTR into consideration
    - QSTR computes the average of the values in the bag QSTR computes the max value
      over each bag
    - Default QSTR
    normalized_docdtype: D_TYPE optional
  norm_type:
    default: '2.0'
    descp: The p of the p-norm to compute for the `max_norm` option. Default `2`.
    doc_dtype: float, optional
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - The p of the p norm to compute for the PARAM option
    - Default CONSTANT_NUM
    normalized_docdtype: D_TYPE optional
  num_embeddings:
    descp: size of the dictionary of embeddings
    doc_dtype: int
    normalized_descp:
    - size of the D_STRUCTURE of embeddings
    normalized_docdtype: ONE_WORD D_TYPE
  scale_grad_by_freq:
    default: 'False'
    descp: 'if given, this will scale gradients by the inverse of frequency of the
      words in the mini-batch. Default `False`. Note: this option is not supported
      when `mode="max"`.'
    doc_dtype: boolean, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if given this will scale gradients by the inverse of frequency of the words
      in the mini batch
    - Default CONSTANT_BOOL
    - Note this option is not supported when PARAM QSTR
    normalized_docdtype: D_TYPE optional
  sparse:
    default: 'False'
    descp: 'if `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See
      Notes for more details regarding sparse gradients. Note: this option is not
      supported when `mode="max"`.'
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if CONSTANT_BOOL gradient w r t
    - QSTR matrix will be a sparse D_STRUCTURE
    - See Notes for more details regarding sparse gradients
    - Note this option is not supported when PARAM QSTR
    normalized_docdtype: D_TYPE optional
inputs:
  optional:
  - max_norm
  - norm_type
  - scale_grad_by_freq
  - mode
  - sparse
  - _weight
  - include_last_offset
  required:
  - num_embeddings
  - embedding_dim
link: https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag
package: torch
target: EmbeddingBag
title: torch.nn.EmbeddingBag
version: 1.5.0
