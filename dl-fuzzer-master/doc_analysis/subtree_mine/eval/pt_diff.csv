API,Arg,Descp,default,doc_dtype,Catrgory,Label,Extracted
torch.nn.functional.adaptive_max_pool3d,output_size,the target output size (single integer or triple-integer tuple),None,,dtype,['int'],[]
torch.nn.functional.adaptive_max_pool3d,output_size,the target output size (single integer or triple-integer tuple),None,,structure,['tuple'],[]
torch.nn.functional.adaptive_max_pool3d,output_size,the target output size (single integer or triple-integer tuple),None,,shape,['[3]'],[]
torch.nn.functional.adaptive_max_pool3d,output_size,the target output size (single integer or triple-integer tuple),None,,ndim,"['0', '1']",[]
torch.nn.functional.adaptive_max_pool3d,output_size,the target output size (single integer or triple-integer tuple),None,,range,"['[0,inf)']",[]
torch.mean,keepdim,whether the output tensor has `dim` retained or not.,False,['bool'],tensor_t,[],['torch.tensor']
torch.nn.functional.avg_pool3d,kernel_size,"size of the pooling region. Can be a single number or a tuple (kT, kH, kW)",,,shape,['[3]'],[]
torch.cholesky,out,the output matrix,None,['Tensor'],dtype,['numeric'],[]
torch.nn.functional.avg_pool1d,kernel_size,"the size of the window. Can be a single number or a tuple (kW,)",,,shape,['[1]'],[]
torch.as_strided,stride,the stride of the output tensor,,"['tuple', 'ints']",tensor_t,[],['torch.tensor']
torch.utils.cpp_extension.load_inline,cuda_sources,"A string, or list of strings, containing CUDA source code.",None,,structure,[],['list']
torch.nn.functional.conv2d,weight,"filters of shape (out _channels ,  in _channels/groups , kH , kW) ",,,dtype,['numeric'],[]
torch.nn.utils.prune.l1_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",,"['int', 'float']",range,"['[0,1]', '[0,inf)']","['[0,inf)']"
torch.nn.ReplicationPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",,"['int', 'tuple']",ndim,"['0', '1']",[]
torch.nn.Unfold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,1,"['int', 'tuple']",dtype,['int'],"['int', 'torch.bool']"
torch.nn.Unfold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,1,"['int', 'tuple']",range,"['[0,inf)']",[]
torch.cdist,p,"p value for the p-norm distance to calculate between each vector pair  in [0,  infty] .",2.0,,range,"['[0,inf)']",[]
torch.autograd.functional.vhp,inputs,inputs to the function `func`.,,"['tuple of Tensors', 'Tensor']",dtype,[],['torch.bool']
torch.autograd.functional.vhp,inputs,inputs to the function `func`.,,"['tuple of Tensors', 'Tensor']",ndim,[],['0']
torch.cuda.current_stream,device,"selected device. Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",None,"['torch.device', 'int']",enum,[],"['None', 'Stream', 'device']"
torch.nn.utils.prune.global_unstructured,**kwargs,"other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",,,dtype,"['int', 'torch.float32']","['int', 'torch.bool']"
torch.nn.utils.prune.global_unstructured,**kwargs,"other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",,,range,"['[0,1]', '[0,inf)']","['[0,inf)']"
torch.nn.functional.dropout,p,probability of an element to be zeroed. Default: 0.5,0.5,,range,"['[0,1]']",[]
torch.nn.functional.binary_cross_entropy,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",None,['bool'],enum,[],['size_average']
torch.cuda.comm.scatter,dim,A dimension along which to chunk the tensor.,0,['int'],tensor_t,[],['torch.tensor']
torch.nn.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1",1,,shape,['[3]'],[]
torch.nn.functional.conv3d,weight,"filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",,,dtype,['numeric'],[]
torch.symeig,input,"the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",,['Tensor'],dtype,['numeric'],[]
torch.symeig,input,"the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",,['Tensor'],shape,"['[...,n,n]']",[]
torch.symeig,input,"the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",,['Tensor'],ndim,['>=2'],[]
torch.true_divide,divisor,the divisor,"['Tensor', 'Scalar']",tensor_t,['torch.tensor'],
torch.true_divide,divisor,the divisor,"['Tensor', 'Scalar']",ndim,['0'],
torch.nn.utils.vector_to_parameters,vec,a single vector represents the parameters of a model.,,['Tensor'],dtype,[],['torch.bool']
torch.nn.utils.vector_to_parameters,vec,a single vector represents the parameters of a model.,,['Tensor'],structure,['list'],[]
torch.nn.utils.vector_to_parameters,vec,a single vector represents the parameters of a model.,,['Tensor'],ndim,['1'],['0']
torch.linspace,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,False,['bool'],dtype,['torch.bool'],"['torch.bool', 'torch.dtype']"
torch.linspace,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,False,['bool'],tensor_t,[],['torch.tensor']
torch.bmm,mat2,the second batch of matrices to be multiplied,,['Tensor'],dtype,['numeric'],[]
torch.mvlgamma,input,the tensor to compute the multivariate log-gamma function,,['Tensor'],dtype,[],['torch.bool']
torch.mvlgamma,input,the tensor to compute the multivariate log-gamma function,,['Tensor'],ndim,[],['0']
torch.nn.BatchNorm1d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,0.1,,dtype,['torch.float32'],"['torch.bool', 'torch.float32']"
torch.narrow,length,the distance to the ending dimension,,['int'],ndim,[],['0']
torch.nn.ZeroPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",,"['int', 'tuple']",ndim,"['0', '1']",[]
torch.nn.utils.prune.custom_from_mask,module,module containing the tensor to prune,,['nn.Module'],tensor_t,[],['torch.tensor']
torch.nn.functional.pad,pad,"m-elements tuple, where  m/2  <=  input dimensions and m  is even.",,['tuple'],shape,['[m]'],[]
torch.nn.functional.pad,pad,"m-elements tuple, where  m/2  <=  input dimensions and m  is even.",,['tuple'],ndim,['1'],[]
torch.flip,dims,axis to flip on,,"['a list', 'tuple']",dtype,['int'],[]
torch.distributed.new_group,backend,"The backend to use. Depending on build-time configurations, valid values are `gloo` and `nccl`. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",None,"['str', 'Backend']",dtype,['string'],"['string', 'torch.bool']"
torch.distributed.new_group,backend,"The backend to use. Depending on build-time configurations, valid values are `gloo` and `nccl`. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",None,"['str', 'Backend']",ndim,[],"['0', '1']"
torch.distributed.new_group,backend,"The backend to use. Depending on build-time configurations, valid values are `gloo` and `nccl`. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",None,"['str', 'Backend']",enum,"['gloo', 'nccl']",[]
torch.bartlett_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",None,['torch.device'],dtype,[],['torch.dtype']
torch.bartlett_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",None,['torch.device'],tensor_t,[],['torch.tensor']
torch.bartlett_window,layout,the desired layout of returned window tensor. Only `torch.strided` (dense layout) is supported.,torch.strided,['torch.layout'],dtype,[],['torch.dtype']
torch.bartlett_window,layout,the desired layout of returned window tensor. Only `torch.strided` (dense layout) is supported.,torch.strided,['torch.layout'],tensor_t,[],['torch.tensor']
torch.nn.functional.cross_entropy,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",mean,['string'],dtype,['string'],"['int', 'string']"
torch.nn.functional.cross_entropy,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",mean,['string'],range,[],"['[0,inf)']"
torch.tensor,data,"Initial data for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",,['array_like'],structure,"['list', 'tuple']",['list']
torch.tensor,data,"Initial data for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",,['array_like'],tensor_t,[],['torch.tensor']
torch.tensor,data,"Initial data for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",,['array_like'],ndim,['0'],[]
torch.unique_consecutive,return_inverse,Whether to also return the indices for where elements in the original input ended up in the returned unique list.,False,['bool'],dtype,['torch.bool'],"['torch.bool', 'torch.dtype']"
torch.unique_consecutive,return_inverse,Whether to also return the indices for where elements in the original input ended up in the returned unique list.,False,['bool'],structure,[],['list']
torch.nn.functional.binary_cross_entropy_with_logits,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",None,['bool'],enum,[],['size_average']
torch.randn,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,,['int'],tensor_t,[],['torch.tensor']
torch.randn,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,,['int'],ndim,['1'],"['0', '1']"
torch.randn,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",None,['torch.device'],dtype,[],['torch.dtype']
torch.randn,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",None,['torch.device'],tensor_t,[],['torch.tensor']
torch.nn.UpsamplingBilinear2d,scale_factor,multiplier for spatial size.,None,"['float', 'Tuple[float', 'float]']",ndim,['1'],"['0', '1']"
torch.hub.load_state_dict_from_url,map_location,a function or a dict specifying how to remap storage locations (see torch.load),None,,ndim,[],"['0', '1']"
torch.nn.quantized.functional.conv3d,padding_mode,"the padding mode to use. Only ""zeros"" is supported for quantized convolution at the moment. Default: ""zeros""",zeros,,dtype,['string'],"['string', 'torch.bool']"
torch.nn.quantized.functional.conv3d,padding_mode,"the padding mode to use. Only ""zeros"" is supported for quantized convolution at the moment. Default: ""zeros""",zeros,,enum,['zeros'],[]
