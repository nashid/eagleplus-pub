constraints:
  '**kwargs':
    default: null
    descp: ''
    normalized_descp: []
  adjustment:
    default: None
    descp: 'A function taking the `Tensor` containing the (dynamic) shape of the input
      tensor and returning a pair (scale, bias) to apply to the normalized values
      (before gamma and beta), only during training. For example, if axis==-1,`adjustment
      = lambda shape: (   tf.random.uniform(shape[-1:], 0.93, 1.07),   tf.random.uniform(shape[-1:],
      -0.1, 0.1))`will scale the normalized value by up to 7% up or down, then shift
      the result by up to 0.1 (with independent scaling and bias for each feature
      but shared across all examples), and finally apply gamma and/or beta. If`None`,
      no adjustment is applied. Cannot be specified if virtual_batch_size is specified.'
    normalized_default: DEFAULT None
    normalized_descp:
    - A function taking the D_STRUCTURE containing the BSTR only during training
    - For example if PARAM CONSTANT_NUM adjustment lambda shape tf random uniform
      shape CONSTANT_NUM CONSTANT_FLOAT CONSTANT_FLOAT tf random uniform shape CONSTANT_NUM
      CONSTANT_FLOAT CONSTANT_FLOAT will PARAM the normalized value by up to CONSTANT_NUM
      up or down then shift the result by up to CONSTANT_FLOAT BSTR and finally apply
      gamma and or beta
    - If QSTR no adjustment is applied
    - Cannot be specified if PARAM is specified
  axis:
    default: '-1'
    descp: Integer, the axis that should be normalized (typically the features axis).
      For instance, after a `Conv2D` layer with`data_format="channels_first"`, set
      `axis=1` in `BatchNormalization`.
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - D_TYPE the axis that should be normalized BSTR
    - For instance after a QSTR layer with data_format QSTR set QSTR BatchNormalization
  beta_constraint:
    default: None
    descp: Optional constraint for the beta weight.
    normalized_default: DEFAULT None
    normalized_descp:
    - Optional constraint for the beta weight
  beta_initializer:
    default: zeros
    descp: Initializer for the beta weight.
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the beta weight
  beta_regularizer:
    default: None
    descp: Optional regularizer for the beta weight.
    normalized_default: DEFAULT None
    normalized_descp:
    - Optional regularizer for the beta weight
  center:
    default: 'True'
    descp: If True, add offset of `beta` to normalized tensor. If False, `beta` is
      ignored.
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL add offset of QSTR to normalized D_STRUCTURE
    - If CONSTANT_BOOL QSTR is ignored
  epsilon:
    default: '0.001'
    descp: Small float added to variance to avoid dividing by zero.
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - Small D_TYPE added to variance to avoid dividing by zero
  fused:
    default: None
    descp: if `True`, use a faster, fused implementation, or raise a ValueError if
      the fused implementation cannot be used. If `None`, use the faster implementation
      if possible. If False, do not used the fused implementation.
    normalized_default: DEFAULT None
    normalized_descp:
    - if CONSTANT_BOOL use a faster fused implementation or raise a ValueError if
      the fused implementation cannot be used
    - If QSTR use the faster implementation if possible
    - If CONSTANT_BOOL do not used the fused implementation
  gamma_constraint:
    default: None
    descp: Optional constraint for the gamma weight.
    normalized_default: DEFAULT None
    normalized_descp:
    - Optional constraint for the gamma weight
  gamma_initializer:
    default: ones
    descp: Initializer for the gamma weight.
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the gamma weight
  gamma_regularizer:
    default: None
    descp: Optional regularizer for the gamma weight.
    normalized_default: DEFAULT None
    normalized_descp:
    - Optional regularizer for the gamma weight
  momentum:
    default: '0.99'
    descp: Momentum for the moving average.
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - Momentum for the moving average
  moving_mean_initializer:
    default: zeros
    descp: Initializer for the moving mean.
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the moving mean
  moving_variance_initializer:
    default: ones
    descp: Initializer for the moving variance.
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - Initializer for the moving variance
  name:
    default: None
    descp: ''
    normalized_default: DEFAULT None
    normalized_descp: []
  renorm:
    default: 'False'
    descp: Whether to use Batch Renormalization (https://arxiv.org/abs/1702.03275).
      This adds extra variables during training. The inference is the same for either
      value of this parameter.
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - Whether to use Batch Renormalization https arxiv org abs CONSTANT_FLOAT
    - This adds extra variables during training
    - The inference is the same for either value of this parameter
  renorm_clipping:
    default: None
    descp: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar `Tensors`
      used to clip the renorm correction. The correction`(r, d)` is used as `corrected_value
      = normalized_value * r + d`, with`r` clipped to [rmin, rmax], and `d` to [-dmax,
      dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.
    normalized_default: DEFAULT None
    normalized_descp:
    - A D_STRUCTURE that may map keys QSTR to scalar D_STRUCTURE used to clip the
      PARAM correction
    - The correction BSTR is used as corrected_value normalized_value r d with QSTR
      clipped to BSTR and QSTR to BSTR
    - Missing rmax rmin dmax are set to inf CONSTANT_NUM inf respectively
  renorm_momentum:
    default: '0.99'
    descp: Momentum used to update the moving means and standard deviations with renorm.
      Unlike `momentum`, this affects training and should be neither too small (which
      would add noise) nor too large (which would give stale estimates). Note that
      `momentum` is still applied to get the means and variances for inference.
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp:
    - PARAM used to update the moving means and standard deviations with PARAM
    - Unlike PARAM this affects training and should be neither too small BSTR
    - Note that PARAM is still applied to get the means and variances for inference
  scale:
    default: 'True'
    descp: If True, multiply by `gamma`. If False, `gamma` is not used. When the next
      layer is linear (also e.g. `nn.relu`), this can be disabled since the scaling
      will be done by the next layer.
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - If CONSTANT_BOOL multiply by QSTR
    - If CONSTANT_BOOL QSTR is not used
    - When the next layer is linear also e g
    - nn relu this can be disabled since the scaling will be done by the next layer
  trainable:
    default: 'True'
    descp: Boolean, if `True` the variables will be marked as trainable.
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - D_TYPE if CONSTANT_BOOL the variables will be marked as trainable
  virtual_batch_size:
    default: None
    descp: An `int`. By default, `virtual_batch_size` is `None`, which means batch
      normalization is performed across the whole batch. When`virtual_batch_size`
      is not `None`, instead perform "Ghost Batch Normalization", which creates virtual
      sub-batches which are each normalized separately (with shared gamma, beta, and
      moving statistics). Must divide the actual batch size during execution.
    normalized_default: DEFAULT None
    normalized_descp:
    - An D_TYPE
    - By default QSTR is QSTR which means batch normalization is performed across
      the whole batch
    - When QSTR is not QSTR instead perform QSTR which creates virtual sub batches
      which are each normalized separately BSTR
    - Must divide the actual batch size during execution
inputs:
  optional:
  - axis
  - momentum
  - epsilon
  - center
  - scale
  - beta_initializer
  - gamma_initializer
  - moving_mean_initializer
  - moving_variance_initializer
  - beta_regularizer
  - gamma_regularizer
  - beta_constraint
  - gamma_constraint
  - renorm
  - renorm_clipping
  - renorm_momentum
  - fused
  - trainable
  - virtual_batch_size
  - adjustment
  - name
  - '**kwargs'
  required: []
link: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/layers/BatchNormalization
package: tensorflow
target: BatchNormalization
title: tf.keras.layers.BatchNormalization
version: 2.1.0
