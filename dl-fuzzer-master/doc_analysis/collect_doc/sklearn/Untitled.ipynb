{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from parse_utils import *\n",
    "from yaml_file_cls import yaml_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parser:\n",
    "    def __init__(self, fname, content):\n",
    "        self.data = yaml_file(title=fname, api_name=fname.split('.')[-1], url='', package='scikit-learn', version='0.24.X')\n",
    "        self.content = content\n",
    "        self.fname = fname\n",
    "    \n",
    "    def parse_sig(self):\n",
    "        def process_sig(parsed_sig):\n",
    "            ret = {}\n",
    "            for key in parsed_sig:\n",
    "                if key=='' or key.isspace() or key=='...':\n",
    "                    continue\n",
    "                else:\n",
    "                    ret[key] = parsed_sig[key]\n",
    "            return ret\n",
    "        m = re.match('(.*?)\\s=\\s([\\w_.]+)\\((.*?)\\)$', self.content[0])\n",
    "        if m:\n",
    "            assert(fname == m.group(1))\n",
    "            assert(fname.split('.')[-1] == m.group(2))\n",
    "            sig = m.group(3)\n",
    "            parsed_sig = parse_input(sig)\n",
    "            parsed_sig = process_sig(parsed_sig)\n",
    "            if self.data.init_input(parsed_sig):\n",
    "                return True\n",
    "        \n",
    "        raise GoToTheNextOne('' , self.fname, '[Sig] Fail to parse signature', save=True)\n",
    "    \n",
    "    def get_sect(self, sect):\n",
    "        rule = r'\\n\\s*{}\\n\\s*---+\\n(.*?)(\\n\\s*(\\w+|See [aA]lso)\\n\\s*---+\\n|$)'.format(sect)\n",
    "        m = re.search(rule, ''.join(self.content), flags=re.DOTALL)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        raise GoToTheNextOne('' , self.fname, '[{}] Fail to parse {} section'.format(sect, sect), save=True)\n",
    "        \n",
    "    def descp_pre_process(self,descp):\n",
    "        return re.sub(r'\\.\\.\\s+version(changed|added)::.*?(\\n\\s+\\n|$)', '\\n \\n', descp, flags=re.DOTALL)\n",
    "    \n",
    "    def descp_post_process(self,descp):\n",
    "        return re.sub(r'\\s*\\n\\s*', ' ', descp.lstrip())\n",
    "    \n",
    "    def update_descp(self, descp_dict):\n",
    "        if not descp_dict:\n",
    "            raise GoToTheNextOne('' , self.fname, '[Descp] Fail to parse descp section (empty return)', save=True)\n",
    "        \n",
    "        for arg in descp_dict:\n",
    "            self.data.update_constraint(arg, descp_dict[arg], allow_inconsistent_when_kwargs=False, ignore_star=False)\n",
    "        \n",
    "        \n",
    "    def parse_descp(self, raw_descp):\n",
    "        ret = {}\n",
    "        raw_descp = self.descp_pre_process(raw_descp)\n",
    "#         print(raw_descp)\n",
    "        for a in re.split(r'\\n\\s+\\n', raw_descp):\n",
    "            if not a:\n",
    "                continue\n",
    "            m = re.match(r'^\\s*([\\w_\\*]+)\\s*:(.*?)$', a, flags=re.DOTALL)\n",
    "            if not m:\n",
    "                return self.parse_descp2(raw_descp)\n",
    "                \n",
    "                \n",
    "            varname = m.group(1)\n",
    "            descp = self.descp_post_process(m.group(2))\n",
    "            ret[varname] = descp\n",
    "            \n",
    "        return ret\n",
    "        \n",
    "    def parse_descp2(self, raw_descp):\n",
    "        # match the descp by the args\n",
    "        # cannot detect inconsistencies, but can solve the itmes inside param descp\n",
    "        ret = {}\n",
    "        # raw_descp = descp_pre_process(raw_descp)\n",
    "        arg_list = list(self.data.data['constraints'].keys())\n",
    "        non_space_seg = []\n",
    "        for seg in re.split(r'(^|\\n\\s+\\n)\\s+({})\\s+:(.*?)'.format(get_bigrex(arg_list, boundary=False, escape=True)), raw_descp, flags=re.DOTALL):\n",
    "            if seg and not seg.isspace():\n",
    "                non_space_seg.append(seg)\n",
    "        try:\n",
    "            assert(len(non_space_seg) == 2*len(arg_list))\n",
    "        except:\n",
    "            raise GoToTheNextOne('' , self.fname, '[SPEC_Descp] Fail to parse descp section', save=True)\n",
    "            \n",
    "#         print(non_space_seg)\n",
    "        for i in range(0, len(non_space_seg), 2):\n",
    "            varname = non_space_seg[i]\n",
    "            descp = self.descp_post_process(non_space_seg[i+1])\n",
    "            ret[varname] = descp\n",
    "            \n",
    "        return ret\n",
    "        \n",
    "    \n",
    "        \n",
    "    def parse(self, folder):\n",
    "        self.parse_sig()\n",
    "        param_str = self.get_sect('Parameters')\n",
    "        descp_dict = self.parse_descp(param_str)\n",
    "        self.update_descp(descp_dict)\n",
    "        self.data.save_file(folder, filename = self.fname)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = '/Users/danning/Desktop/deepflaw/exp2/code/dl-fuzzer/doc_analysis/collect_doc/scikitlearn/raw/'\n",
    "dst_path = '/Users/danning/Desktop/deepflaw/exp2/code/dl-fuzzer/doc_analysis/collect_doc/scikitlearn/parsed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn.utils.Bunch: [Sig] Fail to parse signature\n",
      "sklearn.pipeline.make_pipeline: arg memory doesn't exist\n",
      "sklearn.utils.arrayfuncs.min_pos: [Sig] Fail to parse signature\n",
      "sklearn.utils.estimator_checks.check_estimator: arg estimator doesn't exist\n",
      "sklearn.manifold.locally_linear_embedding: arg arpack doesn't exist\n",
      "sklearn.compose.make_column_transformer: arg transformer doesn't exist\n",
      "sklearn.metrics.pairwise.distance_metrics: [Sig] Fail to parse signature\n",
      "sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l1: [Sig] Fail to parse signature\n",
      "sklearn.datasets.load_sample_images: [Sig] Fail to parse signature\n",
      "sklearn.config_context: arg assume_finite doesn't exist\n",
      "sklearn.show_versions: [Sig] Fail to parse signature\n",
      "sklearn.utils.parallel_backend: [Sig] Fail to parse signature\n",
      "sklearn.linear_model.PassiveAggressiveRegressor: [Sig] Fail to parse signature\n",
      "sklearn.utils.graph_shortest_path.graph_shortest_path: [Sig] Fail to parse signature\n",
      "sklearn.metrics.silhouette_samples: [SPEC_Descp] Fail to parse descp section\n",
      "sklearn.utils.random.sample_without_replacement: [Sig] Fail to parse signature\n",
      "sklearn.get_config: [Sig] Fail to parse signature\n",
      "sklearn.compose.make_column_selector: [Sig] Fail to parse signature\n",
      "sklearn.metrics.plot_confusion_matrix: arg y doesn't exist\n",
      "sklearn.utils.resample: [SPEC_Descp] Fail to parse descp section\n",
      "sklearn.pipeline.make_union: arg n_jobs doesn't exist\n",
      "sklearn.model_selection.train_test_split: arg test_size doesn't exist\n",
      "sklearn.utils.shuffle: [SPEC_Descp] Fail to parse descp section\n",
      "sklearn.utils.sparsefuncs_fast.inplace_csr_row_normalize_l2: [Sig] Fail to parse signature\n",
      "sklearn.metrics.pairwise_distances_chunked: [SPEC_Descp] Fail to parse descp section\n",
      "sklearn.metrics.pairwise.kernel_metrics: [Sig] Fail to parse signature\n",
      "sklearn.utils.deprecated: [Sig] Fail to parse signature\n",
      "sklearn.utils.register_parallel_backend: [Parameters] Fail to parse Parameters section\n",
      "sklearn.utils.murmurhash3_32: [Sig] Fail to parse signature\n"
     ]
    }
   ],
   "source": [
    "\n",
    "del_file(dst_path)\n",
    "for fname in get_file_list(src_path):\n",
    "    try:\n",
    "        p = parser(fname, content = read_file(os.path.join(src_path, fname)))\n",
    "        p.parse(dst_path)\n",
    "#         descp_dict = p.parse_descp(p.get_sect('Parameters'))\n",
    "#         prettyprint(descp_dict)\n",
    "#         print()\n",
    "    except GoToTheNextOne as gttno:\n",
    "        if gttno.save:\n",
    "            # continue\n",
    "            print(fname+': '+gttno.msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'score_func|greater_is_better|needs_proba|needs_threshold|\\\\*\\\\*kwargs'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_list = ['score_func', 'greater_is_better', 'needs_proba', 'needs_threshold', '**kwargs']\n",
    "get_bigrex(arg_list, boundary=False, escape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r'\\b\\*\\*kwargs\\b', ' **kwargs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 'X': 'array-like The data to fit. Can be, for example a list, or an array at '\n",
      "       'least 2d.',\n",
      "  'cv': 'int, cross-validation generator or an iterable, optional Determines '\n",
      "        'the cross-validation splitting strategy. Possible inputs for cv are: '\n",
      "        '- None, to use the default 5-fold cross validation, - integer, to '\n",
      "        'specify the number of folds in a `(Stratified)KFold`, - :term:`CV '\n",
      "        'splitter`, - An iterable yielding (train, test) splits as arrays of '\n",
      "        'indices. For integer/None inputs, if the estimator is a classifier '\n",
      "        'and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is '\n",
      "        'used. In all other cases, :class:`KFold` is used. Refer :ref:`User '\n",
      "        'Guide <cross_validation>` for the various cross-validation strategies '\n",
      "        'that can be used here.',\n",
      "  'estimator': \"estimator object implementing 'fit' and 'predict' The object \"\n",
      "               'to use to fit the data.',\n",
      "  'fit_params': 'dict, optional Parameters to pass to the fit method of the '\n",
      "                'estimator.',\n",
      "  'groups': 'array-like, with shape (n_samples,), optional Group labels for '\n",
      "            'the samples used while splitting the dataset into train/test set. '\n",
      "            'Only used in conjunction with a \"Group\" :term:`cv` instance '\n",
      "            '(e.g., :class:`GroupKFold`).',\n",
      "  'method': \"string, optional, default: 'predict' Invokes the passed method \"\n",
      "            \"name of the passed estimator. For method='predict_proba', the \"\n",
      "            'columns correspond to the classes in sorted order.',\n",
      "  'n_jobs': 'int or None, optional (default=None) The number of CPUs to use to '\n",
      "            'do the computation. ``None`` means 1 unless in a '\n",
      "            ':obj:`joblib.parallel_backend` context. ``-1`` means using all '\n",
      "            'processors. See :term:`Glossary <n_jobs>` for more details.',\n",
      "  'pre_dispatch': 'int, or string, optional Controls the number of jobs that '\n",
      "                  'get dispatched during parallel execution. Reducing this '\n",
      "                  'number can be useful to avoid an explosion of memory '\n",
      "                  'consumption when more jobs get dispatched than CPUs can '\n",
      "                  'process. This parameter can be: - None, in which case all '\n",
      "                  'the jobs are immediately created and spawned. Use this for '\n",
      "                  'lightweight and fast-running jobs, to avoid delays due to '\n",
      "                  'on-demand spawning of the jobs - An int, giving the exact '\n",
      "                  'number of total jobs that are spawned - A string, giving an '\n",
      "                  \"expression as a function of n_jobs, as in '2*n_jobs'\",\n",
      "  'verbose': 'integer, optional The verbosity level.',\n",
      "  'y': 'array-like, optional, default: None The target variable to try to '\n",
      "       'predict in the case of supervised learning.'}\n"
     ]
    }
   ],
   "source": [
    "# fname = 'sklearn.metrics.make_scorer'\n",
    "fname = 'sklearn.model_selection.cross_val_predict'\n",
    "p = parser(fname, content = read_file(os.path.join(src_path, fname)))\n",
    "# print(p.data.data)\n",
    "# p.parse(dst_path)\n",
    "p.parse_sig()\n",
    "s= p.get_sect('Parameters')\n",
    "prettyprint(p.parse_descp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrex(sep, boundary=True, escape=True):\n",
    "    if boundary:\n",
    "        s1 = r'\\b%s\\b'\n",
    "        s2 = r'\\b|\\b'\n",
    "    else:\n",
    "        s1 = r'%s'\n",
    "        s2 = r'|'\n",
    "\n",
    "\n",
    "    if escape:\n",
    "        return s1 % s2.join(map(re.escape, sep))\n",
    "    else:\n",
    "        return s1 % s2.join(sep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bigrex(args, boundary=True, escape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_descp = p.get_sect('Parameters')\n",
    "raw_descp = p.descp_pre_process(raw_descp)\n",
    "args = list(p.data.data['constraints'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimator\n",
      "**********************\n",
      " estimator object implementing 'fit' and 'predict'\n",
      "        The object to use to fit the data.\n",
      "**********************\n",
      "X\n",
      "**********************\n",
      " array-like\n",
      "        The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "**********************\n",
      "y\n",
      "**********************\n",
      " array-like, optional, default: None\n",
      "        The target variable to try to predict in the case of\n",
      "        supervised learning.\n",
      "**********************\n",
      "groups\n",
      "**********************\n",
      " array-like, with shape (n_samples,), optional\n",
      "        Group labels for the samples used while splitting the dataset into\n",
      "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "        instance (e.g., :class:`GroupKFold`).\n",
      "**********************\n",
      "cv\n",
      "**********************\n",
      " int, cross-validation generator or an iterable, optional\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "    \n",
      "        - None, to use the default 5-fold cross validation,\n",
      "        - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable yielding (train, test) splits as arrays of indices.\n",
      "    \n",
      "        For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used.\n",
      "    \n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "**********************\n",
      "n_jobs\n",
      "**********************\n",
      " int or None, optional (default=None)\n",
      "        The number of CPUs to use to do the computation.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "**********************\n",
      "verbose\n",
      "**********************\n",
      " integer, optional\n",
      "        The verbosity level.\n",
      "**********************\n",
      "fit_params\n",
      "**********************\n",
      " dict, optional\n",
      "        Parameters to pass to the fit method of the estimator.\n",
      "**********************\n",
      "pre_dispatch\n",
      "**********************\n",
      " int, or string, optional\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "    \n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "    \n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "    \n",
      "            - A string, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "**********************\n",
      "method\n",
      "**********************\n",
      " string, optional, default: 'predict'\n",
      "        Invokes the passed method name of the passed estimator. For\n",
      "        method='predict_proba', the columns correspond to the classes\n",
      "        in sorted order.\n",
      "**********************\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "m = re.split(r'(^|\\n\\s+\\n)\\s+({})\\s+:(.*?)'.format(get_bigrex(args, boundary=False, escape=True)), raw_descp, flags=re.DOTALL)\n",
    "cnt = 0\n",
    "for line in m:\n",
    "    if line and not line.isspace():\n",
    "        cnt+=1\n",
    "        print(line)\n",
    "        print('**********************')\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ['estimator', 'X', 'y', 'groups', 'scoring', 'cv', 'n_jobs', 'verbose', 'fit_params', 'pre_dispatch', 'error_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in re.split(r'\\n\\s+\\n', s):\n",
    "    m = re.match(r'^\\s*([\\w_]+)\\s*:(.*?)$', a, flags=re.DOTALL)\n",
    "\n",
    "    varname = m.group(1)\n",
    "    descp = m.group(2)\n",
    "    descp = re.sub(r'\\s*\\n\\s*', ' ', descp.lstrip())\n",
    "    print(varname)\n",
    "    print(descp)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\n\\s+\\n', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'((\\n\\s+\\n|^)\\s*([\\w_]+)\\s*:(.*?))', s, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docterfuzz",
   "language": "python",
   "name": "docterfuzz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
