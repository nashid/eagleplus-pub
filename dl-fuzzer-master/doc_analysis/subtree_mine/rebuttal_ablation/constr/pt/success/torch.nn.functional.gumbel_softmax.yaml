constraints:
  dim:
    default: '-1'
    descp: 'A dimension along which softmax will be computed. Default: -1.'
    doc_dtype: int
    dtype:
    - int
    - torch.bool
    ndim:
    - '0'
    range:
    - '[0,inf)'
  eps:
    default: 1e-10
    descp: ''
    dtype:
    - int
    - torch.float32
    ndim:
    - '0'
  hard:
    default: 'False'
    descp: if `True`, the returned samples will be discretized as one-hot vectors,
      but will be differentiated as if it is the soft sample in autograd
    dtype:
    - torch.bool
    ndim:
    - '0'
  logits:
    descp: '[ u2026, num_features] unnormalized log probabilities'
  tau:
    default: '1'
    descp: non-negative scalar temperature
    dtype:
    - int
    - torch.bool
    ndim:
    - '0'
    range:
    - '[0,inf)'
inputs:
  optional:
  - tau
  - hard
  - eps
  - dim
  required:
  - logits
link: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.gumbel_softmax
package: torch
target: gumbel_softmax
title: torch.nn.functional.gumbel_softmax
version: 1.5.0
