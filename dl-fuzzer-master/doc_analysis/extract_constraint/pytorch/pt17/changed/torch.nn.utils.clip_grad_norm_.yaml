constraints:
  max_norm:
    descp: max norm of the gradients
    doc_dtype:
    - float
    - int
    dtype:
    - int
    - torch.float32
    sig_dtype: float
  norm_type:
    default: '2.0'
    descp: type of the used p-norm. Can be `'inf'` for infinity norm.
    doc_dtype:
    - float
    - int
    dtype:
    - int
    - torch.float32
    ndim:
    - '0'
    sig_dtype: float
  parameters:
    descp: an iterable of Tensors or a single Tensor that will have gradients normalized
    doc_dtype:
    - 'Iterable[Tensor] '
    - Tensor
    sig_dtype: Union[torch.Tensor,Iterable[torch.Tensor]]
    structure:
    - list(torch.tensor)
    tensor_t:
    - torch.tensor
inputs:
  optional:
  - norm_type
  required:
  - parameters
  - max_norm
link: https://pytorch.org/docs/1.7.0/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_
package: torch
ret_type: torch.Tensor
target: clip_grad_norm_
title: torch.nn.utils.clip_grad_norm_
version: 1.7.0
