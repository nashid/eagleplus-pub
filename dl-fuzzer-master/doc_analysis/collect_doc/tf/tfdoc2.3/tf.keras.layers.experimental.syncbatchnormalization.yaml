constraints:
  '**kwargs':
    default: null
    descp: ''
  adjustment:
    default: None
    descp: ''
  axis:
    default: '-1'
    descp: Integer, the axis that should be normalized (typically the features axis).
      For instance, after a Conv2D layer with data_format="channels_first", set axis=1
      in BatchNormalization.
  beta_constraint:
    default: None
    descp: Optional constraint for the beta weight.
  beta_initializer:
    default: zeros
    descp: Initializer for the beta weight.
  beta_regularizer:
    default: None
    descp: Optional regularizer for the beta weight.
  center:
    default: 'True'
    descp: If True, add offset of beta to normalized tensor. If False, beta is ignored.
  epsilon:
    default: '0.001'
    descp: Small float added to variance to avoid dividing by zero.
  gamma_constraint:
    default: None
    descp: Optional constraint for the gamma weight.
  gamma_initializer:
    default: ones
    descp: Initializer for the gamma weight.
  gamma_regularizer:
    default: None
    descp: Optional regularizer for the gamma weight.
  momentum:
    default: '0.99'
    descp: Momentum for the moving average.
  moving_mean_initializer:
    default: zeros
    descp: Initializer for the moving mean.
  moving_variance_initializer:
    default: ones
    descp: Initializer for the moving variance.
  name:
    default: None
    descp: ''
  renorm:
    default: 'False'
    descp: Whether to use Batch Renormalization. This adds extra variables during
      training. The inference is the same for either value of this parameter.
  renorm_clipping:
    default: None
    descp: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar Tensors
      used to clip the renorm correction. The correction (r, d) is used as corrected_value
      = normalized_value * r + d, with r clipped to [rmin, rmax], and d to [-dmax,
      dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.
  renorm_momentum:
    default: '0.99'
    descp: Momentum used to update the moving means and standard deviations with renorm.
      Unlike momentum, this affects training and should be neither too small (which
      would add noise) nor too large (which would give stale estimates). Note that
      momentum is still applied to get the means and variances for inference.
  scale:
    default: 'True'
    descp: If True, multiply by gamma. If False, gamma is not used. When the next
      layer is linear (also e.g. nn.relu), this can be disabled since the scaling
      will be done by the next layer.
  trainable:
    default: 'True'
    descp: Boolean, if True the variables will be marked as trainable.
inputs:
  optional:
  - axis
  - momentum
  - epsilon
  - center
  - scale
  - beta_initializer
  - gamma_initializer
  - moving_mean_initializer
  - moving_variance_initializer
  - beta_regularizer
  - gamma_regularizer
  - beta_constraint
  - gamma_constraint
  - renorm
  - renorm_clipping
  - renorm_momentum
  - trainable
  - adjustment
  - name
  - '**kwargs'
  required: []
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization
package: tensorflow
target: SyncBatchNormalization
title: tf.keras.layers.experimental.SyncBatchNormalization
version: 2.3.0
