# DocTer: Document-Guided Fuzzing for Deep Learning APIs
## General Idea
To test DL APIs in large scale, we leverage constraints extracted from documents to guide fuzzing.
DocTer focuses on generating Python inputs for the DL APIs.

## Constraints
DocTer uses Sequential Pattern Mining to get frequent subsequences as patterns.
We then manually transform the patterns to extraction rules for constraint extraction.
Constraints are stored in YAML format for the fuzzing component to process.

## How to run DocTer
### For TensorFlow
Environment:

Under `tensorflow` directory, there's a Dockerfile to create an environment to test TensorFlow with.

To create the docker image: in project root, `docker build -f tensorflow/Dockerfile .`

There's also an `tensorflow/environment.yml` specifying the dependencies for the TensorFlow testing environment.
It's possible to create a conda environment directly using that file for testing, but it's not recommended.

Once inside the docker container:

Activate the environment:
```
conda activate fuzzer-test
```
It's better to create a work directory so the fuzzer stores generated input within.
```
mkdir workdir
```
To run the fuzzer:
```
python fuzzer/fuzzer-driver.py doc_analysis/extract_constraint/tf/constraint_2/changed/tf.broadcast_to.yaml \
       tensorflow/tensorflow_dtypes.yml \
       --max_iter=100 --obey --consec_fail=4 --workdir=workdir
```
This lets the fuzzer run for 100 iterations by obeying the constraints;

If there has been 4 consecutive failure (only refer to exceptions occured in --obey case),
the generation process will adapt to generate valid input;

The generated input are stored in `workdir`

To see the options of the fuzzer:
```
python fuzzer/fuzzer-driver.py -h
```

### Caveat
Since we decided to not include adaptive generation, use `--consec_fail=<XX>` and replace `<XX>` with a number larger than `--max_iter` to disable it.

### Cluster the Exception Messages
Documents often contain insufficient information,
so it's very difficult for the fuzzer to generate valid input based on little info.

This is problematic for the `expect_ok` mode
where we try to generate constraint-obeying input.
What happens is: with missing constraints,
the fuzzer thinks it has generated valid (i.e. obeying all constraints)
input, so during testing, it expects there's no exception,
but the input actually is invalid (e.g. unacceptable dtype) due to missing info.

To mitigate this issue, we cluster the Exception messages for `expect_ok` mode,
so the user can investigate the Exception messages and decide whether the unexpected
exception is a bug or not.

To make the fuzzer perform the clustering,
specify `--cluster`, `dist_metric`, and `dist_threshodl` flags:
```
python fuzzer/fuzzer-driver.py doc_analysis/extract_constraint/tf/constraint_2/changed/tf.broadcast_to.yaml \
       tensorflow/tensorflow_dtypes.yml \
       --max_iter=100 --obey --consec_fail=4 --workdir=workdir --cluster \
       --dist_metric=jaccard --dist_threshold=0.5
```
Only `jaccard` and `levenshtein` distances are supported now.

Note: `levenshtein` distance computation is very expensive,
so `jaccard` is recommended for faster performance.

Recommended threshold for `levenshtein` is 20, and for `jaccard` is 0.5

### For PyTorch
Under `pytorch` directory, there's a Dockerfile to create an environment to test PyTorch with.

To create the docker image to test PyTorch: in project root, `docker build -f pytorch/Dockerfile .`

Once inside the docker container:

Note that we don't need to do `conda activate` as we did for TensorFlow because all the package dependencies are directly installed in the docker image.

It's better to create a work directory so the fuzzer stores generated input within.
```
mkdir workdir
```
To run the fuzzer:
```
python fuzzer/fuzzer-driver.py doc_analysis/extract_constraint/pytorch/constraint_1/changed/torch.abs.yaml \
       pytorch/pytorch_dtypes.yml --data_construct=from_numpy \
       --max_iter=100 --obey --consec_fail=4 --workdir=workdir
```
NOTE: there's one additional option is important to run with pytorch: `--data_construct=tensor`:
since PyTorch works differently from TensorFlow, PyTorch does not directly take numpy array as input,
but instead, the user needs to specifically convert numpy array to a PyTorch Tensor using `torch.tensor` function.
Therefore, we have this option here to specify any numpy array input generated by the fuzzer needs
to be converted using `torch.tensor`.

### For MXNet
The procedure to test on MXNet is same as for PyTorch, except we should use `--data_construct=nd.array`.


### To Run Baseline
We disable constraints by using `--ignore` flag and disable adaptive generation by `--consec_fail=<XX>` with large number.

### To Run Experiments
`statistics/run_batch.sh` is the script to run expriments.


## Investigating Potential Bugs
### Use pdb to investigate python execution
We want to figure out why the input can trigger such output:
>F tensorflow/core/framework/tensor_shape.cc:345] Check failed: size >= 0 (-2142219066 vs. 0)

The problem seems to be obviously in C++ code.
However, if you wish to see how python is responsible for the behavior,
you can use `pdb` to inspect the python execution:

  * in the generated python file `xxx.py`:
  * add `import pdb`
  * add `pdb.set_trace()` before the API call (e.g., before `tf.broadcast_to(**data)`)
  * `python xxx.py`
  * step into the execution and do what you wish with `pdb`
  * NOTE: when you try to step into a function, but it just keeps going with the execution,
    then likely the python has called a C++ function, so the control is in C++ from this point.

### Use gdb to investigate C++ execution
You can try to use `gdb` in the docker container (need `apt-get install gdb`).
Or start a docker with a tensorflow debug version for 2.1.0.

Option 1: use `gdb` with `dlfuzzer-tf-2.1.0:latest`

  * the image installed tensorflow using `pip`, so many necessary details are optimized out.
  * however, it's faster to launch `gdb` and if the bug is related to a segfault or abort,
    it could be useful to see the backtrace.

Option 2: use `gdb` with docker image `tf21_debug:latest`

  * this image has TF2.1.0 build from source with debug flags
  * launching `gdb` for tensorflow takes a few minutes because it needs to read large symbol tables
  * for several cases, the tf2.1.0 debug version behaves slightly differently from the offical tf2.1.0
    (e.g., no longer segfault but abort before segfault occurs).
    Not sure about the cause yet (likely due to compiler optimizations).
  * To run container with this image:
```bash
docker run --rm -it --cap-add=SYS_PTRACE --security-opt seccomp=unconfined \
  -v /local1/m346kim/dl-fuzzing/docker_4_f19509:/home  -w /tmp \
  --memory=32g --memory-swap=32g --name some_name_you_want   \
  tf21_debug
```
### Use a second terminal window
open a terminal for python interpreter (terminal-1)
open a terminal for `gdb` (terminal-2)

in terminal-1, do:

```python
>>> import tensorflow as tf
>>> import pickle
>>> import os
>>> os.getpid()
17511
```

in terminal-2, do:

```bash
gdb -p 17511
```

terminal-1 will pause and terminal-2 takes control.

in terminal-2, set breakpoints as needed:

* first thing to do: `dir /tensorflow` to set the search directory for `gdb`
* Setting breakpoints is actually often challenging
  because often it's not clear which function we should set breakpoint at.
* Sometimes, setting the breakpoint at the right function is challenging because
  many C++ functions are overloaded, so it's not obvious which one to set.
* Two effective ways to set breakpoints:
  
    1. function signature, you could try:
     ```
     b tensorflow::(anonymous namespace)::BroadcastToOp::Compile(tensorflow::XlaOpKernelContext*)
     ```
     hitting `tab` will make `gdb` try to auto-complete, but the response is quite slow, so need to wait for several minutes.
    2. line number in source file.
     ```
     b /tensorflow/tensorflow/compiler/tf2xla/kernels/broadcast_to_op.cc:31
     ```
     
     Note: I find it useful to use VSCode for the C++ code to:
     
     * view the tensorflow source files
     * search for a candidate function to set breakpoint at
     * find function definitions

After breakpoints are set, enter `c` in `gdb`, the control will return back to terminal-1.

Then, in terminal-1, set up the necessary input and function call as needed:

```python
>>> data = pickle.load(open("/home/workdir/expect_ok_prev_ok/tf.broadcast_to.yaml_workdir/2341e91d5593a0e02de602a2f6b5052ec9feb642.p",'rb'))
>>> tf.broadcast_to(**data)
```

`gdb` will take back control when breakpoints are hit.

## Git Bisect
If it's known 2 commits behave differently,
it might be a good idea to use `git bisect` to find the error introducing commit.

Or, reverse the usage, to find the bug fixing commit.

The building time for tensorflow is expensive, ~1hr per build with build cache.
But, much shorter if the the commit checked out is not much different from the last build (~5min per build).
