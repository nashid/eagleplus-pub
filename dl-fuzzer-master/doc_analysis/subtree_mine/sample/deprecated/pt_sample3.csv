API,Arg,Descp,Normalized_descp,dtype,tensor_t,structure,shape,ndim,range,enum
torch.nn.functional.avg_pool3d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor in the formula to compute the output shape,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool3d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.where,x,values selected at indices where `condition` is `True`,values selected at indices where PARAM is CONSTANT_BOOL,,D_STRUCTURE,,,,,
torch.where,x,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.unique,return_inverse,Whether to also return the indices for where elements in the original input ended up in the returned unique list.,Whether to also return the indices for where elements in the original PARAM ended up in the returned unique D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.unique,return_inverse,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.unique,return_inverse,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.index_select,index,the 1-D tensor containing the indices to index,the CONSTANT_NUM D D_STRUCTURE containing the indices to index,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,,
torch.index_select,index,DD: LongTensor,ONE_WORD D_TYPE,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,,
torch.autograd.functional.hessian,inputs,inputs to the function `func`.,inputs to the function PARAM,,D_STRUCTURE,D_STRUCTURE,,,,
torch.autograd.functional.hessian,inputs,DD: tuple of Tensors or Tensor,D_STRUCTURE of D_STRUCTURE,,D_STRUCTURE,D_STRUCTURE,,,,
torch.hub.load,**kwargs,the corresponding kwargs for callable model.,the corresponding kwargs for callable PARAM,,,,,,,
torch.hub.load,**kwargs,DD: optional,ONE_WORD optional,,,,,,,
torch.std,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,D_TYPE,,,,CONSTANT_VAL,,
torch.std,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.std,unbiased,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,beta,exponent. Default: 0.75,ONE_WORD exponent,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,beta,exponent. Default: 0.75,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,beta,DF: 0.75,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.renorm,dim,the dimension to slice over to get the sub-tensors,the dimension to slice over to get the sub D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.renorm,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.barrier,group,The process group to work on,The process group to work on,,,,,,,
torch.distributed.barrier,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,,
torch.distributed.barrier,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,,
torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,,
torch.hamming_window,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,,
torch.hamming_window,device,"DD: `torch.device`, optional",torch device optional,,,,,,,
torch.hamming_window,device,DF: None,DEFAULT None,,,,,,,
torch.autograd.grad,inputs,Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).,Inputs w r t,,D_STRUCTURE,D_STRUCTURE,,,,
torch.autograd.grad,inputs,Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).,which the gradient will be returned and not accumulated into grad,,D_STRUCTURE,D_STRUCTURE,,,,
torch.autograd.grad,inputs,DD: sequence of Tensor,D_STRUCTURE of D_STRUCTURE,,D_STRUCTURE,D_STRUCTURE,,,,
torch.quantization.quantize_qat,run_args,positional arguments for run_fn,positional arguments for PARAM,,,,,,,
torch.nn.init.zeros_,tensor,an n-dimensional torch.Tensor,an n dimensional D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.UpsamplingBilinear2d,size,output spatial sizes,output spatial sizes,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.UpsamplingBilinear2d,size,"DD: int or Tuple[int, int], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.UpsamplingBilinear2d,size,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.jit.load,_extra_files,The extra filenames given in the map would be loaded and their content would be stored in the provided map.,The extra filenames given in the map would be loaded and their content would be stored in the provided map,D_TYPE,,D_STRUCTURE,,,,
torch.jit.load,_extra_files,DD: dictionary of filename to content,D_STRUCTURE of filename to content,D_TYPE,,D_STRUCTURE,,,,
torch.jit.load,_extra_files,DF: ExtraFilesMap{},DEFAULT DF_STR,D_TYPE,,D_STRUCTURE,,,,
torch.nn.RNNCell,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",If CONSTANT_BOOL then the layer does not use bias weights b_ih and b_hh,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.RNNCell,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.RNNCell,bias,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.addmv,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.addmv,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.addmv,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.bernoulli,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.bernoulli,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.bernoulli,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.cuda.comm.gather,destination,"output device (-1 means CPU, default: current device)",output device CONSTANT_NUM means CPU default current device,D_TYPE,,,,,,
torch.cuda.comm.gather,destination,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,,
torch.cuda.comm.gather,destination,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.add,input,the first input tensor,the first input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.add,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.atan2,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.atan2,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.atan2,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.normal,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.normal,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.normal,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.distributed.get_rank,group,The process group to work on,The process group to work on,,,,,,,
torch.distributed.get_rank,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,,
torch.distributed.get_rank,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.var,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.var,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.utils.prune.random_unstructured,module,module containing the tensor to prune,module containing the D_STRUCTURE to prune,,,,,,,
torch.nn.utils.prune.random_unstructured,module,DD: nn.Module,nn Module,,,,,,,
torch.hann_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL returns a window to be used as periodic function,D_TYPE,,,,CONSTANT_VAL,,
torch.hann_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL return a symmetric window,D_TYPE,,,,CONSTANT_VAL,,
torch.hann_window,periodic,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.hann_window,periodic,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",optional output D_STRUCTURE,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",If PARAM is CONSTANT_BOOL then the elements in the D_STRUCTURE are D_STRUCTURE IntTensor and IntTensor,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",If PARAM is CONSTANT_BOOL then the elements in the D_STRUCTURE are D_STRUCTURE IntTensor,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.lu,out,"optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`",Default QSTR,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.lu,out,"DD: tuple, optional",D_STRUCTURE optional,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.lu,out,DF: None,DEFAULT None,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.TransformerEncoderLayer,dropout,the dropout value (default=0.1).,the dropout value default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.TransformerEncoderLayer,dropout,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.binary_cross_entropy_with_logits,target,Tensor of the same shape as input,D_STRUCTURE of the same shape as PARAM,,D_STRUCTURE,,BSTR,,,
torch.quantization.quantize_qat,run_fn,"a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",a function for evaluating the prepared PARAM can be a function that simply runs the prepared PARAM or a training loop,,,,,,,
torch.nn.TransformerDecoderLayer,nhead,the number of heads in the multiheadattention models (required).,the number of heads in the multiheadattention models BSTR,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.min,out,"the tuple of two output tensors (min, min_indices)",the D_STRUCTURE of two output D_STRUCTURE BSTR,,,D_STRUCTURE,,,,
torch.min,out,"DD: tuple, optional",D_STRUCTURE optional,,,D_STRUCTURE,,,,
torch.min,out,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.MultiLabelMarginLoss,size_average,DF: None,DEFAULT None,,,,,,,
torch.nn.utils.prune.custom_from_mask,mask,binary mask to be applied to the parameter.,binary mask to be applied to the parameter,,D_STRUCTURE,,,,,
torch.nn.utils.prune.custom_from_mask,mask,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.bitwise_not,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.bitwise_not,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.eye,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.eye,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.eye,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.svd,out,the output tuple of tensors,the output D_STRUCTURE of D_STRUCTURE,,,D_STRUCTURE,,,,
torch.svd,out,"DD: tuple, optional",D_STRUCTURE optional,,,D_STRUCTURE,,,,
torch.svd,out,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.lu_unpack,LU_pivots,the packed LU factorization pivots,the packed LU factorization pivots,,D_STRUCTURE,,,,,
torch.lu_unpack,LU_pivots,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.sparse.mm,mat1,the first sparse matrix to be multiplied,the first sparse matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,,
torch.sparse.mm,mat1,DD: SparseTensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.where,y,values selected at indices where `condition` is `False`,values selected at indices where PARAM is CONSTANT_BOOL,,D_STRUCTURE,,,,,
torch.where,y,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.tensordot,dims,number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively,number of dimensions to contract or explicit D_STRUCTURE of dimensions for QSTR respectively,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.tensordot,dims,DD: int or tuple of two lists of python:integers,D_TYPE or D_STRUCTURE of two D_STRUCTURE of python D_TYPE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.tensordot,dims,DF: 2,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.cuda.set_rng_state,device,"The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",The device to set the RNG state,D_TYPE,,,,,,
torch.cuda.set_rng_state,device,"The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",Default QSTR i e torch device QSTR the current CUDA device,D_TYPE,,,,,,
torch.cuda.set_rng_state,device,"DD: torch.device or int, optional",torch device or D_TYPE optional,D_TYPE,,,,,,
torch.cuda.set_rng_state,device,DF: cuda,DEFAULT DF_STR,D_TYPE,,,,,,
torch.digamma,input,the tensor to compute the digamma function on,the D_STRUCTURE to compute the digamma function on,,D_STRUCTURE,,,,,
torch.digamma,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.Fold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,a parameter that controls the PARAM of elements within the neighborhood,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,dilation,a parameter that controls the stride of elements within the neighborhood. Default: 1,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,dilation,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.pad,value,fill value for `'constant'` padding. Default: `0`,fill value for QSTR padding,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.pad,value,fill value for `'constant'` padding. Default: `0`,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.pad,value,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.kthvalue,dim,the dimension to find the kth value along,the dimension to find the kth value along,D_TYPE,,,,CONSTANT_VAL,,
torch.kthvalue,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.kthvalue,dim,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.ge,input,the tensor to compare,the D_STRUCTURE to compare,,D_STRUCTURE,,,,,
torch.ge,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.bincount,weights,"optional, weight for each value in the input tensor. Should be of same size as input tensor.",optional weight for each value in the PARAM D_STRUCTURE,D_TYPE,D_STRUCTURE,,BSTR,,,
torch.bincount,weights,"optional, weight for each value in the input tensor. Should be of same size as input tensor.",Should be of same size as PARAM D_STRUCTURE,D_TYPE,D_STRUCTURE,,BSTR,,,
torch.bincount,weights,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,BSTR,,,
torch.bincount,weights,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,BSTR,,,
torch.nn.TransformerDecoder,num_layers,the number of sub-decoder-layers in the decoder (required).,the number of sub decoder layers in the decoder BSTR,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.PoissonNLLLoss,eps,Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8,Small value to avoid evaluation of log BSTR when PARAM CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PoissonNLLLoss,eps,Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PoissonNLLLoss,eps,"DD: float, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PoissonNLLLoss,eps,DF: 1e-08,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.TransformerEncoderLayer,d_model,the number of expected features in the input (required).,the number of expected features in the input BSTR,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.MaxUnpool2d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,Stride of the max pooling window,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool2d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool2d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool2d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,,,
torch.nn.AdaptiveLogSoftmaxWithLoss,in_features,Number of features in the input tensor,Number of features in the input D_STRUCTURE,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,"[0,inf)",
torch.nn.AdaptiveLogSoftmaxWithLoss,in_features,DD: int,ONE_WORD D_TYPE,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,"[0,inf)",
torch.logspace,steps,number of points to sample between `start` and `end`. Default: `100`.,number of points to sample between PARAM and PARAM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.logspace,steps,number of points to sample between `start` and `end`. Default: `100`.,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.logspace,steps,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.logspace,steps,DF: 100,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.avg_pool2d,count_include_pad,"when True, will include the zero-padding in the averaging calculation. Default: `True`",when CONSTANT_BOOL will include the zero PARAM in the averaging calculation,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,count_include_pad,"when True, will include the zero-padding in the averaging calculation. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,count_include_pad,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.mm,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.mm,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.mm,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.ReLU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ReLU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ReLU,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.reshape,input,the tensor to be reshaped,the D_STRUCTURE to be reshaped,,D_STRUCTURE,,,,,
torch.reshape,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.ConvTranspose1d,out_channels,Number of channels produced by the convolution,Number of channels produced by the convolution,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose1d,out_channels,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.distributed.send,tag,Tag to match send with remote recv,Tag to match send with remote recv,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.send,tag,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.send,tag,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.init.constant_,val,the value to fill the tensor with,the value to fill the D_STRUCTURE with,,,,,,,
torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",the stride of the window,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,stride,"the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`",Default PARAM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,,
torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if None uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,,
torch.sparse_coo_tensor,device,"the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,,
torch.sparse_coo_tensor,device,"DD: `torch.device`, optional",torch device optional,,,,,,,
torch.sparse_coo_tensor,device,DF: None,DEFAULT None,,,,,,,
torch.allclose,equal_nan,"if `True`, then two `NaN` s will be compared as equal. Default: `False`",if CONSTANT_BOOL then two QSTR will be compared as equal,D_TYPE,,,,CONSTANT_VAL,,
torch.allclose,equal_nan,"if `True`, then two `NaN` s will be compared as equal. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.allclose,equal_nan,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.allclose,equal_nan,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.std_mean,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.std_mean,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.lt,other,the tensor or value to compare,the D_STRUCTURE or value to compare,D_TYPE,D_STRUCTURE,,,,,
torch.lt,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,D_STRUCTURE,,,,,
torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,,
torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,,
torch.randperm,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,,
torch.randperm,device,"DD: `torch.device`, optional",torch device optional,,,,,,,
torch.randperm,device,DF: None,DEFAULT None,,,,,,,
torch.clamp,max,upper-bound of the range to be clamped to,upper bound of the range to be clamped to,D_TYPE,,,,,,
torch.clamp,max,DD: Number,ONE_WORD Number,D_TYPE,,,,,,
torch.zeros,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.zeros,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,Can be a variable number of arguments or a collection like a D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.zeros,*size,DD: int...,ONE_WORD D_TYPE,D_TYPE,,D_STRUCTURE,,,,
torch.nn.quantized.functional.avg_pool2d,kernel_size,"size of the pooling region. Can be a single number or a tuple (kH, kW)",size of the pooling region,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.quantized.functional.avg_pool2d,kernel_size,"size of the pooling region. Can be a single number or a tuple (kH, kW)",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",the spacing between kernel elements,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv2d,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.distributed.broadcast_multigpu,group,The process group to work on,The process group to work on,,,,,,,
torch.distributed.broadcast_multigpu,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,,
torch.distributed.broadcast_multigpu,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.nn.Bilinear,in2_features,size of each second input sample,size of each second input sample,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.atan2,other,the second input tensor,the second PARAM D_STRUCTURE,,D_STRUCTURE,,,,,
torch.atan2,other,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.ones,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.ones,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.ones,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.remainder,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.remainder,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.remainder,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.unique,input,the input tensor,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.unique,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.matrix_rank,input,the input 2-D tensor,the input CONSTANT_NUM D D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.matrix_rank,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",additional size added to one side of each dimension in the output shape,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose3d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose3d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.lobpcg,n,"if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns.",if X is not specified then n specifies the size of the generated random approximation of eigenvectors,D_TYPE,,,,,,
torch.lobpcg,n,"if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns.",Default value for n is k If X is specifed the value of n BSTR must be the number of X columns,D_TYPE,,,,,,
torch.lobpcg,n,"DD: integer, optional",D_TYPE optional,D_TYPE,,,,,,
torch.lobpcg,n,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.Softmax,dim,A dimension along which Softmax will be computed (so every slice along dim will sum to 1).,A dimension along which Softmax will be computed BSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Softmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Softmax,dim,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.var,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,D_TYPE,,,,CONSTANT_VAL,,
torch.var,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.var,unbiased,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.recv,tensor,Tensor to fill with received data.,D_STRUCTURE to fill with received data,,D_STRUCTURE,,,,,
torch.distributed.recv,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.ConvTranspose2d,output_padding,Additional size added to one side of each dimension in the output shape. Default: 0,Additional size added to one side of each dimension in the output shape,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose2d,output_padding,Additional size added to one side of each dimension in the output shape. Default: 0,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose2d,output_padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose2d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.ctc_loss,target_lengths,(N) . Lengths of the targets,ONE_WORD BSTR,,,,BSTR,CONSTANT_VAL,,
torch.nn.functional.ctc_loss,target_lengths,(N) . Lengths of the targets,Lengths of the PARAM,,,,BSTR,CONSTANT_VAL,,
torch.nn.ConvTranspose2d,out_channels,Number of channels produced by the convolution,Number of channels produced by the convolution,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose2d,out_channels,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",the desired data type of returned D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",If specified the PARAM D_STRUCTURE is casted to QSTR before the operation is performed,D_TYPE,D_STRUCTURE,,,,,
torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",This is useful for preventing data type overflows,D_TYPE,D_STRUCTURE,,,,,
torch.cumprod,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",Default None,D_TYPE,D_STRUCTURE,,,,,
torch.cumprod,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,D_STRUCTURE,,,,,
torch.cumprod,dtype,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.cuda.memory_stats,device,"selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",selected device,D_TYPE,,,,,,
torch.cuda.memory_stats,device,"selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",Returns statistics for the current device given by current_device if QSTR is QSTR BSTR,D_TYPE,,,,,,
torch.cuda.memory_stats,device,"DD: torch.device or int, optional",torch device or D_TYPE optional,D_TYPE,,,,,,
torch.cuda.memory_stats,device,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.std,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.std,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.renorm,p,the power for the norm computation,the power for the norm computation,D_TYPE,,,,,,
torch.renorm,p,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.rot90,dims,axis to rotate,axis to rotate,,,D_STRUCTURE,,,,
torch.rot90,dims,DD: a list or tuple,a D_STRUCTURE,,,D_STRUCTURE,,,,
torch.nn.LPPool1d,kernel_size,"a single int, the size of the window",a single D_TYPE the size of the window,D_TYPE,,,,,"[0,inf)",
torch.nn.Conv2d,padding_mode,"`'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`",ONE_WORD QSTR,D_TYPE,,,,,,
torch.nn.Conv2d,padding_mode,"`'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`",Default QSTR,D_TYPE,,,,,,
torch.nn.Conv2d,padding_mode,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,
torch.nn.Conv2d,padding_mode,DF: zeros,DEFAULT DF_STR,D_TYPE,,,,,,
torch.ormqr,input3,the matrix to be multiplied.,the matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,,
torch.ormqr,input3,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.cosine_similarity,x1,First input.,First input,,D_STRUCTURE,,,,,
torch.nn.functional.cosine_similarity,x1,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.addmm,alpha,multiplier for mat1 @ mat2  ( alpha ),multiplier for PARAM PARAM BSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.addmm,alpha,"DD: Number, optional",Number optional,D_TYPE,,,,CONSTANT_VAL,,
torch.addmm,alpha,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.max,out,"the result tuple of two output tensors (max, max_indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,,D_STRUCTURE,,,,
torch.max,out,"DD: tuple, optional",D_STRUCTURE optional,,,D_STRUCTURE,,,,
torch.max,out,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.SyncBatchNorm,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module tracks the running mean and variance and when set to CONSTANT_BOOL this module does not track such statistics and always uses batch statistics in both training and eval modes,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.SyncBatchNorm,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.SyncBatchNorm,track_running_stats,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.cuda.comm.broadcast_coalesced,tensors,tensors to broadcast.,D_STRUCTURE to broadcast,,D_STRUCTURE,D_STRUCTURE,,,,
torch.cuda.comm.broadcast_coalesced,tensors,DD: sequence,ONE_WORD D_STRUCTURE,,D_STRUCTURE,D_STRUCTURE,,,,
torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",If CONSTANT_BOOL the graph used to compute the grad will be freed,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",Note that in nearly all cases setting this option to CONSTANT_BOOL is not needed and often can be worked around in a much more efficient way,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,retain_graph,"If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.",Defaults to the value of PARAM,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,retain_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,retain_graph,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.log_softmax,input,input,ONE_WORD input,,D_STRUCTURE,,,,,
torch.nn.functional.log_softmax,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.MaxPool3d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later",if CONSTANT_BOOL will return the max indices along with the outputs,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool3d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later",Useful for torch nn MaxUnpool3d later,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool3d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.utils.checkpoint.checkpoint,*args,tuple containing inputs to the `function`,D_STRUCTURE containing inputs to the PARAM,,,,,,,
torch.empty_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.empty_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,D_TYPE,,,,CONSTANT_VAL,,
torch.empty_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.empty_like,dtype,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.logical_or,other,the tensor to compute OR with,the D_STRUCTURE to compute OR with,,D_STRUCTURE,,,,,
torch.logical_or,other,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",the size of the padding,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad3d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReLU6,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ReLU6,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ReLU6,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,input,"quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",quantized input D_STRUCTURE of shape BSTR,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.mean,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.mean,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.Linear,in_features,size of each input sample,size of each input sample,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.dist,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.dist,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.functional.affine_grid,size,"the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",the target output image size,D_TYPE,,,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.affine_grid,size,"the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",BSTR Example torch Size BSTR,D_TYPE,,,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.affine_grid,size,DD: torch.Size,torch Size,D_TYPE,,,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.init.eye_,tensor,a 2-dimensional torch.Tensor,a CONSTANT_NUM dimensional D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pad_sequence,padding_value,value for padded elements. Default: 0.,value for padded elements,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pad_sequence,padding_value,value for padded elements. Default: 0.,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pad_sequence,padding_value,"DD: float, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pad_sequence,padding_value,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool2d,kernel_size,the size of the window to take a max over,the size of the window to take a max over,D_TYPE,,,,,"[0,inf)",
torch.rand,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.rand,*size,a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.,Can be a variable number of arguments or a collection like a D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.rand,*size,DD: int...,ONE_WORD D_TYPE,D_TYPE,,D_STRUCTURE,,,,
torch.nn.quantized.functional.avg_pool2d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`",when CONSTANT_BOOL will use ceil instead of floor in the formula to compute the output shape,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,ceil_mode,"when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.gather,input,the source tensor,the source D_STRUCTURE,,D_STRUCTURE,,,,,
torch.gather,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.remainder,other,the divisor that may be either a number or a Tensor of the same shape as the dividend,the divisor that may be either a number or a D_STRUCTURE of the same shape as the dividend,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,,
torch.remainder,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,,
torch.normal,std,the tensor of per-element standard deviations,the D_STRUCTURE of per element standard deviations,,D_STRUCTURE,,,,,
torch.normal,std,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.init.orthogonal_,gain,optional scaling factor,optional scaling factor,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.init.orthogonal_,gain,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.chain_matmul,*matrices,a sequence of 2 or more 2-D tensors whose product is to be determined.,a D_STRUCTURE of CONSTANT_NUM or more CONSTANT_NUM D D_STRUCTURE whose product is to be determined,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.chain_matmul,*matrices,DD: Tensors...,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.distributed.send,group,The process group to work on,The process group to work on,,,,,,,
torch.distributed.send,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,,
torch.distributed.send,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.nn.LayerNorm,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LayerNorm,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LayerNorm,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.DataParallel,output_device,device location of output (default: device_ids[0]),device location of output default PARAM BSTR,D_TYPE,,,,,,
torch.nn.DataParallel,output_device,DD: int or torch.device,D_TYPE or torch device,D_TYPE,,,,,,
torch.nn.DataParallel,output_device,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.div,out,the output tensor.,the output D_STRUCTURE,,,,,,,
torch.div,out,"DD: Tensor, optional",D_STRUCTURE optional,,,,,,,
torch.div,out,DF: None,DEFAULT None,,,,,,,
torch.autograd.functional.hvp,func,a Python function that takes Tensor inputs and returns a Tensor with a single element.,a Python function that takes D_STRUCTURE PARAM and returns a D_STRUCTURE with a single element,,,,,,,
torch.autograd.functional.hvp,func,DD: function,ONE_WORD function,,,,,,,
torch.arange,start,the starting value for the set of points. Default: `0`.,the starting value for the set of points,D_TYPE,,,,CONSTANT_VAL,,
torch.arange,start,the starting value for the set of points. Default: `0`.,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.arange,start,DD: Number,ONE_WORD Number,D_TYPE,,,,CONSTANT_VAL,,
torch.arange,start,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.unique,sorted,Whether to sort the unique elements in ascending order before returning as output.,Whether to sort the unique elements in ascending order before returning as output,D_TYPE,,,,CONSTANT_VAL,,
torch.unique,sorted,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.unique,sorted,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.spectral_norm,dim,"dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",dimension corresponding to number of outputs the default is CONSTANT_NUM except for modules that are instances of ConvTranspose CONSTANT_NUM CONSTANT_NUM CONSTANT_NUM d when it is CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.spectral_norm,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.spectral_norm,dim,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.std_mean,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,D_TYPE,,,,CONSTANT_VAL,,
torch.std_mean,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.std_mean,unbiased,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.cummax,out,"the result tuple of two output tensors (values, indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,,D_STRUCTURE,,,,
torch.cummax,out,"DD: tuple, optional",D_STRUCTURE optional,,,D_STRUCTURE,,,,
torch.cummax,out,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.RReLU,lower,lower bound of the uniform distribution. Default:  1/8 ,lower bound of the uniform distribution,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.RReLU,lower,lower bound of the uniform distribution. Default:  1/8 ,Default CONSTANT_NUM CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.RReLU,lower,DF: 0.125,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.solve,A,"input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",PARAM square matrix of size BSTR where is zero or more batch dimensions,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.solve,A,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",the size of the padding,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReflectionPad2d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReflectionPad2d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.distributed.gather,dst,Destination rank (default is 0),Destination rank BSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.gather,dst,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.gather,dst,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.load,map_location,A simplified version of `map_location` in `torch.save` used to dynamically remap storages to an alternative set of devices.,A simplified version of QSTR in torch save used to dynamically remap storages to an alternative set of devices,D_TYPE,,,,,,
torch.jit.load,map_location,DD: string or torch.device,D_TYPE or torch device,D_TYPE,,,,,,
torch.jit.load,map_location,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.var_mean,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,,,CONSTANT_VAL,,
torch.var_mean,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.var_mean,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.broadcast_multigpu,src,Source rank.,Source rank,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.broadcast_multigpu,src,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool1d,output_size,the target output size H,the target output size H,D_TYPE,,,,,"[0,inf)",
torch.nn.BatchNorm2d,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module tracks the running mean and variance and when set to CONSTANT_BOOL this module does not track such statistics and always uses batch statistics in both training and eval modes,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.BatchNorm2d,track_running_stats,"a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.BatchNorm2d,track_running_stats,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,groups,Number of blocked connections from input channels to output channels. Default: 1,Number of blocked connections from input channels to output channels,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose3d,groups,Number of blocked connections from input channels to output channels. Default: 1,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose3d,groups,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose3d,groups,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,,,QSTR
torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,,,QSTR
torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,,,QSTR
torch.nn.SmoothL1Loss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,,,QSTR
torch.nn.SmoothL1Loss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,QSTR
torch.nn.SmoothL1Loss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.nn.functional.avg_pool2d,input,"input tensor (minibatch , in _channels , iH , iW) ",input D_STRUCTURE BSTR,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.addmm,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.addmm,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.addmm,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",PARAM BSTR padding zero padding will be added to both sides of each dimension in the PARAM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose1d,padding,"`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose1d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.ne,input,the tensor to compare,the D_STRUCTURE to compare,,D_STRUCTURE,,,,,
torch.ne,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.var,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.var,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.var,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.quantized.functional.conv3d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,non quantized bias D_STRUCTURE of shape BSTR,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,The D_STRUCTURE type must be D_TYPE,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.Fold,kernel_size,the size of the sliding blocks,the size of the sliding blocks,D_TYPE,,D_STRUCTURE,,,"[0,inf)",
torch.nn.Fold,kernel_size,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,"[0,inf)",
torch.histc,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.histc,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.histc,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.Upsample,scale_factor,multiplier for spatial size. Has to match input size if it is a tuple.,multiplier for spatial PARAM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Upsample,scale_factor,multiplier for spatial size. Has to match input size if it is a tuple.,Has to match input PARAM if it is a D_STRUCTURE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Upsample,scale_factor,"DD: float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional",D_TYPE or D_STRUCTURE BSTR or D_STRUCTURE BSTR or D_STRUCTURE BSTR optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Upsample,scale_factor,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.distributed.all_reduce,tensor,Input and output of the collective. The function operates in-place.,Input and output of the collective,,D_STRUCTURE,,,,,
torch.distributed.all_reduce,tensor,Input and output of the collective. The function operates in-place.,The function operates in place,,D_STRUCTURE,,,,,
torch.distributed.all_reduce,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.utils.remove_spectral_norm,name,name of weight parameter,name of weight parameter,D_TYPE,,,,,,
torch.nn.utils.remove_spectral_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,,
torch.nn.utils.remove_spectral_norm,name,DF: weight,DEFAULT DF_STR,D_TYPE,,,,,,
torch.is_complex,input,the PyTorch tensor to test,the PyTorch D_STRUCTURE to test,,D_STRUCTURE,,,,,
torch.is_complex,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.sinh,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.sinh,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.load,map_location,"a function, `torch.device`, string or a dict specifying how to remap storage locations",a function torch device D_TYPE or a D_STRUCTURE specifying how to remap storage locations,,,,,,,
torch.load,map_location,DF: None,DEFAULT None,,,,,,,
torch.addmv,vec,vector to be multiplied,vector to be multiplied,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.addmv,vec,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.onnx.export,f,a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file.,a file like object BSTR or a D_TYPE containing a file name,D_TYPE,,,,,,
torch.onnx.export,f,a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file.,A binary Protobuf will be written to this file,D_TYPE,,,,,,
torch.normal,std,the tensor of per-element standard deviations,the D_STRUCTURE of per element standard deviations,,D_STRUCTURE,,,,,
torch.normal,std,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.GRU,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",If CONSTANT_BOOL then the layer does not use bias weights b_ih and b_hh,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.GRU,bias,"If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.GRU,bias,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.new_group,ranks,List of ranks of group members.,D_STRUCTURE of ranks of group members,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.distributed.new_group,ranks,DD: list[int],D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.distributed.new_group,ranks,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.eig,out,the output tensors,the output D_STRUCTURE,,D_STRUCTURE,D_STRUCTURE,,,,
torch.eig,out,"DD: tuple, optional",D_STRUCTURE optional,,D_STRUCTURE,D_STRUCTURE,,,,
torch.eig,out,DF: None,DEFAULT None,,D_STRUCTURE,D_STRUCTURE,,,,
torch.jit.save,f,A file-like object (has to implement write and flush) or a string containing a file name.,A file like object BSTR or a D_TYPE containing a file name,D_TYPE,,,,,,
torch.clamp,min,lower-bound of the range to be clamped to,lower bound of the range to be clamped to,D_TYPE,,,,,,
torch.clamp,min,DD: Number,ONE_WORD Number,D_TYPE,,,,,,
torch.nn.functional.poisson_nll_loss,eps,Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8,Small value to avoid evaluation of log BSTR when PARAM CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,eps,Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,eps,"DD: float, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,eps,DF: 1e-08,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.linspace,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,D_TYPE,,,,,,
torch.linspace,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,D_TYPE,,,,,,
torch.linspace,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,,
torch.linspace,dtype,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.autograd.grad,allow_unused,"If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`.",If CONSTANT_BOOL specifying PARAM that were not used when computing PARAM BSTR is an error,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,allow_unused,"If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,allow_unused,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.grad,allow_unused,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.send,tensor,Tensor to send.,D_STRUCTURE to send,,D_STRUCTURE,,,,,
torch.distributed.send,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,,,
torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,,,
torch.empty_strided,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,,,
torch.empty_strided,device,"DD: `torch.device`, optional",torch device optional,,,,,,,
torch.empty_strided,device,DF: None,DEFAULT None,,,,,,,
torch.nn.MaxPool1d,kernel_size,the size of the window to take a max over,the size of the window to take a max over,D_TYPE,,,,,"[0,inf)",
torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If it is an D_TYPE vector norm will be calculated if it is CONSTANT_NUM D_STRUCTURE of D_TYPE matrix norm will be calculated,D_TYPE,D_STRUCTURE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If the value is None matrix norm will be calculated when the PARAM D_STRUCTURE only has two dimensions vector norm will be calculated when the PARAM D_STRUCTURE only has one dimension,D_TYPE,D_STRUCTURE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.norm,dim,"If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",If the PARAM D_STRUCTURE has more than two dimensions the vector norm will be applied to last dimension,D_TYPE,D_STRUCTURE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.norm,dim,"DD: int, 2-tuple of python:ints, 2-list of python:ints, optional",D_TYPE CONSTANT_NUM D_STRUCTURE of python D_TYPE CONSTANT_NUM D_STRUCTURE of python D_TYPE optional,D_TYPE,D_STRUCTURE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.norm,dim,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.hub.load,*args,the corresponding args for callable model.,the corresponding args for callable PARAM,,,,,,,
torch.hub.load,*args,DD: optional,ONE_WORD optional,,,,,,,
torch.addbmm,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.addbmm,beta,"DD: Number, optional",Number optional,D_TYPE,,,,CONSTANT_VAL,,
torch.addbmm,beta,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",quantity of parameters to prune,D_TYPE,,,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE should be between CONSTANT_FLOAT and CONSTANT_FLOAT and represent the fraction of parameters to prune,D_TYPE,,,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_structured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE it represents the absolute number of parameters to prune,D_TYPE,,,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_structured,amount,DD: int or float,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.Upsample,size,output spatial sizes,output spatial sizes,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.Upsample,size,"DD: int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional",D_TYPE or D_STRUCTURE BSTR or D_STRUCTURE BSTR or D_STRUCTURE BSTR optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.Upsample,size,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.mode,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.mode,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.TransformerDecoderLayer,dropout,the dropout value (default=0.1).,the dropout value default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.TransformerDecoderLayer,dropout,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.sparse.addmm,mat1,a sparse matrix to be multiplied,a sparse matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,,
torch.sparse.addmm,mat1,DD: SparseTensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.dropout,inplace,"If set to `True`, will do this operation in-place. Default: `False`",If set to CONSTANT_BOOL will do this operation in place,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.dropout,inplace,"If set to `True`, will do this operation in-place. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.dropout,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ifft,normalized,controls whether to return normalized results. Default: `False`,controls whether to return normalized results,D_TYPE,,,,CONSTANT_VAL,,
torch.ifft,normalized,controls whether to return normalized results. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ifft,normalized,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.ifft,normalized,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",Check if the same inputs run through traced code produce the same outputs,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.trace,check_trace,"Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.",You might want to disable this if for example your network contains non deterministic ops or if you are sure that the network is correct despite a checker failure,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.trace,check_trace,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.jit.trace,check_trace,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ger,vec2,1-D input vector,CONSTANT_NUM D PARAM vector,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.ger,vec2,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.svd,some,controls the shape of returned U and V,controls the shape of returned U and V,D_TYPE,,,,CONSTANT_VAL,,
torch.svd,some,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.svd,some,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.all_gather,group,The process group to work on,The process group to work on,,,,,,,
torch.distributed.all_gather,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,,,
torch.distributed.all_gather,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.std,unbiased,whether to use the unbiased estimation or not,whether to use the unbiased estimation or not,D_TYPE,,,,CONSTANT_VAL,,
torch.std,unbiased,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.std,unbiased,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.PoissonNLLLoss,size_average,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.PoissonNLLLoss,size_average,DF: None,DEFAULT None,,,,,,,
torch.nn.MaxPool3d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool3d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ones,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,D_TYPE,,,,,,
torch.ones,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,D_TYPE,,,,,,
torch.ones,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,,
torch.ones,dtype,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.triu_indices,dtype,"the desired data type of returned tensor. Default: if `None`, `torch.long`.",the desired data type of returned D_STRUCTURE,D_TYPE,,,,,,
torch.triu_indices,dtype,"the desired data type of returned tensor. Default: if `None`, `torch.long`.",Default if QSTR D_TYPE,D_TYPE,,,,,,
torch.triu_indices,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,,
torch.triu_indices,dtype,DF: torch.long,DEFAULT D_TYPE,D_TYPE,,,,,,
torch.multinomial,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.multinomial,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.multinomial,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",If CONSTANT_BOOL gradient w r t,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",PARAM will be a sparse D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.embedding,sparse,"If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.",See Notes under torch nn Embedding for more details regarding sparse gradients,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.embedding,sparse,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.embedding,sparse,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.SyncBatchNorm,process_group,synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world,synchronization of stats happen within each process group individually,,,,,,,
torch.nn.SyncBatchNorm,process_group,synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world,Default behavior is synchronization across the whole world,,,,,,,
torch.nn.SyncBatchNorm,process_group,DF: None,DEFAULT None,,,,,,,
torch.nn.Fold,padding,implicit zero padding to be added on both sides of input. Default: 0,implicit zero padding to be added on both sides of input,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,padding,implicit zero padding to be added on both sides of input. Default: 0,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Fold,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,,,QSTR
torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,,,QSTR
torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,,,QSTR
torch.nn.MultiLabelSoftMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,,,QSTR
torch.nn.MultiLabelSoftMarginLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,QSTR
torch.nn.MultiLabelSoftMarginLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.sparse.sum,dtype,the desired data type of returned Tensor. Default: dtype of `input`.,the desired data type of returned D_STRUCTURE,D_TYPE,,,,,,
torch.sparse.sum,dtype,the desired data type of returned Tensor. Default: dtype of `input`.,Default dtype of PARAM,D_TYPE,,,,,,
torch.sparse.sum,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,,
torch.sparse.sum,dtype,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",the size of the padding,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad1d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,D_TYPE,,D_STRUCTURE,,,,
torch.nn.ReplicationPad1d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.load,**pickle_load_args,"(Python 3 only) optional keyword arguments passed over to `pickle_module.load()` and `pickle_module.Unpickler()`, e.g., `errors=...`.",BSTR and PARAM Unpickler e g errors,,,,,,,
torch.nn.Transformer,num_decoder_layers,the number of sub-decoder-layers in the decoder (default=6).,the number of sub decoder layers in the decoder default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.Transformer,num_decoder_layers,DF: 6,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.utils.checkpoint.checkpoint,preserve_rng_state,Omit stashing and restoring the RNG state during each checkpoint.,Omit stashing and restoring the RNG state during each checkpoint,D_TYPE,,,,,,
torch.utils.checkpoint.checkpoint,preserve_rng_state,"DD: bool, optional, default=True",D_TYPE optional default CONSTANT_BOOL,D_TYPE,,,,,,
torch.utils.checkpoint.checkpoint,preserve_rng_state,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",if CONSTANT_BOOL checks that the input contains PARAM sorted by length in a decreasing order,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",If CONSTANT_BOOL this condition is not checked,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_sequence,enforce_sorted,"if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_sequence,enforce_sorted,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_sequence,enforce_sorted,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.randperm,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.randperm,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.randperm,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.neg,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.neg,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.neg,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.rot90,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.rot90,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.Embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.matmul,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.matmul,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.matmul,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",if CONSTANT_BOOL the PARAM is expected to contain sequences sorted by length in a decreasing order,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",If CONSTANT_BOOL the PARAM will get sorted unconditionally,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,enforce_sorted,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.combinations,with_replacement,whether to allow duplication in combination,whether to allow duplication in combination,D_TYPE,,,,CONSTANT_VAL,,
torch.combinations,with_replacement,"DD: boolean, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.combinations,with_replacement,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,,,QSTR
torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,,,QSTR
torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,,,QSTR
torch.nn.functional.binary_cross_entropy_with_logits,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,,,QSTR
torch.nn.functional.binary_cross_entropy_with_logits,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,QSTR
torch.nn.functional.binary_cross_entropy_with_logits,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.nn.MaxPool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool1d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.TransformerDecoder,norm,the layer normalization component (optional).,the layer normalization component BSTR,,,,,,,
torch.nn.TransformerDecoder,norm,DF: None,DEFAULT None,,,,,,,
torch.nn.EmbeddingBag,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.EmbeddingBag,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.EmbeddingBag,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.EmbeddingBag,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.roll,shifts,"The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",The number of places by which the elements of the D_STRUCTURE are shifted,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.roll,shifts,"The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",If shifts is a D_STRUCTURE PARAM must be a D_STRUCTURE of the same size and each dimension will be rolled by the corresponding value,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.roll,shifts,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.autograd.functional.vjp,func,a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,a Python function that takes D_STRUCTURE PARAM and returns a D_STRUCTURE of D_STRUCTURE,,,,,,,
torch.autograd.functional.vjp,func,DD: function,ONE_WORD function,,,,,,,
torch.matrix_rank,symmetric,indicates whether `input` is symmetric. Default: `False`,indicates whether PARAM is symmetric,D_TYPE,,,,CONSTANT_VAL,,
torch.matrix_rank,symmetric,indicates whether `input` is symmetric. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.matrix_rank,symmetric,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.matrix_rank,symmetric,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.gather,index,the indices of elements to gather,the indices of elements to gather,D_TYPE,,,,,,
torch.gather,index,DD: LongTensor,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.round,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.round,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.poisson,generator,a pseudorandom number generator for sampling,a pseudorandom number generator for sampling,,,,,,,
torch.poisson,generator,"DD: `torch.Generator`, optional",torch Generator optional,,,,,,,
torch.poisson,generator,DF: None,DEFAULT None,,,,,,,
torch.log1p,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.log1p,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.RNN,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.RNN,hidden_size,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.nll_loss,input,"(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",BSTR where C number of classes or BSTR where K REXPR in the case of K dimensional loss,,,,BSTR,CONSTANT_VAL,,
torch.renorm,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.renorm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the vjp for said PARAM which is the expected mathematical value,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vjp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vjp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vjp,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.can_cast,from,The original `torch.dtype`.,The original D_TYPE,,,,,,,
torch.can_cast,from,DD: dpython:type,dpython type,,,,,,,
torch.cartesian_prod,*tensors,any number of 1 dimensional tensors.,any number of CONSTANT_NUM dimensional D_STRUCTURE,,,D_STRUCTURE,,,,
torch.utils.data.random_split,dataset,Dataset to be split,Dataset to be split,,,,,,,
torch.utils.data.random_split,dataset,DD: Dataset,ONE_WORD Dataset,,,,,,,
torch.nn.BCEWithLogitsLoss,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,a PARAM of positive examples,D_TYPE,D_STRUCTURE,,,,,
torch.nn.BCEWithLogitsLoss,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,Must be a vector with length equal to the number of classes,D_TYPE,D_STRUCTURE,,,,,
torch.nn.BCEWithLogitsLoss,pos_weight,"DD: Tensor, optional",D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,,,,
torch.nn.BCEWithLogitsLoss,pos_weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,,
torch.nn.NLLLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.NLLLoss,size_average,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.NLLLoss,size_average,DF: None,DEFAULT None,,,,,,,
torch.gt,input,the tensor to compare,the D_STRUCTURE to compare,,D_STRUCTURE,,,,,
torch.gt,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.acos,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.acos,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.baddbmm,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.baddbmm,beta,"DD: Number, optional",Number optional,D_TYPE,,,,CONSTANT_VAL,,
torch.baddbmm,beta,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.trapz,dim,"The dimension along which to integrate.By default, use the last dimension.",The dimension along which to integrate By default use the last dimension,D_TYPE,,,,CONSTANT_VAL,,
torch.trapz,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.trapz,dim,DF: -1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,padding,`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0,PARAM BSTR padding zero padding will be added to both sides of each dimension in the input,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,padding,`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,padding,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.parallel.data_parallel,device_ids,GPU ids on which to replicate module,GPU ids on which to replicate PARAM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.parallel.data_parallel,device_ids,DD: list of python:int or torch.device,D_STRUCTURE of python D_TYPE or torch device,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.parallel.data_parallel,device_ids,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.quantization.quantize,mapping,correspondence between original module types and quantized counterparts,correspondence between original module types and quantized counterparts,,,,,,,
torch.quantization.quantize,mapping,DF: None,DEFAULT None,,,,,,,
torch.onnx.export,output_names,"names to assign to the output nodes of the graph, in order",names to assign to the output nodes of the graph in order,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.onnx.export,output_names,"DD: list of strings, default empty list",D_STRUCTURE of D_TYPE default empty D_STRUCTURE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.onnx.export,output_names,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.distributed.isend,tensor,Tensor to send.,D_STRUCTURE to send,,D_STRUCTURE,,,,,
torch.distributed.isend,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.GRUCell,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.var,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,,,CONSTANT_VAL,,
torch.var,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.var,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.normalize,out,"the output tensor. If `out` is used, this operation won't be differentiable.",the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.functional.normalize,out,"the output tensor. If `out` is used, this operation won't be differentiable.",If QSTR is used this operation won t be differentiable,,D_STRUCTURE,,,,,
torch.nn.functional.normalize,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.nn.functional.normalize,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.UpsamplingNearest2d,size,output spatial sizes,output spatial sizes,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.UpsamplingNearest2d,size,"DD: int or Tuple[int, int], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.UpsamplingNearest2d,size,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,The compiler executable name to check e g,D_TYPE,,,,,,
torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,ONE_WORD QSTR,D_TYPE,,,,,,
torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.,Must be executable in a shell process,D_TYPE,,,,,,
torch.utils.cpp_extension.check_compiler_abi_compatibility,compiler,DD: str,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.nn.utils.prune.ln_structured,name,parameter name within `module` on which pruning will act.,parameter name within PARAM on which pruning will act,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.prune.ln_structured,name,DD: str,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.cross_entropy,target,"(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",BSTR where each value is CONSTANT_NUM REXPR BSTR REXPR CONSTANT_NUM or BSTR where K REXPR for K dimensional loss,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.functional.cross_entropy,target,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.mv,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.mv,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.mv,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",If CONSTANT_BOOL gradient w r t,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",QSTR matrix will be a sparse D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,sparse,"If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.",See Notes for more details regarding sparse gradients,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,sparse,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Embedding,sparse,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,,,QSTR
torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,,,QSTR
torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,,,QSTR
torch.nn.MSELoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,,,QSTR
torch.nn.MSELoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,QSTR
torch.nn.MSELoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.nn.MaxPool1d,dilation,a parameter that controls the stride of elements in the window,a parameter that controls the PARAM of elements in the window,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool1d,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv2d,input,"quantized input tensor of shape (minibatch , in _channels , iH , iW) ",quantized input D_STRUCTURE of shape BSTR,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.log10,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.log10,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.rfft,input,the input tensor of at least `signal_ndim` dimensions,the input D_STRUCTURE of at least PARAM dimensions,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.rfft,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.GRU,dropout,"If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0",If non zero introduces a Dropout layer on the outputs of each GRU layer except the last layer with dropout probability equal to QSTR,,,,,,,
torch.nn.GRU,dropout,"If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0",Default CONSTANT_NUM,,,,,,,
torch.nn.GRU,dropout,DF: None,DEFAULT None,,,,,,,
torch.arange,end,the ending value for the set of points,the ending value for the set of points,D_TYPE,,,,,,
torch.arange,end,DD: Number,ONE_WORD Number,D_TYPE,,,,,,
torch.cummin,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.cummin,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",the stride of the convolving kernel,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,stride,"the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,stride,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",if specified it will be used as divisor otherwise size of the pooling region will be used,,,,,,,
torch.nn.functional.avg_pool2d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",Default None,,,,,,,
torch.nn.functional.avg_pool2d,divisor_override,DF: None,DEFAULT None,,,,,,,
torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",additional size added to one side of each dimension in the output shape,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,output_padding,"additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,output_padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.diag,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.diag,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.diag,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.quantization.fuse_modules,fuser_func,"Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",Function that takes in a D_STRUCTURE of modules and outputs a D_STRUCTURE of fused modules of the same length,,,,,,,
torch.quantization.fuse_modules,fuser_func,"Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",For example fuser_func BSTR returns the D_STRUCTURE BSTR Defaults to torch quantization fuse_known_modules,,,,,,,
torch.quantization.fuse_modules,fuser_func,DF: <functionfuse_known_modules>,DEFAULT REXPR,,,,,,,
torch.quantization.convert,mapping,"a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",a D_STRUCTURE that maps from D_TYPE PARAM type to quantized PARAM type can be overwrritten to allow swapping user defined Modules,,,D_STRUCTURE,,,,
torch.quantization.convert,mapping,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.Hardtanh,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Hardtanh,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Hardtanh,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",ONE_WORD DEPRECATED,D_TYPE,,,,CONSTANT_VAL,,
torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",use PARAM export the PARAM in aten mode,D_TYPE,,,,CONSTANT_VAL,,
torch.onnx.export,aten,"[DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.",If using aten mode all the ops original exported by the functions in symbolic_opset REXPR REXPR are exported as ATen ops,D_TYPE,,,,CONSTANT_VAL,,
torch.onnx.export,aten,"DD: bool, default False",D_TYPE default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.onnx.export,aten,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",the stride of the convolving kernel,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",Can be a single number or a one element D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv1d,stride,"the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1",Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv1d,stride,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.quantization.quantize,inplace,"carry out model transformations in-place, the original module is mutated",carry out PARAM transformations in place the original module is mutated,D_TYPE,,,,CONSTANT_VAL,,
torch.quantization.quantize,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.fmod,input,the dividend,the dividend,,D_STRUCTURE,,,,,
torch.fmod,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.transpose,dim1,the second dimension to be transposed,the second dimension to be transposed,D_TYPE,,,,,,
torch.transpose,dim1,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.nn.MaxPool3d,stride,the stride of the window. Default value is `kernel_size`,the stride of the window,,,,,,,
torch.nn.MaxPool3d,stride,the stride of the window. Default value is `kernel_size`,Default value is PARAM,,,,,,,
torch.nn.MaxPool3d,stride,DF: None,DEFAULT None,,,,,,,
torch.randperm,dtype,the desired data type of returned tensor. Default: `torch.int64`.,the desired data type of returned D_STRUCTURE,D_TYPE,,,,,,
torch.randperm,dtype,the desired data type of returned tensor. Default: `torch.int64`.,Default D_TYPE,D_TYPE,,,,,,
torch.randperm,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,,,
torch.randperm,dtype,DF: torch.int64,DEFAULT D_TYPE,D_TYPE,,,,,,
torch.nn.MaxUnpool3d,padding,Padding that was added to the input,Padding that was added to the input,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.MaxUnpool3d,padding,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.MaxUnpool3d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.prod,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,,,CONSTANT_VAL,,
torch.prod,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.prod,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.quantization.convert,inplace,"carry out model transformations in-place, the original module is mutated",carry out model transformations in place the original PARAM is mutated,D_TYPE,,,,CONSTANT_VAL,,
torch.quantization.convert,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.addr,vec1,the first vector of the outer product,the first vector of the outer product,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.addr,vec1,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool3d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",if specified it will be used as divisor otherwise size of the pooling region will be used,,,,,,,
torch.nn.functional.avg_pool3d,divisor_override,"if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None",Default None,,,,,,,
torch.nn.functional.avg_pool3d,divisor_override,DF: None,DEFAULT None,,,,,,,
torch.poisson,input*,the input tensor containing the rates of the Poisson distribution,the input D_STRUCTURE containing the rates of the Poisson distribution,,,,,,,
torch.poisson,input*,DD: Tensor,ONE_WORD D_STRUCTURE,,,,,,,
torch.matrix_power,n,the power to raise the matrix to,the power to raise the matrix to,D_TYPE,,,,,,
torch.matrix_power,n,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,,
torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,,
torch.nn.SoftMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.SoftMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.SoftMarginLoss,reduce,DF: None,DEFAULT None,,,,,,,
torch.nn.Conv3d,stride,Stride of the convolution. Default: 1,Stride of the convolution,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv3d,stride,Stride of the convolution. Default: 1,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv3d,stride,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv3d,stride,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.ormqr,input2,the tau from `torch.geqrf()`.,the tau from torch geqrf,,D_STRUCTURE,,,,,
torch.ormqr,input2,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.randn_like,input,the size of `input` will determine size of the output tensor.,the size of QSTR will determine size of the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.randn_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.HingeEmbeddingLoss,size_average,DF: None,DEFAULT None,,,,,,,
torch.pow,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.pow,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.pow,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.median,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.median,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",CUDA devices for which to fork the RNG,,,D_STRUCTURE,,,,
torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",CPU RNG state is always forked,,,D_STRUCTURE,,,,
torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",By default fork_rng operates on all devices but will emit a warning if your machine has a lot of devices since this function will run very slowly in that case,,,D_STRUCTURE,,,,
torch.random.fork_rng,devices,"CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed",If you explicitly specify devices this warning will be suppressed,,,D_STRUCTURE,,,,
torch.random.fork_rng,devices,DD: iterable of CUDA IDs,D_STRUCTURE of CUDA IDs,,,D_STRUCTURE,,,,
torch.random.fork_rng,devices,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.AvgPool2d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AvgPool2d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.stft,n_fft,size of Fourier transform,size of Fourier transform,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.stft,n_fft,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",Geometrically we consider the pixels of the PARAM and output as squares rather than points,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the center points of their corner pixels preserving the values at the corner pixels,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the corner points of their corner pixels and the interpolation uses edge value padding for out of boundary values making this operation independent of PARAM PARAM when PARAM is kept the same,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",This only has an effect when PARAM is QSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.interpolate,align_corners,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.parallel.data_parallel,module,the module to evaluate in parallel,the module to evaluate in parallel,,,,,,,
torch.nn.parallel.data_parallel,module,DD: Module,ONE_WORD Module,,,,,,,
torch.nn.Upsample,mode,"the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`",the upsampling algorithm one of QSTR,D_TYPE,,,,,,
torch.nn.Upsample,mode,"the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`",Default QSTR,D_TYPE,,,,,,
torch.nn.Upsample,mode,"DD: str, optional",D_TYPE optional,D_TYPE,,,,,,
torch.nn.Upsample,mode,DF: nearest,DEFAULT DF_STR,D_TYPE,,,,,,
torch.set_printoptions,sci_mode,"Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by _Formatter",Enable BSTR scientific notation,,,,,,,
torch.set_printoptions,sci_mode,"Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by _Formatter",If None BSTR is specified the value is defined by Formatter,,,,,,,
torch.set_printoptions,sci_mode,DF: None,DEFAULT None,,,,,,,
torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,whether the output D_STRUCTURE have PARAM retained or not,D_TYPE,,,,CONSTANT_VAL,,
torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,Ignored if PARAM QSTR and PARAM QSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.norm,keepdim,whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.norm,keepdim,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.norm,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.diagflat,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.diagflat,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.distributed.recv,src,Source rank. Will receive from any process if unspecified.,Source rank,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.recv,src,Source rank. Will receive from any process if unspecified.,Will receive from any process if unspecified,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.recv,src,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.recv,src,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,,
torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,,
torch.nn.MSELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.MSELoss,reduce,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.MSELoss,reduce,DF: None,DEFAULT None,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,,,
torch.nn.SoftMarginLoss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.SoftMarginLoss,size_average,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.SoftMarginLoss,size_average,DF: None,DEFAULT None,,,,,,,
torch.cummax,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.cummax,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.MultiMarginLoss,weight,"a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.",a manual rescaling weight given to each class,D_TYPE,D_STRUCTURE,,,,,
torch.nn.MultiMarginLoss,weight,"a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.",If given it has to be a D_STRUCTURE of size C Otherwise it is treated as if having all ones,D_TYPE,D_STRUCTURE,,,,,
torch.nn.MultiMarginLoss,weight,"DD: Tensor, optional",D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,,,,
torch.nn.MultiMarginLoss,weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.nn.FractionalMaxPool2d,kernel_size,"the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)",the size of the window to take a max over,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,"[0,inf)",
torch.nn.FractionalMaxPool2d,kernel_size,"the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)",Can be a single number k BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,"[0,inf)",
torch.imag,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.imag,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.load,f,"a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",a file like object has to implement read meth QSTR meth QSTR and meth QSTR or a D_TYPE containing a file name,D_TYPE,,,,,,
torch.bmm,input,the first batch of matrices to be multiplied,the first batch of matrices to be multiplied,D_TYPE,D_STRUCTURE,,,,,
torch.bmm,input,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.hub.load_state_dict_from_url,url,URL of the object to download,URL of the object to download,D_TYPE,,,,,,
torch.hub.load_state_dict_from_url,url,DD: string,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.reciprocal,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.reciprocal,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.reciprocal,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,,,
torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,,,
torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,,,
torch.nn.functional.nll_loss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,,,
torch.nn.functional.nll_loss,reduce,"DD: bool, optional",D_TYPE optional,,,,,,,
torch.nn.functional.nll_loss,reduce,DF: None,DEFAULT None,,,,,,,
torch.nn.AvgPool3d,count_include_pad,"when True, will include the zero-padding in the averaging calculation",when CONSTANT_BOOL will include the zero PARAM in the averaging calculation,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AvgPool3d,count_include_pad,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.pinverse,rcond,A floating point value to determine the cutoff for small singular values. Default: 1e-15,A D_TYPE point value to determine the cutoff for small singular values,D_TYPE,,,,CONSTANT_VAL,,
torch.pinverse,rcond,A floating point value to determine the cutoff for small singular values. Default: 1e-15,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.pinverse,rcond,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.pinverse,rcond,DF: 1e-15,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",maximum number of iterations,D_TYPE,,,,,,
torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",When reached the iteration process is hard stopped and the current approximation of eigenpairs is returned,D_TYPE,,,,,,
torch.lobpcg,niter,"maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.",For infinite iteration but until convergence criteria is met use CONSTANT_NUM,D_TYPE,,,,,,
torch.lobpcg,niter,"DD: int, optional",D_TYPE optional,D_TYPE,,,,,,
torch.lobpcg,niter,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.quantized.functional.conv3d,groups,"split input into groups, in _channels  should be divisible by the number of groups. Default: 1",split PARAM into groups in channels should be divisible by the number of groups,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,groups,"split input into groups, in _channels  should be divisible by the number of groups. Default: 1",Default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,groups,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.distributed.scatter,tensor,Output tensor.,Output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.distributed.scatter,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",if CONSTANT_BOOL will return the indices along with the outputs,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",Useful to pass to nn MaxUnpool2d,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool2d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool2d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",if CONSTANT_BOOL the corner pixels of the input and output D_STRUCTURE are aligned and thus preserving the values at those pixels,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",This only has effect when PARAM is QSTR,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Upsample,align_corners,"if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Upsample,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Upsample,align_corners,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.topk,dim,the dimension to sort along,the dimension to sort along,D_TYPE,,,,CONSTANT_VAL,,
torch.topk,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.topk,dim,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.zeros_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.zeros_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,D_TYPE,,,,CONSTANT_VAL,,
torch.zeros_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.zeros_like,dtype,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.BatchNorm2d,num_features,"C  from an expected input of size (N, C, H, W) ",C from an expected input of size BSTR,,,,BSTR,CONSTANT_VAL,,
torch.eye,m,the number of columns with default being `n`,the number of columns with default being QSTR,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.eye,m,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.eye,m,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.init.dirac_,groups,number of groups in the conv layer (default: 1),number of groups in the conv layer default CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.init.dirac_,groups,DD: optional,ONE_WORD optional,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.init.dirac_,groups,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.utils.data.random_split,lengths,lengths of splits to be produced,lengths of splits to be produced,,,D_STRUCTURE,,,,
torch.utils.data.random_split,lengths,DD: sequence,ONE_WORD D_STRUCTURE,,,D_STRUCTURE,,,,
torch.quantization.quantize,run_args,positional arguments for run_fn,positional arguments for PARAM,,,,,,,
torch.hub.list,force_reload,whether to discard the existing cache and force a fresh download. Default is False.,whether to discard the existing cache and force a fresh download,D_TYPE,,,,CONSTANT_VAL,,
torch.hub.list,force_reload,whether to discard the existing cache and force a fresh download. Default is False.,Default is CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.hub.list,force_reload,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.hub.list,force_reload,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool3d,input,"input tensor (minibatch , in _channels , iT  times iH , iW) ",input D_STRUCTURE BSTR,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,weight,"filters of shape (in _channels ,  out _channels/groups , kH , kW) ",filters of shape BSTR,,,,BSTR,CONSTANT_VAL,,
torch.set_default_tensor_type,t,the floating point tensor type or its name,the D_TYPE point D_STRUCTURE type or its name,D_TYPE,,,,,,
torch.set_default_tensor_type,t,DD: type or string,type or D_TYPE,D_TYPE,,,,,,
torch.nn.functional.grid_sample,input,"input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",input of shape BSTR,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.nn.functional.grid_sample,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL both the output and result will be computed in a differentiable way,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vhp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vhp,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.vhp,create_graph,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.empty_like,input,the size of `input` will determine size of the output tensor.,the size of QSTR will determine size of the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.empty_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.cuda.comm.reduce_add,inputs,an iterable of tensors to add.,an D_STRUCTURE of D_STRUCTURE to add,,,D_STRUCTURE,,,,
torch.cuda.comm.reduce_add,inputs,DD: Iterable[Tensor],D_STRUCTURE BSTR,,,D_STRUCTURE,,,,
torch.conj,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.conj,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.conj,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.erf,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.erf,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.erf,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.chunk,dim,dimension along which to split the tensor,dimension along which to split the D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.chunk,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.chunk,dim,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.sum,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,,
torch.sum,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.set_rng_state,new_state,The desired state,The desired state,D_TYPE,,,,,,
torch.set_rng_state,new_state,DD: torch.ByteTensor,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.nn.UpsamplingNearest2d,scale_factor,multiplier for spatial size.,multiplier for spatial PARAM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.UpsamplingNearest2d,scale_factor,"DD: float or Tuple[float, float], optional",D_TYPE or D_STRUCTURE BSTR optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.UpsamplingNearest2d,scale_factor,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.full,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,,,
torch.full,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,,,
torch.full,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,,,
torch.full,layout,DF: torch.strided,torch strided,,,,,,,
torch.cumsum,dim,the dimension to do the operation over,the dimension to do the operation over,D_TYPE,,,,CONSTANT_VAL,,
torch.cumsum,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.cat,dim,the dimension over which the tensors are concatenated,the dimension over which the D_STRUCTURE are concatenated,D_TYPE,,,,CONSTANT_VAL,,
torch.cat,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.cat,dim,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxUnpool3d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,Stride of the max pooling window,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool3d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool3d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool3d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,,,
torch.multinomial,replacement,whether to draw with replacement or not,whether to draw with replacement or not,D_TYPE,,,,CONSTANT_VAL,,
torch.multinomial,replacement,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.multinomial,replacement,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Conv1d,dilation,Spacing between kernel elements. Default: 1,Spacing between kernel elements,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv1d,dilation,Spacing between kernel elements. Default: 1,Default CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv1d,dilation,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Conv1d,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.repeat_interleave,dim,"The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",The dimension along which to repeat values By default use the flattened PARAM D_STRUCTURE and return a flat outputarray,D_TYPE,,,,CONSTANT_VAL,,
torch.repeat_interleave,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.repeat_interleave,dim,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.svd,compute_uv,option whether to compute U and V or not,option whether to compute U and V or not,D_TYPE,,,,CONSTANT_VAL,,
torch.svd,compute_uv,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.svd,compute_uv,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.utils.cpp_extension.load_inline,functions,"A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",A D_STRUCTURE of function names for which to generate function bindings,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.utils.cpp_extension.load_inline,functions,"A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",If a D_STRUCTURE is given it should map function names to docstrings BSTR,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.utils.cpp_extension.load_inline,functions,DF: None,DEFAULT None,,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.kl_div,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,Specifies the reduction to apply to the output QSTR QSTR QSTR QSTR,D_TYPE,,,,,,QSTR
torch.nn.functional.kl_div,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,QSTR no reduction will be applied QSTR the sum of the output will be divided by the batchsize QSTR the output will be summed QSTR the output will be divided by the number of elements in the output Default QSTR,D_TYPE,,,,,,QSTR
torch.nn.functional.kl_div,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,,,QSTR
torch.nn.functional.kl_div,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",stride of the pooling operation,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Default PARAM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool2d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PairwiseDistance,keepdim,Determines whether or not to keep the vector dimension. Default: False,Determines whether or not to keep the vector dimension,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PairwiseDistance,keepdim,Determines whether or not to keep the vector dimension. Default: False,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PairwiseDistance,keepdim,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.PairwiseDistance,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.div,other,the number to be divided to each element of `input`,the number to be divided to each element of PARAM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.div,other,DD: Number,ONE_WORD Number,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.set_grad_enabled,mode,"Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients.",Flag whether to enable grad CONSTANT_BOOL or disable CONSTANT_BOOL,D_TYPE,,,,,,
torch.set_grad_enabled,mode,"Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients.",This can be used to conditionally enable gradients,D_TYPE,,,,,,
torch.set_grad_enabled,mode,DD: bool,ONE_WORD D_TYPE,D_TYPE,,,,,,
torch.nn.GRU,hidden_size,The number of features in the hidden state h,The number of features in the hidden state h,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.GRU,hidden_size,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.ones_like,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,D_TYPE,,,,CONSTANT_VAL,,
torch.ones_like,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ones_like,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,,,CONSTANT_VAL,,
torch.ones_like,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.argmax,dim,"the dimension to reduce. If `None`, the argmax of the flattened input is returned.",the dimension to reduce,D_TYPE,,,,CONSTANT_VAL,,
torch.argmax,dim,"the dimension to reduce. If `None`, the argmax of the flattened input is returned.",If QSTR the argmax of the flattened PARAM is returned,D_TYPE,,,,CONSTANT_VAL,,
torch.argmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,CONSTANT_VAL,,
torch.cholesky_solve,input2,"input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",PARAM matrix u of size BSTR where is zero of more batch dimensions composed of PARAM or lower triangular Cholesky factor,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.cholesky_solve,input2,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,BSTR,CONSTANT_VAL,,
torch.mv,input,matrix to be multiplied,matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,,
torch.mv,input,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",the desired data type of returned D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",If specified the PARAM D_STRUCTURE is casted to QSTR before the operation is performed,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",This is useful for preventing data type overflows,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,"the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.",Default None,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.softmax,dtype,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.acos,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,,
torch.acos,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,,
torch.acos,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.orgqr,input2,the tau from `torch.geqrf()`.,the tau from torch geqrf,,D_STRUCTURE,,,,,
torch.orgqr,input2,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,,
torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",if CONSTANT_BOOL will return the indices along with the outputs,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",Useful to pass to nn MaxUnpool3d,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool3d,return_indices,"if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveMaxPool3d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.linspace,steps,DF: 100,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.TripletMarginLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.logical_and,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.jit.save,_extra_files,DF: ExtraFilesMap{},DEFAULT DF_STR,D_TYPE,,,,,,
torch.nn.utils.spectral_norm,name,DF: weight,DEFAULT DF_STR,D_TYPE,,,,,,
torch.lobpcg,method,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.round,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.CTCLoss,zero_infinity,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.rand,layout,DF: torch.strided,torch strided,,,,,,,
torch.nn.functional.nll_loss,ignore_index,DF: -100,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.addr,beta,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.multinomial,generator,DF: None,DEFAULT None,,,,,,,
torch.nn.RReLU,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.symeig,out,DF: None,DEFAULT None,,,D_STRUCTURE,BSTR,,,
torch.eye,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxUnpool2d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.triu,diagonal,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MultiLabelMarginLoss,reduce,DF: None,DEFAULT None,,,,,,,
torch.nn.ConvTranspose2d,groups,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.distributed.get_world_size,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.nn.CrossEntropyLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.pca_lowrank,niter,DF: 2,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.KLDivLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.empty_like,device,DF: None,DEFAULT None,,,,,,,
torch.nn.BCELoss,reduce,DF: None,DEFAULT None,,,,,,,
torch.utils.cpp_extension.load,extra_cflags,DF: None,DEFAULT None,,,,,,,
torch.nn.BatchNorm3d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxUnpool1d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,,,,
torch.distributed.all_gather,async_op,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,DF: 4.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.mode,out,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.lu,get_infos,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.embedding,padding_idx,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.gt,out,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,batch_first,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.TripletMarginLoss,reduce,DF: None,DEFAULT None,,,,,,,
torch.nn.LSTM,input_size,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.rand_like,dtype,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.hub.load_state_dict_from_url,progress,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.as_strided,storage_offset,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,affine,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.hamming_window,periodic,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.histc,min,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.utils.weight_norm,name,DF: weight,DEFAULT DF_STR,D_TYPE,,,,,,
torch.autograd.functional.vjp,create_graph,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.ones,device,DF: None,DEFAULT None,,,,,,,
torch.sum,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool2d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.logsumexp,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.addcdiv,value,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.quantization.propagate_qconfig_,qconfig_dict,DF: None,DEFAULT None,,,D_STRUCTURE,,,,
torch.nn.init.uniform_,a,DF: 0.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.argsort,descending,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.MaxPool2d,stride,DF: None,DEFAULT None,,,,,,,
torch.sparse_coo_tensor,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.log1p,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.autograd.functional.vhp,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.kthvalue,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.min,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.adaptive_max_pool3d,return_indices,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.NLLLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,,,QSTR
torch.utils.cpp_extension.load,is_python_module,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.linspace,layout,DF: torch.strided,torch strided,,,,,,,
torch.clamp,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.quantized.functional.conv3d,scale,DF: 1.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,alpha,DF: 0.0001,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.RNN,input_size,DF: None,DEFAULT None,D_TYPE,,,,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.dropout3d,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,full,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.empty_like,layout,DF: None,DEFAULT None,,,,,,,
torch.autograd.functional.hessian,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.gumbel_softmax,dim,DF: -1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.eye,dtype,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.functional.embedding,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.BatchNorm1d,affine,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.logical_not,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.Softshrink,lambd,DF: 0.5,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.Unfold,stride,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.binary_cross_entropy_with_logits,weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,,
torch.distributed.reduce,group,DF: <objectobject>,DEFAULT REXPR,,,,,,,
torch.sin,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.nn.CosineSimilarity,dim,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.gradgradcheck,grad_outputs,DF: None,DEFAULT None,,D_STRUCTURE,D_STRUCTURE,,,,
torch.autograd.gradcheck,rtol,DF: 0.001,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.L1Loss,size_average,DF: None,DEFAULT None,,,,,,,
torch.nn.Transformer,activation,DF: relu,DEFAULT DF_STR,D_TYPE,,,,,,
torch.logical_or,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.bartlett_window,dtype,DF: None,DEFAULT None,D_TYPE,,,,,,
torch.nn.ConvTranspose3d,bias,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.log,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,,
torch.onnx.export,verbose,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.InstanceNorm3d,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.CELU,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,,,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,stride,DF: None,DEFAULT None,D_TYPE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
