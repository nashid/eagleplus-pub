constraints:
  Tout:
    descp: A list of tf.DTypes that has length >= 1. the type list for the input list.
    dtype:
    - tf.dtype
    ndim:
    - '1'
    shape:
    - '[>=1]'
    structure:
    - list(tf.dtype)
  f:
    descp: A function decorated with @Defun. The function we want to compute the gradient
      for. The function 'f' must be a numerical function which takes N inputs and
      produces M outputs. Its gradient function 'g', which is computed by this SymbolicGradient
      op is a function taking N + M inputs and produces N outputs. I.e. if we have
      (y1, y2, ..., y_M) = f(x1, x2, ..., x_N), then, g is (dL/dx1, dL/dx2, ..., dL/dx_N)
      = g(x1, x2, ..., x_N, dL/dy1, dL/dy2, ..., dL/dy_M), where L is a scalar-value
      function of (x1, x2, ..., xN) (e.g., the loss function). dL/dx_i is the partial
      derivative of L with respect to x_i. (Needs some math expert to say the comment
      above better.)
    dtype:
    - callable
  input:
    descp: A list of Tensor objects. a list of input tensors of size N + M;
    structure:
    - list
    - list(tf.tensor)
  name:
    default: None
    descp: A name for the operation (optional).
    dtype:
    - tf.string
    ndim:
    - '0'
inputs:
  optional:
  - name
  required:
  - input
  - Tout
  - f
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/SymbolicGradient
outputs:
- A list of Tensor objects of type Tout.
package: tensorflow
target: SymbolicGradient
title: tf.raw_ops.SymbolicGradient
version: 2.3.0
