,idx,freq,sequence,sentence,api,len,cat,pat
0,2,200,"['input', 'tensor']",the input tensor.,torch.std2.yaml,2,tensor_t,^the (first/second) input/output tensor
1,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,2,tensor_t,^the (first/second) input/output tensor
2,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,2,tensor_t,^the (first/second) input/output tensor
3,2,200,"['input', 'tensor']",the input tensor.,torch.real.yaml,2,tensor_t,^the (first/second) input/output tensor
4,2,200,"['input', 'tensor']",the input tensor.,torch.log1p.yaml,2,tensor_t,^the (first/second) input/output tensor
5,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype (dtype),
6,2,200,"['input', 'tensor']",the input tensor.,torch.prod2.yaml,2,tensor_t,^the (first/second) input/output tensor
7,2,200,"['input', 'tensor']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,2,tensor_t,^the (first/second) input/output tensor
8,2,200,"['input', 'tensor']",the input tensor.,torch.pow.yaml,2,tensor_t,^the (first/second) input/output tensor
9,2,200,"['input', 'tensor']",the input tensor.,torch.exp.yaml,2,tensor_t,^the (first/second) input/output tensor
10,2,200,"['input', 'tensor']",the input tensor.,torch.cummax.yaml,2,tensor_t,^the (first/second) input/output tensor
11,2,200,"['input', 'tensor']",the input tensor.,torch.mean2.yaml,2,tensor_t,^the (first/second) input/output tensor
12,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
13,2,200,"['input', 'tensor']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,2,shape,
14,2,200,"['input', 'tensor']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
15,2,200,"['input', 'tensor']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,shape,
16,2,200,"['input', 'tensor']",the input tensor.,torch.ceil.yaml,2,tensor_t,^the (first/second) input/output tensor
17,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,
18,2,200,"['input', 'tensor']",the input tensor.,torch.sum2.yaml,2,tensor_t,^the (first/second) input/output tensor
19,2,200,"['input', 'tensor']",the input tensor.,torch.min22.yaml,2,tensor_t,^the (first/second) input/output tensor
20,2,200,"['input', 'tensor']",the second input tensor,torch.min22.yaml,2,tensor_t,^the (first/second) input/output tensor
21,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,
22,2,200,"['input', 'tensor']",the input tensor.,torch.sum.yaml,2,tensor_t,^the (first/second) input/output tensor
23,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
24,2,200,"['input', 'tensor']",the input tensor.,torch.var_mean.yaml,2,tensor_t,^the (first/second) input/output tensor
25,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
26,2,200,"['input', 'tensor']",the input tensor.,torch.as_strided.yaml,2,tensor_t,^the (first/second) input/output tensor
27,2,200,"['input', 'tensor']",the input tensor.,torch.max22.yaml,2,tensor_t,^the (first/second) input/output tensor
28,2,200,"['input', 'tensor']",the second input tensor,torch.max22.yaml,2,tensor_t,^the (first/second) input/output tensor
29,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
30,2,200,"['input', 'tensor']",the input tensor.,torch.div.yaml,2,tensor_t,^the (first/second) input/output tensor
31,2,200,"['input', 'tensor']",the input tensor.,torch.nonzero.yaml,2,tensor_t,^the (first/second) input/output tensor
32,2,200,"['input', 'tensor']",the input tensor.,torch.sinh.yaml,2,tensor_t,^the (first/second) input/output tensor
33,2,200,"['input', 'tensor']",the input tensor.,torch.var.yaml,2,tensor_t,^the (first/second) input/output tensor
34,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
35,2,200,"['input', 'tensor']",the input tensor.,torch.std_mean.yaml,2,tensor_t,^the (first/second) input/output tensor
50,2,200,"['input', 'tensor']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
37,2,200,"['input', 'tensor']",input tensor,torch.nn.functional.glu.yaml,2,tensor_t,
38,2,200,"['input', 'tensor']",the input tensor.,torch.erf.yaml,2,tensor_t,^the (first/second) input/output tensor
39,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
40,2,200,"['input', 'tensor']",the input tensor.,torch.neg.yaml,2,tensor_t,^the (first/second) input/output tensor
42,2,200,"['input', 'tensor']",the first input tensor,torch.add.yaml,2,tensor_t,^the (first/second) input/output tensor
43,2,200,"['input', 'tensor']",the second input tensor,torch.add.yaml,2,tensor_t,^the (first/second) input/output tensor
44,2,200,"['input', 'tensor']",the input tensor.,torch.diag.yaml,2,tensor_t,^the (first/second) input/output tensor
45,2,200,"['input', 'tensor']",the input tensor.,torch.sigmoid.yaml,2,tensor_t,^the (first/second) input/output tensor
47,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,ndim,
46,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
49,2,200,"['input', 'tensor']",the input tensor.,torch.erfinv.yaml,2,tensor_t,^the (first/second) input/output tensor
48,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
51,2,200,"['input', 'tensor']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
52,2,200,"['input', 'tensor']",the input tensor.,torch.cummin.yaml,2,tensor_t,^the (first/second) input/output tensor
54,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_and.yaml,2,tensor_t,^the (first/second) input/output tensor
55,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_and.yaml,2,tensor_t,^the (first/second) input/output tensor
53,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
56,2,200,"['input', 'tensor']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
57,2,200,"['input', 'tensor']",Input tensor.,torch.distributed.gather.yaml,2,tensor_t,
58,2,200,"['input', 'tensor']",the input tensor.,torch.topk.yaml,2,tensor_t,^the (first/second) input/output tensor
59,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
60,2,200,"['input', 'tensor']",the input tensor.,torch.reciprocal.yaml,2,tensor_t,^the (first/second) input/output tensor
62,2,200,"['input', 'tensor']",the input tensor.,torch.cos.yaml,2,tensor_t,^the (first/second) input/output tensor
63,2,200,"['input', 'tensor']",the input tensor.,torch.masked_select.yaml,2,tensor_t,^the (first/second) input/output tensor
61,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
64,2,200,"['input', 'tensor']",the input tensor.,torch.matrix_power.yaml,2,tensor_t,^the (first/second) input/output tensor
65,2,200,"['input', 'tensor']",the input tensor.,torch.min2.yaml,2,tensor_t,^the (first/second) input/output tensor
66,2,200,"['input', 'tensor']",the input tensor.,torch.angle.yaml,2,tensor_t,^the (first/second) input/output tensor
67,2,200,"['input', 'tensor']",the input tensor.,torch.erfc.yaml,2,tensor_t,^the (first/second) input/output tensor
68,2,200,"['input', 'tensor']",the input tensor.,torch.round.yaml,2,tensor_t,^the (first/second) input/output tensor
69,2,200,"['input', 'tensor']",the input tensor.,torch.polygamma.yaml,2,tensor_t,^the (first/second) input/output tensor
70,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,ndim,
71,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
72,2,200,"['input', 'tensor']",the input tensor.,torch.cross.yaml,2,tensor_t,^the (first/second) input/output tensor
73,2,200,"['input', 'tensor']",the second input tensor,torch.cross.yaml,2,tensor_t,^the (first/second) input/output tensor
74,2,200,"['input', 'tensor']",the input tensor.,torch.argmin2.yaml,2,tensor_t,^the (first/second) input/output tensor
75,2,200,"['input', 'tensor']",the input tensor.,torch.acos.yaml,2,tensor_t,^the (first/second) input/output tensor
76,2,200,"['input', 'tensor']","input tensor (minibatch , in _channels , iH , iW)",torch.nn.functional.avg_pool2d.yaml,2,shape,
77,2,200,"['input', 'tensor']",the input tensor.,torch.numel.yaml,2,tensor_t,^the (first/second) input/output tensor
79,2,200,"['input', 'tensor']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
80,2,200,"['input', 'tensor']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
78,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
81,2,200,"['input', 'tensor']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
82,2,200,"['input', 'tensor']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
85,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
87,2,200,"['input', 'tensor']",the input tensor.,torch.bitwise_not.yaml,2,tensor_t,^the (first/second) input/output tensor
4246,216,8,"['default', 'specified']","Default is ""env://"" if no `init_method` or `store` is specified.",torch.distributed.init_process_group.yaml,2,can't handle,
88,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,ndim,
89,2,200,"['input', 'tensor']",the input tensor.,torch.var_mean2.yaml,2,tensor_t,^the (first/second) input/output tensor
86,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2,not useful,
90,2,200,"['input', 'tensor']",the input tensor.,torch.trunc.yaml,2,tensor_t,^the (first/second) input/output tensor
91,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
92,2,200,"['input', 'tensor']",the input tensor.,torch.conj.yaml,2,tensor_t,^the (first/second) input/output tensor
93,2,200,"['input', 'tensor']",the input tensor.,torch.repeat_interleave.yaml,2,tensor_t,^the (first/second) input/output tensor
94,2,200,"['input', 'tensor']",the input tensor.,torch.median2.yaml,2,tensor_t,^the (first/second) input/output tensor
95,2,200,"['input', 'tensor']",the input tensor.,torch.histc.yaml,2,tensor_t,^the (first/second) input/output tensor
96,2,200,"['input', 'tensor']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
97,2,200,"['input', 'tensor']",the input tensor.,torch.clamp.yaml,2,tensor_t,^the (first/second) input/output tensor
98,2,200,"['input', 'tensor']",the input tensor.,torch.flatten.yaml,2,tensor_t,^the (first/second) input/output tensor
99,2,200,"['input', 'tensor']",the input tensor.,torch.argsort.yaml,2,tensor_t,^the (first/second) input/output tensor
100,2,200,"['input', 'tensor']",the input tensor.,torch.logical_or.yaml,2,tensor_t,^the (first/second) input/output tensor
101,2,200,"['input', 'tensor']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
102,2,200,"['input', 'tensor']",the input tensor.,torch.logsumexp.yaml,2,tensor_t,^the (first/second) input/output tensor
103,2,200,"['input', 'tensor']",the input tensor.,torch.mean.yaml,2,tensor_t,^the (first/second) input/output tensor
104,2,200,"['input', 'tensor']",the input tensor.,torch.sort.yaml,2,tensor_t,^the (first/second) input/output tensor
105,2,200,"['input', 'tensor']",the input tensor.,torch.logical_and.yaml,2,tensor_t,^the (first/second) input/output tensor
106,2,200,"['input', 'tensor']",the input tensor.,torch.sin.yaml,2,tensor_t,^the (first/second) input/output tensor
107,2,200,"['input', 'tensor']",the input tensor.,torch.tan.yaml,2,tensor_t,^the (first/second) input/output tensor
108,2,200,"['input', 'tensor']",the input tensor.,torch.argmin.yaml,2,tensor_t,^the (first/second) input/output tensor
109,2,200,"['input', 'tensor']",the input tensor.,torch.max2.yaml,2,tensor_t,^the (first/second) input/output tensor
110,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2,dtype,
111,2,200,"['input', 'tensor']",the input tensor.,torch.cumprod.yaml,2,tensor_t,^the (first/second) input/output tensor
112,2,200,"['input', 'tensor']",the input tensor.,torch.diag_embed.yaml,2,tensor_t,^the (first/second) input/output tensor
113,2,200,"['input', 'tensor']",the input tensor.,torch.imag.yaml,2,tensor_t,^the (first/second) input/output tensor
114,2,200,"['input', 'tensor']",the input tensor.,torch.atan.yaml,2,tensor_t,^the (first/second) input/output tensor
115,2,200,"['input', 'tensor']",the input tensor.,torch.cosh.yaml,2,tensor_t,^the (first/second) input/output tensor
116,2,200,"['input', 'tensor']",the input tensor.,torch.asin.yaml,2,tensor_t,^the (first/second) input/output tensor
117,2,200,"['input', 'tensor']",the input tensor.,torch.argmax2.yaml,2,tensor_t,^the (first/second) input/output tensor
118,2,200,"['input', 'tensor']",the input tensor,torch.stft.yaml,2,tensor_t,^the (first/second) input/output tensor
119,2,200,"['input', 'tensor']",the input tensor.,torch.square.yaml,2,tensor_t,^the (first/second) input/output tensor
120,2,200,"['input', 'tensor']",the input tensor.,torch.median.yaml,2,tensor_t,^the (first/second) input/output tensor
121,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
122,2,200,"['input', 'tensor']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
123,2,200,"['input', 'tensor']",the input tensor.,torch.log2.yaml,2,tensor_t,^the (first/second) input/output tensor
124,2,200,"['input', 'tensor']",the input tensor.,torch.logical_xor.yaml,2,tensor_t,^the (first/second) input/output tensor
126,2,200,"['input', 'tensor']",the input tensor.,torch.prod.yaml,2,tensor_t,^the (first/second) input/output tensor
127,2,200,"['input', 'tensor']",the input tensor.,torch.argmax.yaml,2,tensor_t,^the (first/second) input/output tensor
128,2,200,"['input', 'tensor']",the input tensor.,torch.floor.yaml,2,tensor_t,^the (first/second) input/output tensor
125,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,not useful,
132,2,200,"['input', 'tensor']",the input tensor,torch.norm.yaml,2,tensor_t,^the (first/second) input/output tensor
133,2,200,"['input', 'tensor']",the input tensor,torch.unique.yaml,2,tensor_t,^the (first/second) input/output tensor
134,2,200,"['input', 'tensor']",the input tensor.,torch.index_select.yaml,2,tensor_t,^the (first/second) input/output tensor
135,2,200,"['input', 'tensor']",the input tensor.,torch.kthvalue.yaml,2,tensor_t,^the (first/second) input/output tensor
136,2,200,"['input', 'tensor']",the first input tensor,torch.atan2.yaml,2,tensor_t,^the (first/second) input/output tensor
131,2,200,"['input', 'tensor']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2,not useful,
137,2,200,"['input', 'tensor']",the second input tensor,torch.atan2.yaml,2,tensor_t,^the (first/second) input/output tensor
138,2,200,"['input', 'tensor']",the input tensor.,torch.flip.yaml,2,tensor_t,^the (first/second) input/output tensor
139,2,200,"['input', 'tensor']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
140,2,200,"['input', 'tensor']",the input tensor.,torch.var2.yaml,2,tensor_t,^the (first/second) input/output tensor
141,2,200,"['input', 'tensor']",the input tensor.,torch.logical_not.yaml,2,tensor_t,^the (first/second) input/output tensor
143,2,200,"['input', 'tensor']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2,ndim,
144,2,200,"['input', 'tensor']",the input tensor.,torch.min.yaml,2,tensor_t,^the (first/second) input/output tensor
145,2,200,"['input', 'tensor']",the input tensor.,torch.expm1.yaml,2,tensor_t,^the (first/second) input/output tensor
146,2,200,"['input', 'tensor']",the input tensor.,torch.mode.yaml,2,tensor_t,^the (first/second) input/output tensor
147,2,200,"['input', 'tensor']",the input tensor.,torch.log.yaml,2,tensor_t,^the (first/second) input/output tensor
3836,170,9,"['true', 'computed']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2,not useful,
148,2,200,"['input', 'tensor']",the input tensor.,torch.abs.yaml,2,tensor_t,^the (first/second) input/output tensor
149,2,200,"['input', 'tensor']",the input tensor.,torch.tril.yaml,2,tensor_t,^the (first/second) input/output tensor
150,2,200,"['input', 'tensor']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2,dtype,
151,2,200,"['input', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,2,dependency(shape),
152,2,200,"['input', 'tensor']",the input tensor.,torch.triu.yaml,2,tensor_t,^the (first/second) input/output tensor
153,2,200,"['input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
154,2,200,"['input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
155,2,200,"['input', 'tensor']",the input tensor,torch.nn.quantized.functional.interpolate.yaml,2,tensor_t,^the (first/second) input/output tensor
157,2,200,"['input', 'tensor']",the input tensor.,torch.unsqueeze.yaml,2,tensor_t,^the (first/second) input/output tensor
158,2,200,"['input', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,"range (-1,inf)",
159,2,200,"['input', 'tensor']",an input tensor or number,torch.result_type.yaml,2,dtype,
160,2,200,"['input', 'tensor']",an input tensor or number,torch.result_type.yaml,2,dtype ,
161,2,200,"['input', 'tensor']",the input tensor.,torch.log10.yaml,2,tensor_t,^the (first/second) input/output tensor
156,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2,not useful,
162,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_or.yaml,2,tensor_t,^the (first/second) input/output tensor
163,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_or.yaml,2,tensor_t,^the (first/second) input/output tensor
164,2,200,"['input', 'tensor']",the input tensor.,torch.std_mean2.yaml,2,tensor_t,^the (first/second) input/output tensor
165,2,200,"['input', 'tensor']",the input tensor.,torch.sqrt.yaml,2,tensor_t,^the (first/second) input/output tensor
166,2,200,"['input', 'tensor']",the input tensor.,torch.lgamma.yaml,2,tensor_t,^the (first/second) input/output tensor
167,2,200,"['input', 'tensor']",the input tensor.,torch.sign.yaml,2,tensor_t,^the (first/second) input/output tensor
168,2,200,"['input', 'tensor']",the input tensor,torch.unique_consecutive.yaml,2,tensor_t,^the (first/second) input/output tensor
169,2,200,"['input', 'tensor']",the input tensor.,torch.rot90.yaml,2,tensor_t,^the (first/second) input/output tensor
170,2,200,"['input', 'tensor']",the input tensor.,torch.dist.yaml,2,tensor_t,^the (first/second) input/output tensor
171,2,200,"['input', 'tensor']",the Right-hand-side input tensor,torch.dist.yaml,2,tensor_t,
172,2,200,"['input', 'tensor']",the input tensor.,torch.squeeze.yaml,2,tensor_t,^the (first/second) input/output tensor
174,2,200,"['input', 'tensor']",the input tensor.,torch.cumsum.yaml,2,tensor_t,^the (first/second) input/output tensor
177,2,200,"['input', 'tensor']",the input tensor.,torch.tanh.yaml,2,tensor_t,^the (first/second) input/output tensor
178,2,200,"['input', 'tensor']",the input tensor.,torch.t.yaml,2,tensor_t,^the (first/second) input/output tensor
180,2,200,"['input', 'tensor']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
181,2,200,"['input', 'tensor']",the input tensor.,torch.transpose.yaml,2,tensor_t,^the (first/second) input/output tensor
173,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2,not useful,
182,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
175,2,200,"['input', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2,not useful,
176,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2,not useful,
183,2,200,"['input', 'tensor']",the input 2-D tensor,torch.matrix_rank.yaml,2,tensor_t,^the (first/second) input/output tensor
184,2,200,"['input', 'tensor']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2,dtype(bool),if `true/false`
179,2,200,"['input', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
186,2,200,"['input', 'tensor']",the input tensor.,torch.roll.yaml,2,tensor_t,^the (first/second) input/output tensor
187,2,200,"['input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
188,2,200,"['input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
189,2,200,"['input', 'tensor']",the input tensor,torch.nn.functional.interpolate.yaml,2,tensor_t,^the (first/second) input/output tensor
190,2,200,"['input', 'tensor']",the input tensor.,torch.std.yaml,2,tensor_t,^the (first/second) input/output tensor
185,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
191,2,200,"['input', 'tensor']",the input tensor.,torch.diagonal.yaml,2,tensor_t,^the (first/second) input/output tensor
192,2,200,"['input', 'tensor']",the input tensor.,torch.diagflat.yaml,2,tensor_t,^the (first/second) input/output tensor
193,2,200,"['input', 'tensor']",the input tensor.,torch.renorm.yaml,2,tensor_t,^the (first/second) input/output tensor
194,2,200,"['input', 'tensor']",the input tensor.,torch.rsqrt.yaml,2,tensor_t,^the (first/second) input/output tensor
195,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2,ndim,
196,2,200,"['input', 'tensor']",the input tensor.,torch.max.yaml,2,tensor_t,^the (first/second) input/output tensor
197,2,200,"['input', 'tensor']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
198,2,200,"['input', 'tensor']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
199,2,200,"['input', 'tensor']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,bool,
84,2,200,"['input', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2,numeric,
36,2,200,"['input', 'tensor']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
41,2,200,"['input', 'tensor']",input tensor of any shape,torch.nn.functional.normalize.yaml,2,tensor_t,
142,2,200,"['input', 'tensor']",the input tensor containing probabilities,torch.multinomial.yaml,2,tensor_t,
200,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,2,dtype(bool),
201,3,163,"['output', 'tensor']",the output tensor.,torch.std2.yaml,2,tensor_t,^the (first/second) input/output tensor
202,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_xor.yaml,2,tensor_t,^the (first/second) input/output tensor
203,3,163,"['output', 'tensor']",the output tensor.,torch.real.yaml,2,tensor_t,^the (first/second) input/output tensor
204,3,163,"['output', 'tensor']",the output tensor.,torch.addcdiv.yaml,2,tensor_t,^the (first/second) input/output tensor
205,3,163,"['output', 'tensor']",the output tensor.,torch.log1p.yaml,2,tensor_t,^the (first/second) input/output tensor
206,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,2,dtype(bool),
207,3,163,"['output', 'tensor']",the output tensor.,torch.bernoulli.yaml,2,tensor_t,^the (first/second) input/output tensor
208,3,163,"['output', 'tensor']",the shape of the output tensor,torch.empty_strided.yaml,2,dtype(torch.Size)+ndim(1),
210,3,163,"['output', 'tensor']",the output tensor.,torch.pow.yaml,2,tensor_t,^the (first/second) input/output tensor
209,3,163,"['output', 'tensor']",the strides of the output tensor,torch.empty_strided.yaml,2,not useful,
211,3,163,"['output', 'tensor']",the output tensor.,torch.exp.yaml,2,tensor_t,^the (first/second) input/output tensor
213,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,2,dtype(bool),
2460,65,20,"['n', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
214,3,163,"['output', 'tensor']",the output tensor.,torch.mean2.yaml,2,tensor_t,^the (first/second) input/output tensor
215,3,163,"['output', 'tensor']",the output tensor.,torch.arange.yaml,2,tensor_t,^the (first/second) input/output tensor
216,3,163,"['output', 'tensor']",the output tensor.,torch.ceil.yaml,2,tensor_t,^the (first/second) input/output tensor
217,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,2,dtype(bool),
218,3,163,"['output', 'tensor']",the output tensor.,torch.min22.yaml,2,tensor_t,^the (first/second) input/output tensor
219,3,163,"['output', 'tensor']",the output tensor.,torch.lerp.yaml,2,tensor_t,^the (first/second) input/output tensor
220,3,163,"['output', 'tensor']",the shape of the output tensor,torch.as_strided.yaml,2,dtype(torch.Size)+ndim(1),
223,3,163,"['output', 'tensor']",the output tensor.,torch.max22.yaml,2,tensor_t,^the (first/second) input/output tensor
4507,249,7,"['true', 'compute']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2,not useful,
222,3,163,"['output', 'tensor']",the stride of the output tensor,torch.as_strided.yaml,2,not useful,
225,3,163,"['output', 'tensor']",the output tensor containing indices,torch.nonzero.yaml,2,tensor_t,
224,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
226,3,163,"['output', 'tensor']",the output tensor.,torch.sinh.yaml,2,tensor_t,^the (first/second) input/output tensor
227,3,163,"['output', 'tensor']",the output tensors,torch.eig.yaml,2,tensor_t,^the (first/second) input/output tensor
229,3,163,"['output', 'tensor']",the output tensor.,torch.erf.yaml,2,tensor_t,^the (first/second) input/output tensor
228,3,163,"['output', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
230,3,163,"['output', 'tensor']",the output tensor.,torch.neg.yaml,2,tensor_t,^the (first/second) input/output tensor
231,3,163,"['output', 'tensor']",the output tensor.,torch.baddbmm.yaml,2,tensor_t,^the (first/second) input/output tensor
233,3,163,"['output', 'tensor']",the output tensor.,torch.nn.functional.normalize.yaml,2,tensor_t,^the (first/second) input/output tensor
234,3,163,"['output', 'tensor']",the output tensor.,torch.diag.yaml,2,tensor_t,^the (first/second) input/output tensor
235,3,163,"['output', 'tensor']",the output tensor.,torch.mv.yaml,2,tensor_t,^the (first/second) input/output tensor
236,3,163,"['output', 'tensor']",the output tensor.,torch.addr.yaml,2,tensor_t,^the (first/second) input/output tensor
237,3,163,"['output', 'tensor']",the output tensor.,torch.sigmoid.yaml,2,tensor_t,^the (first/second) input/output tensor
238,3,163,"['output', 'tensor']",the output tensor.,torch.fmod.yaml,2,tensor_t,^the (first/second) input/output tensor
239,3,163,"['output', 'tensor']",the output tensor.,torch.pow2.yaml,2,tensor_t,^the (first/second) input/output tensor
232,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.le.yaml,2,bool+tensor,
243,3,163,"['output', 'tensor']",the output tensor.,torch.erfinv.yaml,2,tensor_t,^the (first/second) input/output tensor
240,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
244,3,163,"['output', 'tensor']",the output tensor.,torch.addbmm.yaml,2,tensor_t,^the (first/second) input/output tensor
246,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_and.yaml,2,tensor_t,^the (first/second) input/output tensor
241,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.gt.yaml,2,bool+tensor,
248,3,163,"['output', 'tensor']",the output tensor.,torch.reciprocal.yaml,2,tensor_t,^the (first/second) input/output tensor
2481,66,19,"['*', '*']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
250,3,163,"['output', 'tensor']",the output tensor.,torch.cos.yaml,2,tensor_t,^the (first/second) input/output tensor
251,3,163,"['output', 'tensor']",the output tensor.,torch.masked_select.yaml,2,tensor_t,^the (first/second) input/output tensor
252,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,2,dtype(bool),
249,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
254,3,163,"['output', 'tensor']",the output tensor.,torch.angle.yaml,2,tensor_t,^the (first/second) input/output tensor
255,3,163,"['output', 'tensor']",the output tensor.,torch.erfc.yaml,2,tensor_t,^the (first/second) input/output tensor
256,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2,structure(sequence),
5401,390,5,"['true', 'otherwise']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2,not useful,
257,3,163,"['output', 'tensor']",the output tensor.,torch.rand.yaml,2,tensor_t,^the (first/second) input/output tensor
258,3,163,"['output', 'tensor']",the output tensor.,torch.round.yaml,2,tensor_t,^the (first/second) input/output tensor
259,3,163,"['output', 'tensor']",the output tensor.,torch.polygamma.yaml,2,tensor_t,^the (first/second) input/output tensor
260,3,163,"['output', 'tensor']",the output tensor.,torch.cross.yaml,2,tensor_t,^the (first/second) input/output tensor
261,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,2,dtype(bool),
262,3,163,"['output', 'tensor']",the output tensor.,torch.acos.yaml,2,tensor_t,^the (first/second) input/output tensor
263,3,163,"['output', 'tensor']",the output tensor.,torch.eye.yaml,2,tensor_t,^the (first/second) input/output tensor
264,3,163,"['output', 'tensor']",the output tensor.,torch.addmm.yaml,2,tensor_t,^the (first/second) input/output tensor
266,3,163,"['output', 'tensor']",the output tensor.,torch.addcmul.yaml,2,tensor_t,^the (first/second) input/output tensor
267,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_not.yaml,2,tensor_t,^the (first/second) input/output tensor
268,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,2,dtype(bool),
265,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
269,3,163,"['output', 'tensor']",the output tensor.,torch.trunc.yaml,2,tensor_t,^the (first/second) input/output tensor
270,3,163,"['output', 'tensor']",the output tensor.,torch.conj.yaml,2,tensor_t,^the (first/second) input/output tensor
271,3,163,"['output', 'tensor']",the output tensor.,torch.mul.yaml,2,tensor_t,^the (first/second) input/output tensor
272,3,163,"['output', 'tensor']",the output tensor.,torch.remainder.yaml,2,tensor_t,^the (first/second) input/output tensor
273,3,163,"['output', 'tensor']",the output tensor for c,torch.cholesky_solve.yaml,2,tensor_t,^the (first/second) input/output tensor
274,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,2,dtype(bool),
276,3,163,"['output', 'tensor']",the output tensor.,torch.histc.yaml,2,tensor_t,^the (first/second) input/output tensor
242,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.ne.yaml,2,bool+tensor,
278,3,163,"['output', 'tensor']",the output tensor.,torch.clamp.yaml,2,tensor_t,^the (first/second) input/output tensor
2657,76,16,"['*', 'm']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
279,3,163,"['output', 'tensor']",the output tensor.,torch.eq.yaml,2,tensor_t,^the (first/second) input/output tensor
280,3,163,"['output', 'tensor']",the output tensor.,torch.logical_or.yaml,2,tensor_t,^the (first/second) input/output tensor
281,3,163,"['output', 'tensor']",the output tensor.,torch.normal.yaml,2,tensor_t,^the (first/second) input/output tensor
282,3,163,"['output', 'tensor']",the output tensor.,torch.inverse.yaml,2,tensor_t,^the (first/second) input/output tensor
283,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,2,dtype(bool),
284,3,163,"['output', 'tensor']",the output tensor.,torch.logsumexp.yaml,2,tensor_t,^the (first/second) input/output tensor
309,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.ge.yaml,2,bool+tensor,
286,3,163,"['output', 'tensor']",the output tensor.,torch.logical_and.yaml,2,tensor_t,^the (first/second) input/output tensor
287,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2,structure(sequence),
288,3,163,"['output', 'tensor']",the output tensor.,torch.ones.yaml,2,tensor_t,^the (first/second) input/output tensor
289,3,163,"['output', 'tensor']",the output tensor.,torch.randperm.yaml,2,tensor_t,^the (first/second) input/output tensor
290,3,163,"['output', 'tensor']",the output tensor.,torch.sin.yaml,2,tensor_t,^the (first/second) input/output tensor
291,3,163,"['output', 'tensor']",the output tensor.,torch.tan.yaml,2,tensor_t,^the (first/second) input/output tensor
292,3,163,"['output', 'tensor']",the output tensor.,torch.stack.yaml,2,tensor_t,^the (first/second) input/output tensor
293,3,163,"['output', 'tensor']",the output tensor.,torch.normal222.yaml,2,tensor_t,^the (first/second) input/output tensor
294,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2,structure(sequence),
295,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,2,dtype(bool),
297,3,163,"['output', 'tensor']",the output tensor.,torch.cumprod.yaml,2,tensor_t,^the (first/second) input/output tensor
298,3,163,"['output', 'tensor']",the output tensor.,torch.imag.yaml,2,tensor_t,^the (first/second) input/output tensor
299,3,163,"['output', 'tensor']",the output tensor.,torch.matmul.yaml,2,tensor_t,^the (first/second) input/output tensor
3186,112,13,"['*', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
300,3,163,"['output', 'tensor']",the output tensor.,torch.atan.yaml,2,tensor_t,^the (first/second) input/output tensor
301,3,163,"['output', 'tensor']",the output tensor.,torch.cosh.yaml,2,tensor_t,^the (first/second) input/output tensor
302,3,163,"['output', 'tensor']",the output tensor.,torch.asin.yaml,2,tensor_t,^the (first/second) input/output tensor
303,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,2,dtype(bool),
304,3,163,"['output', 'tensor']",the output tensor,torch.normal22.yaml,2,tensor_t,^the (first/second) input/output tensor
305,3,163,"['output', 'tensor']",the output tensor.,torch.square.yaml,2,tensor_t,^the (first/second) input/output tensor
306,3,163,"['output', 'tensor']",the output tensor.,torch.linspace.yaml,2,tensor_t,^the (first/second) input/output tensor
307,3,163,"['output', 'tensor']",the output tensor.,torch.bmm.yaml,2,tensor_t,^the (first/second) input/output tensor
308,3,163,"['output', 'tensor']",the output tensor.,torch.normal2.yaml,2,tensor_t,^the (first/second) input/output tensor
316,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.lt.yaml,2,bool+tensor,
310,3,163,"['output', 'tensor']",the output tensor.,torch.log2.yaml,2,tensor_t,^the (first/second) input/output tensor
311,3,163,"['output', 'tensor']",the output tensor.,torch.logical_xor.yaml,2,tensor_t,^the (first/second) input/output tensor
312,3,163,"['output', 'tensor']",the output tensor.,torch.floor.yaml,2,tensor_t,^the (first/second) input/output tensor
313,3,163,"['output', 'tensor']",the output tensor.,torch.addmv.yaml,2,tensor_t,^the (first/second) input/output tensor
314,3,163,"['output', 'tensor']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,2,dtype(bool),
315,3,163,"['output', 'tensor']",the output tensor.,torch.norm.yaml,2,tensor_t,^the (first/second) input/output tensor
317,3,163,"['output', 'tensor']",the output tensor.,torch.index_select.yaml,2,tensor_t,^the (first/second) input/output tensor
318,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,2,dtype(bool),
320,3,163,"['output', 'tensor']",the output tensor.,torch.atan2.yaml,2,tensor_t,^the (first/second) input/output tensor
321,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,2,dtype(bool),
322,3,163,"['output', 'tensor']",the output tensor.,torch.var2.yaml,2,tensor_t,^the (first/second) input/output tensor
323,3,163,"['output', 'tensor']",the output tensor.,torch.logical_not.yaml,2,tensor_t,^the (first/second) input/output tensor
324,3,163,"['output', 'tensor']",the output tensor.,torch.multinomial.yaml,2,tensor_t,^the (first/second) input/output tensor
325,3,163,"['output', 'tensor']",the output tensor for inv,torch.cholesky_inverse.yaml,2,tensor_t,^the (first/second) input/output tensor
326,3,163,"['output', 'tensor']",the output tensor.,torch.expm1.yaml,2,tensor_t,^the (first/second) input/output tensor
327,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,2,dtype(bool),
329,3,163,"['output', 'tensor']",the output tensor.,torch.log.yaml,2,tensor_t,^the (first/second) input/output tensor
330,3,163,"['output', 'tensor']",the output tensor.,torch.abs.yaml,2,tensor_t,^the (first/second) input/output tensor
331,3,163,"['output', 'tensor']",the output tensor.,torch.tril.yaml,2,tensor_t,^the (first/second) input/output tensor
332,3,163,"['output', 'tensor']",the output tensor.,torch.logspace.yaml,2,tensor_t,^the (first/second) input/output tensor
333,3,163,"['output', 'tensor']",the output tensor.,torch.triu.yaml,2,tensor_t,^the (first/second) input/output tensor
3673,153,10,"['*', 'n', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
334,3,163,"['output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
335,3,163,"['output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
336,3,163,"['output', 'tensor']",the output tensor.,torch.log10.yaml,2,tensor_t,^the (first/second) input/output tensor
337,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_or.yaml,2,tensor_t,^the (first/second) input/output tensor
338,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,2,dtype(bool),
339,3,163,"['output', 'tensor']",the output tensor.,torch.sqrt.yaml,2,tensor_t,^the (first/second) input/output tensor
340,3,163,"['output', 'tensor']",the output tensor.,torch.lgamma.yaml,2,tensor_t,^the (first/second) input/output tensor
341,3,163,"['output', 'tensor']",the output tensor.,torch.sign.yaml,2,tensor_t,^the (first/second) input/output tensor
342,3,163,"['output', 'tensor']",the output tensor.,torch.cat.yaml,2,tensor_t,^the (first/second) input/output tensor
345,3,163,"['output', 'tensor']",the output tensor.,torch.squeeze.yaml,2,tensor_t,^the (first/second) input/output tensor
346,3,163,"['output', 'tensor']",the output tensor.,torch.cumsum.yaml,2,tensor_t,^the (first/second) input/output tensor
348,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2,structure(sequence),
349,3,163,"['output', 'tensor']",the output tensor.,torch.zeros.yaml,2,tensor_t,^the (first/second) input/output tensor
350,3,163,"['output', 'tensor']",the output tensor.,torch.tanh.yaml,2,tensor_t,^the (first/second) input/output tensor
3338,124,12,"['SOME_VALUE', 'SOME_VALUE']","a {3, 4, 5}-dimensional torch.Tensor",torch.nn.init.dirac_.yaml,2,not general,
352,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,,2,structure(sequence),
353,3,163,"['output', 'tensor']",the output tensor.,torch.randn.yaml,2,tensor_t,^the (first/second) input/output tensor
355,3,163,"['output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3851,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","a {3, 4, 5}-dimensional torch.Tensor",torch.nn.init.dirac_.yaml,3,not general,
356,3,163,"['output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
358,3,163,"['output', 'tensor']",the output tensor.,torch.full.yaml,2,tensor_t,^the (first/second) input/output tensor
360,3,163,"['output', 'tensor']",the output tensor.,torch.renorm.yaml,2,tensor_t,^the (first/second) input/output tensor
361,3,163,"['output', 'tensor']",the output tensor.,torch.rsqrt.yaml,2,tensor_t,^the (first/second) input/output tensor
362,3,163,"['output', 'tensor']",the output tensor.,torch.mm.yaml,2,tensor_t,^the (first/second) input/output tensor
359,3,163,"['output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2,structure+dtype,a ... or ..
354,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
319,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,tuple+ndim(1)+shape([2]),
247,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,tuple+ndim(1)+shape([2]),
285,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,tuple+ndim(1)+shape([2]),
275,3,163,"['output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2,structure,tuple of 
296,3,163,"['output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2,structure,tuple of 
212,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
245,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
328,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
343,3,163,"['output', 'tensor']",Output tensor.,torch.distributed.scatter.yaml,2,tensor,
277,3,163,"['output', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2,tuple+ndim(1)+shape([2]),
344,3,163,"['output', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2,tuple+ndim(1)+shape([2]),
357,3,163,"['output', 'tensor']",the number to fill the output tensor with.,torch.full.yaml,2,dtype,the <dtype>
221,3,163,"['output', 'tensor']",the offset in the underlying storage of the output tensor,torch.as_strided.yaml,2,int,the number to
351,3,163,"['output', 'tensor']",the output tuple of tensors,torch.svd.yaml,2,tuple+ndim(1)+tensor,
253,3,163,"['output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2,structure,tuple of 
363,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype,the desired <dtype> of
364,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,2,dtype,the desired <dtype> of
365,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,2,dtype,the desired <dtype> of
366,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,2,dtype,the desired <dtype> of
367,4,112,"['returned', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.empty_strided.yaml,2,dtype(bool),
368,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_strided.yaml,2,dtype(bool),
369,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,2,dtype,the desired <dtype> of
370,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,2,dtype,the desired <dtype> of
378,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,2,not useful,
371,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,2,dtype,the desired <dtype> of
372,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.arange.yaml,2,dtype(bool),
373,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,the desired <dtype> of
374,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,the desired <dtype> of
383,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,2,cannot handle,
375,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,2,dtype,the desired <dtype> of
376,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,2,dtype,the desired <dtype> of
377,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,2,dtype,the desired <dtype> of
379,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones_like.yaml,2,dtype(bool),
380,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,2,dtype,the desired <dtype> of
381,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,2,dtype,the desired <dtype> of
382,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,2,dtype,the desired <dtype> of
384,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_like.yaml,2,dtype(bool),
385,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,2,dtype,the desired <dtype> of
386,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,2,dtype,the desired <dtype> of
394,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,2,cannot handle,
387,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,2,dtype,the desired <dtype> of
388,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hann_window.yaml,2,dtype(bool),
389,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2,dtype,the desired <dtype> of
390,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2,dtype,the desired <dtype> of
391,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,2,dtype,the desired <dtype> of
392,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2,dtype,the desired <dtype> of
393,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,2,dtype,the desired <dtype> of
395,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros_like.yaml,2,dtype(bool),
396,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,2,dtype,the desired <dtype> of
397,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,2,dtype,the desired <dtype> of
398,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,2,dtype,the desired <dtype> of
399,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand.yaml,2,dtype(bool),
407,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,2,cannot handle,
400,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,2,dtype,the desired <dtype> of
401,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,2,dtype,the desired <dtype> of
402,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,2,dtype,the desired <dtype> of
403,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.eye.yaml,2,dtype(bool),
404,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,2,dtype,the desired <dtype> of
405,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,2,dtype,the desired <dtype> of
406,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,2,dtype,the desired <dtype> of
408,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn_like.yaml,2,dtype(bool),
409,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2,dtype,the desired <dtype> of
410,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,2,dtype,the desired <dtype> of
411,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,2,dtype,the desired <dtype> of
412,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,2,dtype,the desired <dtype> of
413,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones.yaml,2,dtype(bool),
414,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,2,dtype,the desired <dtype> of
415,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,2,dtype,the desired <dtype> of
416,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,2,dtype,the desired <dtype> of
417,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randperm.yaml,2,dtype(bool),
418,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,2,dtype,the desired <dtype> of
419,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,2,dtype,the desired <dtype> of
420,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,2,dtype,the desired <dtype> of
421,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,2,dtype,the desired <dtype> of
422,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,2,dtype,the desired <dtype> of
423,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,2,dtype,the desired <dtype> of
424,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,2,dtype,the desired <dtype> of
425,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,2,dtype,the desired <dtype> of
426,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.linspace.yaml,2,dtype(bool),
427,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2,dtype,the desired <dtype> of
428,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,2,dtype,the desired <dtype> of
429,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,2,dtype,the desired <dtype> of
430,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2,dtype,the desired <dtype> of
431,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hamming_window.yaml,2,dtype(bool),
432,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,dtype,the desired <dtype> of
433,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,2,dtype,the desired <dtype> of
434,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,2,dtype,the desired <dtype> of
435,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,2,dtype,the desired <dtype> of
436,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2,dtype,the desired <dtype> of
437,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.blackman_window.yaml,2,dtype(bool),
438,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,2,dtype,the desired <dtype> of
439,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,2,dtype,the desired <dtype> of
440,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,2,dtype,the desired <dtype> of
441,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2,dtype,the desired <dtype> of
442,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2,dtype,the desired <dtype> of
443,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.bartlett_window.yaml,2,dtype(bool),
444,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,2,dtype,the desired <dtype> of
445,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,2,dtype,the desired <dtype> of
446,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,2,dtype,the desired <dtype> of
447,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.logspace.yaml,2,dtype(bool),
448,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2,dtype,the desired <dtype> of
449,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,2,dtype,the desired <dtype> of
450,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,2,dtype,the desired <dtype> of
451,4,112,"['returned', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.tensor.yaml,2,dtype(bool),if set
452,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.tensor.yaml,2,dtype(bool),
453,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,2,dtype,the desired <dtype> of
454,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2,dtype,the desired <dtype> of
455,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.sparse_coo_tensor.yaml,2,dtype(bool),
456,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,2,dtype,the desired <dtype> of
457,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,2,dtype,the desired <dtype> of
458,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,2,dtype,the desired <dtype> of
459,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,2,dtype,the desired <dtype> of
460,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros.yaml,2,dtype(bool),
461,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2,dtype,the desired <dtype> of
469,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,2,cannot handle,
462,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,2,dtype,the desired <dtype> of
463,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,2,dtype,the desired <dtype> of
464,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,2,dtype,the desired <dtype> of
465,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn.yaml,2,dtype(bool),
466,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,2,dtype,the desired <dtype> of
467,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,2,dtype,the desired <dtype> of
468,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,2,dtype,the desired <dtype> of
470,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand_like.yaml,2,dtype(bool),
471,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,2,dtype,the desired <dtype> of
472,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,2,dtype,the desired <dtype> of
473,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,2,dtype,the desired <dtype> of
474,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.full.yaml,2,dtype(bool),
475,5,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,3,dtype,the desired <dtype> of
476,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,3,dtype,the desired <dtype> of
477,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,3,dtype,the desired <dtype> of
478,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,3,dtype,the desired <dtype> of
479,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,3,dtype,the desired <dtype> of
487,5,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,3,cannot handle,
480,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,3,dtype,the desired <dtype> of
481,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,3,dtype,the desired <dtype> of
482,5,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,3,dtype,the desired <dtype> of
491,5,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,3,cannot handle,
483,5,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,3,dtype,the desired <dtype> of
484,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,3,dtype,the desired <dtype> of
485,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,3,dtype,the desired <dtype> of
486,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,3,dtype,the desired <dtype> of
488,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,3,dtype,the desired <dtype> of
489,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,3,dtype,the desired <dtype> of
490,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,3,dtype,the desired <dtype> of
492,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,3,dtype,the desired <dtype> of
500,5,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,3,cannot handle,
493,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,3,dtype,the desired <dtype> of
494,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,3,dtype,the desired <dtype> of
495,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,3,dtype,the desired <dtype> of
496,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,3,dtype,the desired <dtype> of
497,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,3,dtype,the desired <dtype> of
498,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,3,dtype,the desired <dtype> of
499,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,3,dtype,the desired <dtype> of
501,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,3,dtype,the desired <dtype> of
502,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,3,dtype,the desired <dtype> of
510,5,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,3,cannot handle,
503,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,3,dtype,the desired <dtype> of
504,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,3,dtype,the desired <dtype> of
505,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,3,dtype,the desired <dtype> of
506,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,3,dtype,the desired <dtype> of
507,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,3,dtype,the desired <dtype> of
508,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,3,dtype,the desired <dtype> of
509,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,3,dtype,the desired <dtype> of
511,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,3,dtype,the desired <dtype> of
512,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,3,dtype,the desired <dtype> of
513,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,3,dtype,the desired <dtype> of
514,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,3,dtype,the desired <dtype> of
515,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,3,dtype,the desired <dtype> of
516,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,3,dtype,the desired <dtype> of
517,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,3,dtype,the desired <dtype> of
518,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,3,dtype,the desired <dtype> of
519,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,3,dtype,the desired <dtype> of
520,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,3,dtype,the desired <dtype> of
521,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,3,dtype,the desired <dtype> of
522,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,3,dtype,the desired <dtype> of
523,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,3,dtype,the desired <dtype> of
524,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,3,dtype,the desired <dtype> of
525,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,3,dtype,the desired <dtype> of
526,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,3,dtype,the desired <dtype> of
527,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,3,dtype,the desired <dtype> of
528,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,3,dtype,the desired <dtype> of
529,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,3,dtype,the desired <dtype> of
530,5,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,3,dtype,the desired <dtype> of
531,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,3,dtype,the desired <dtype> of
532,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,3,dtype,the desired <dtype> of
533,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,3,dtype,the desired <dtype> of
534,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,3,dtype,the desired <dtype> of
535,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,3,dtype,the desired <dtype> of
536,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,3,dtype,the desired <dtype> of
537,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,3,dtype,the desired <dtype> of
538,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,3,dtype,the desired <dtype> of
539,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,3,dtype,the desired <dtype> of
540,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,3,dtype,the desired <dtype> of
541,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,3,dtype,the desired <dtype> of
542,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,3,dtype,the desired <dtype> of
543,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,3,dtype,the desired <dtype> of
544,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,3,dtype,the desired <dtype> of
545,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,3,dtype,the desired <dtype> of
546,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,3,dtype,the desired <dtype> of
547,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,3,dtype,the desired <dtype> of
548,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,3,dtype,the desired <dtype> of
549,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,3,dtype,the desired <dtype> of
550,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,3,dtype,the desired <dtype> of
551,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,3,dtype,the desired <dtype> of
559,5,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,3,cannot handle,
552,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,3,dtype,the desired <dtype> of
553,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,3,dtype,the desired <dtype> of
554,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,3,dtype,the desired <dtype> of
563,6,86,"['default', 'none']",Default: None.,torch.prod2.yaml,2,not useful,
567,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
565,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
566,6,86,"['default', 'none']",Default: None,torch.nn.functional.avg_pool3d.yaml,2,not useful,
555,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,3,dtype,the desired <dtype> of
568,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
569,6,86,"['default', 'none']",Default: None,torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
570,6,86,"['default', 'none']",Default: None.,torch.sum2.yaml,2,not useful,
571,6,86,"['default', 'none']",Default: None.,torch.sum.yaml,2,not useful,
572,6,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
573,6,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2,not useful,
574,6,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2,not useful,
575,6,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2,not useful,
576,6,86,"['default', 'none']",Default: `None`,torch.nn.functional.conv2d.yaml,2,not useful,
577,6,86,"['default', 'none']",Default: `None`,torch.irfft.yaml,2,not useful,
578,6,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2,not useful,
579,6,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2,not useful,
580,6,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2,not useful,
556,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,3,dtype,the desired <dtype> of
582,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
557,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,3,dtype,the desired <dtype> of
584,6,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
585,6,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2,not useful,
586,6,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2,not useful,
587,6,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2,not useful,
964,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
589,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
590,6,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
591,6,86,"['default', 'none']",Default: None,torch.nn.functional.avg_pool2d.yaml,2,not useful,
558,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,3,dtype,the desired <dtype> of
593,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
594,6,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2,not useful,
595,6,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2,not useful,
596,6,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2,not useful,
597,6,86,"['default', 'none']",Default: None,torch.hub.download_url_to_file.yaml,2,not useful,
598,6,86,"['default', 'none']",Default: `None`,torch.nn.functional.conv1d.yaml,2,not useful,
599,6,86,"['default', 'none']",Default: None.,torch.nn.functional.log_softmax.yaml,2,not useful,
600,6,86,"['default', 'none']",Default: None,torch.nn.functional.conv3d.yaml,2,not useful,
601,6,86,"['default', 'none']",Default: `None`,torch.lu.yaml,2,not useful,
560,5,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,3,dtype,the desired <dtype> of
603,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
561,5,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,3,dtype,the desired <dtype> of
605,6,86,"['default', 'none']",Default: None.,torch.cumprod.yaml,2,not useful,
1411,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,
607,6,86,"['default', 'none']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2,not useful,
608,6,86,"['default', 'none']",Default: `None` (treated as equal to `floor(n_fft / 4)`),torch.stft.yaml,2,not useful,
609,6,86,"['default', 'none']",Default: `None` (treated as equal to `n_fft`),torch.stft.yaml,2,not useful,
610,6,86,"['default', 'none']",Default: `None` (treated as window of all 1 s),torch.stft.yaml,2,not useful,
1446,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
612,6,86,"['default', 'none']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2,not useful,
1481,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
614,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
615,6,86,"['default', 'none']",Default: None.,torch.autograd.grad.yaml,2,not useful,
1516,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
617,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
618,6,86,"['default', 'none']",Default: None.,torch.prod.yaml,2,not useful,
619,6,86,"['default', 'none']",Default: None.,torch.norm.yaml,2,not useful,
620,6,86,"['default', 'none']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
621,6,86,"['default', 'none']",default: `None`,torch.unique.yaml,2,not useful,
1551,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,6,not useful,
623,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1585,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,
625,6,86,"['default', 'none']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2,not useful,
1649,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
627,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
562,5,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,3,dtype,the desired <dtype> of
629,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
630,6,86,"['default', 'none']",Default: None.,torch.nn.functional.softmin.yaml,2,not useful,
583,6,86,"['default', 'none']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,not useful,
632,6,86,"['default', 'none']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2,not useful,
650,7,75,"['default', 'SOME_VALUE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,not useful,
634,6,86,"['default', 'none']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2,not useful,
635,6,86,"['default', 'none']",default: `None`,torch.unique_consecutive.yaml,2,not useful,
636,6,86,"['default', 'none']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2,not useful,
637,6,86,"['default', 'none']",Default: None.,torch.cumsum.yaml,2,not useful,
693,7,75,"['default', 'SOME_VALUE']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,not useful,
639,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
640,6,86,"['default', 'none']",Default: None.,torch.nn.functional.softmax.yaml,2,not useful,
694,7,75,"['default', 'SOME_VALUE']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2,not useful,
642,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
643,6,86,"['default', 'none']",Default: `None`,torch.matrix_rank.yaml,2,not useful,
644,6,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2,not useful,
645,6,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2,not useful,
646,6,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2,not useful,
695,7,75,"['default', 'SOME_VALUE']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,not useful,
648,6,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
649,7,75,"['default', 'SOME_VALUE']",Default: 0.,torch.nn.utils.rnn.pad_sequence.yaml,2,not useful,
696,7,75,"['default', 'SOME_VALUE']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,not useful,
651,7,75,"['default', 'SOME_VALUE']",Default: `0`.,torch.arange.yaml,2,not useful,
652,7,75,"['default', 'SOME_VALUE']",Default: `1`.,torch.arange.yaml,2,not useful,
653,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
654,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
655,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
656,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
657,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
658,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
659,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.avg_pool1d.yaml,2,not useful,
660,7,75,"['default', 'SOME_VALUE']",Default 0 .,torch.nn.functional.ctc_loss.yaml,2,not useful,
661,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv2d.yaml,2,not useful,
662,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv2d.yaml,2,not useful,
663,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv2d.yaml,2,not useful,
664,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv2d.yaml,2,not useful,
665,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.normalize.yaml,2,not useful,
666,7,75,"['default', 'SOME_VALUE']",Default: 2,torch.nn.functional.normalize.yaml,2,not useful,
697,7,75,"['default', 'SOME_VALUE']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2,not useful,
668,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.cosine_similarity.yaml,2,not useful,
669,7,75,"['default', 'SOME_VALUE']",Default `2`.,torch.nn.functional.embedding.yaml,2,not useful,
670,7,75,"['default', 'SOME_VALUE']",Destination rank (default is 0),torch.distributed.gather.yaml,2,not useful,
671,7,75,"['default', 'SOME_VALUE']",(default: device_ids[0]),torch.nn.parallel.data_parallel.yaml,2,not useful,
672,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
673,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
674,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
675,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
676,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
677,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
678,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
679,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
680,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
681,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
682,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.avg_pool2d.yaml,2,not useful,
83,2,200,"['input', 'tensor']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
684,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv1d.yaml,2,not useful,
685,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv1d.yaml,2,not useful,
686,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv1d.yaml,2,not useful,
687,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv1d.yaml,2,not useful,
688,7,75,"['default', 'SOME_VALUE']",Default value equals 30 minutes.,torch.distributed.init_process_group.yaml,2,can't handle,
689,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv3d.yaml,2,not useful,
690,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv3d.yaml,2,not useful,
691,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.functional.conv3d.yaml,2,not useful,
692,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.functional.conv3d.yaml,2,not useful,
712,7,75,"['default', 'SOME_VALUE']",Source rank (default is 0),torch.distributed.scatter.yaml,2,not useful,(sorce|destination) (tenosr)* rank
667,7,75,"['default', 'SOME_VALUE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2,not useful,
728,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,dtype,defaults to the <dtype> of
724,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,2,dtype,the desired <dtype> of
727,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,2,dtype,the desired <dtype> of
698,7,75,"['default', 'SOME_VALUE']",Default: 0 (main diagonal).,torch.diag_embed.yaml,2,not useful,
699,7,75,"['default', 'SOME_VALUE']","Default: if not provided, 0.",torch.triu_indices.yaml,2,not useful,
700,7,75,"['default', 'SOME_VALUE']",Default: `None` (treated as equal to `floor(n_fft / 4)`),torch.stft.yaml,2,not useful,
701,7,75,"['default', 'SOME_VALUE']",Default: `None` (treated as window of all 1 s),torch.stft.yaml,2,not useful,
702,7,75,"['default', 'SOME_VALUE']","Default: if not provided, 0.",torch.tril_indices.yaml,2,not useful,
703,7,75,"['default', 'SOME_VALUE']",Default: `100`.,torch.linspace.yaml,2,not useful,
704,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
705,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
706,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
707,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
708,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
709,7,75,"['default', 'SOME_VALUE']",Default: `0`,torch.nn.functional.pad.yaml,2,not useful,
710,7,75,"['default', 'SOME_VALUE']",Default value equals 30 minutes.,torch.distributed.new_group.yaml,2,can't handle,
711,7,75,"['default', 'SOME_VALUE']",Default: `100`.,torch.logspace.yaml,2,not useful,
730,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,2,dtype,the desired <dtype> of
713,7,75,"['default', 'SOME_VALUE']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
714,7,75,"['default', 'SOME_VALUE']",Default: 0.,torch.diagonal.yaml,2,not useful,
715,7,75,"['default', 'SOME_VALUE']",Default: 1.,torch.diagonal.yaml,2,not useful,
716,7,75,"['default', 'SOME_VALUE']",Default: 0 (main diagonal).,torch.diagonal.yaml,2,not useful,
717,7,75,"['default', 'SOME_VALUE']",Default: 0 (main diagonal).,torch.diagflat.yaml,2,not useful,
718,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
719,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
720,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
721,7,75,"['default', 'SOME_VALUE']",Default: 1,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
722,7,75,"['default', 'SOME_VALUE']",Default: 0,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
723,7,75,"['default', 'SOME_VALUE']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2,not useful,
731,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,2,dtype,the desired <dtype> of
732,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,2,dtype,the desired <dtype> of
726,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
735,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,2,dtype,the desired <dtype> of
736,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,2,dtype,the desired <dtype> of
729,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
739,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,2,dtype,the desired <dtype> of
742,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,2,dtype,the desired <dtype> of
743,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,2,dtype,the desired <dtype> of
746,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,2,dtype,the desired <dtype> of
734,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
765,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,dtype,defaults to the <dtype> of
749,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,2,dtype,the desired <dtype> of
752,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,2,dtype,the desired <dtype> of
738,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
755,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,2,dtype,the desired <dtype> of
758,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,2,dtype,the desired <dtype> of
741,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
761,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,2,dtype,the desired <dtype> of
764,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,2,dtype,the desired <dtype> of
624,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
745,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
767,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,2,dtype,the desired <dtype> of
770,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,2,dtype,the desired <dtype> of
748,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
773,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,2,dtype,the desired <dtype> of
776,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,2,dtype,the desired <dtype> of
751,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
779,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,2,dtype,the desired <dtype> of
998,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
754,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
782,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,2,dtype,the desired <dtype> of
785,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,2,dtype,the desired <dtype> of
757,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
786,8,65,"['device', 'tensor']",the desired device of returned tensor.,torch.full.yaml,2,dtype,the desired <dtype> of
768,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,dtype,defaults to the <dtype> of
760,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
762,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,dtype,defaults to the <dtype> of
725,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,dtype,defaults to the <dtype> of
763,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
740,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,dtype,defaults to the <dtype> of
1431,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,
766,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
787,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,dtype,defaults to the <dtype> of
1466,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
769,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
759,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,dtype,defaults to the <dtype> of
1501,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
772,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
733,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,dtype,defaults to the <dtype> of
1539,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
775,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
756,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,dtype,defaults to the <dtype> of
1570,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,6,not useful,
778,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
771,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,dtype,defaults to the <dtype> of
1607,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,
781,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
744,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,dtype,defaults to the <dtype> of
1666,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,
784,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
737,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,dtype,defaults to the <dtype> of
783,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,dtype,defaults to the <dtype> of
747,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,dtype,defaults to the <dtype> of
788,8,65,"['device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
774,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,dtype,defaults to the <dtype> of
753,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,dtype,defaults to the <dtype> of
750,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,dtype,defaults to the <dtype> of
780,8,65,"['device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,dtype,defaults to the <dtype> of
789,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool3d.yaml,2,dtype,
790,9,61,"['default', 'false']",Default: `False`.,torch.empty_strided.yaml,2,dtype,
791,9,61,"['default', 'false']",Default: `False`.,torch.empty_strided.yaml,2,dtype,
792,9,61,"['default', 'false']",Default: `False`.,torch.arange.yaml,2,dtype,
793,9,61,"['default', 'false']",Default: `False`,torch.cholesky.yaml,2,dtype,
794,9,61,"['default', 'false']",Default: `False`,torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype,
795,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.grid_sample.yaml,2,dtype,
796,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.avg_pool1d.yaml,2,dtype,
797,9,61,"['default', 'false']",Default: `False`.,torch.ones_like.yaml,2,dtype,
798,9,61,"['default', 'false']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2,dtype,
799,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool1d.yaml,2,dtype,
800,9,61,"['default', 'false']",Default: `False`,torch.irfft.yaml,2,dtype,
801,9,61,"['default', 'false']",Default: `False`.,torch.empty_like.yaml,2,dtype,
802,9,61,"['default', 'false']",Default: `False`.,torch.hann_window.yaml,2,dtype,
803,9,61,"['default', 'false']",Default `False`.,torch.nn.functional.embedding.yaml,2,dtype,
804,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.affine_grid.yaml,2,dtype,
805,9,61,"['default', 'false']",Default: `False`.,torch.zeros_like.yaml,2,dtype,
806,9,61,"['default', 'false']",Default: `False`.,torch.rand.yaml,2,dtype,
807,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool2d.yaml,2,dtype,
808,9,61,"['default', 'false']",Default: `False`,torch.fft.yaml,2,dtype,
809,9,61,"['default', 'false']",Default: `False`.,torch.triangular_solve.yaml,2,dtype,
810,9,61,"['default', 'false']",Default: `False`.,torch.triangular_solve.yaml,2,dtype,
811,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.avg_pool2d.yaml,2,dtype,
812,9,61,"['default', 'false']",Default: `False`.,torch.eye.yaml,2,dtype,
813,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout.yaml,2,dtype,
814,9,61,"['default', 'false']",Default: `False`.,torch.randn_like.yaml,2,dtype,
815,9,61,"['default', 'false']",Default: `False`,torch.ifft.yaml,2,dtype,
816,9,61,"['default', 'false']",Default: `False`.,torch.cholesky_solve.yaml,2,dtype,
817,9,61,"['default', 'false']",Default: `False`,torch.lu.yaml,2,dtype,
818,9,61,"['default', 'false']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2,dtype,
819,9,61,"['default', 'false']",Default: `False`.,torch.ones.yaml,2,dtype,
820,9,61,"['default', 'false']",Default: `False`.,torch.randperm.yaml,2,dtype,
821,9,61,"['default', 'false']",Default is False.,torch.hub.load.yaml,2,dtype,
822,9,61,"['default', 'false']",Default: `False`.,torch.max2.yaml,2,dtype,
823,9,61,"['default', 'false']",Default is False.,torch.hub.help.yaml,2,dtype,
824,9,61,"['default', 'false']",Default: False,torch.utils.model_zoo.load_url.yaml,2,dtype,
825,9,61,"['default', 'false']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2,dtype,
826,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout2d.yaml,2,dtype,
827,9,61,"['default', 'false']",Default: `False`.,torch.linspace.yaml,2,dtype,
828,9,61,"['default', 'false']",Default: `False`.,torch.autograd.grad.yaml,2,dtype,
829,9,61,"['default', 'false']",Default: `False`.,torch.hamming_window.yaml,2,dtype,
830,9,61,"['default', 'false']",Default: `False`,torch.norm.yaml,2,dtype,
831,9,61,"['default', 'false']",Default: `False`.,torch.blackman_window.yaml,2,dtype,
832,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout3d.yaml,2,dtype,
833,9,61,"['default', 'false']",Default: `False`.,torch.bartlett_window.yaml,2,dtype,
834,9,61,"['default', 'false']",Default: `False`.,torch.logspace.yaml,2,dtype,
835,9,61,"['default', 'false']",Default: `False`,torch.nn.quantized.functional.interpolate.yaml,2,dtype,
836,9,61,"['default', 'false']",Default: `False`,torch.allclose.yaml,2,dtype,
837,9,61,"['default', 'false']",Default: `False`.,torch.tensor.yaml,2,dtype,
838,9,61,"['default', 'false']",Default: `False`.,torch.tensor.yaml,2,dtype,
839,9,61,"['default', 'false']",Default: `False`.,torch.sparse_coo_tensor.yaml,2,dtype,
840,9,61,"['default', 'false']",Default: `False`.,torch.zeros.yaml,2,dtype,
841,9,61,"['default', 'false']",Default: `False`.,torch.randn.yaml,2,dtype,
842,9,61,"['default', 'false']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,dtype,
843,9,61,"['default', 'false']",Default: `False`,torch.matrix_rank.yaml,2,dtype,
844,9,61,"['default', 'false']",Default is False.,torch.hub.list.yaml,2,dtype,
845,9,61,"['default', 'false']",Default: `False`.,torch.rand_like.yaml,2,dtype,
846,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.interpolate.yaml,2,dtype,
851,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
852,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2,not useful,
847,9,61,"['default', 'false']",Default: False,torch.hub.load_state_dict_from_url.yaml,2,dtype,
854,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
855,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2,not useful,
856,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2,not useful,
857,10,57,"['current', 'device']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2,not useful,
848,9,61,"['default', 'false']",Default: `False`.,torch.full.yaml,2,dtype,
859,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2,not useful,
849,9,61,"['default', 'false']",Default: `False`,torch.rfft.yaml,2,dtype,
861,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
862,10,57,"['current', 'device']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2,not useful,
870,10,57,"['current', 'device']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2,not useful,a <dtype>
864,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
853,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,defaults to the <dtype> of
866,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
887,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
868,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2,not useful,
869,10,57,"['current', 'device']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2,not useful,
889,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
885,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
872,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
867,10,57,"['current', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2,not useful,
874,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
858,10,57,"['current', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2,not useful,
626,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
877,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
875,10,57,"['current', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2,not useful,
879,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
850,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,defaults to the <dtype> of
881,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
999,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
883,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
884,10,57,"['current', 'device']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
865,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,defaults to the <dtype> of
886,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
905,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
888,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
882,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
890,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
1432,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,
892,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
893,10,57,"['current', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2,not useful,
1467,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
895,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
1502,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
897,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
1540,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
899,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
900,10,57,"['current', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2,not useful,
1571,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,6,not useful,
902,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
903,10,57,"['current', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2,not useful,
904,10,57,"['current', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2,not useful,
1608,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,
906,10,57,"['current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
860,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,defaults to the <dtype> of
908,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.prod2.yaml,2,not useful,
880,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
891,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
871,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,defaults to the <dtype> of
863,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,defaults to the <dtype> of
913,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.sum2.yaml,2,not useful,
901,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
915,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.sum.yaml,2,not useful,
873,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,defaults to the <dtype> of
894,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
878,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
876,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
898,10,57,"['current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
907,11,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype,the desired <dtype> of
909,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.empty_strided.yaml,2,dtype,the desired <dtype> of
910,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.arange.yaml,2,dtype,the desired <dtype> of
912,11,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,the desired <dtype> of
925,11,54,"['data', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
914,11,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,the desired <dtype> of
927,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.log_softmax.yaml,2,not useful,
916,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.ones_like.yaml,2,dtype,the desired <dtype> of
917,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.empty_like.yaml,2,dtype,the desired <dtype> of
918,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.hann_window.yaml,2,dtype,the desired <dtype> of
931,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.cumprod.yaml,2,not useful,
919,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2,dtype,the desired <dtype> of
920,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2,dtype,the desired <dtype> of
921,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2,dtype,the desired <dtype> of
922,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.rand.yaml,2,dtype,the desired <dtype> of
923,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.eye.yaml,2,dtype,the desired <dtype> of
924,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.randn_like.yaml,2,dtype,the desired <dtype> of
926,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2,dtype,the desired <dtype> of
939,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.prod.yaml,2,not useful,
928,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.ones.yaml,2,dtype,the desired <dtype> of
929,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.randperm.yaml,2,dtype,the desired <dtype> of
930,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.cumprod.yaml,2,dtype,the desired <dtype> of
943,11,54,"['data', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2,not useful,
932,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.triu_indices.yaml,2,dtype,the desired <dtype> of
933,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.tril_indices.yaml,2,dtype,the desired <dtype> of
934,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.linspace.yaml,2,dtype,the desired <dtype> of
947,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.softmin.yaml,2,not useful,
935,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2,dtype,the desired <dtype> of
949,11,54,"['data', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2,not useful,
936,11,54,"['data', 'type']",quantization data type to use.,torch.nn.quantized.functional.conv2d.yaml,2,dtype (in varname),
951,11,54,"['data', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2,not useful,
937,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.hamming_window.yaml,2,dtype,the desired <dtype> of
953,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.cumsum.yaml,2,not useful,
938,11,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,dtype,the desired <dtype> of
940,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.norm.yaml,2,dtype,the desired <dtype> of
956,11,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.softmax.yaml,2,not useful,
941,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.blackman_window.yaml,2,dtype,the desired <dtype> of
942,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.as_tensor.yaml,2,dtype,the desired <dtype> of
944,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2,dtype,the desired <dtype> of
945,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.logspace.yaml,2,dtype,the desired <dtype> of
1667,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
962,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
963,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2,not useful,
946,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2,dtype,the desired <dtype> of
965,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
966,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2,not useful,
967,12,51,"['none', 'default']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2,not useful,
968,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2,not useful,
969,12,51,"['none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2,not useful,
948,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.tensor.yaml,2,dtype,the desired <dtype> of
971,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2,not useful,
950,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2,dtype,the desired <dtype> of
973,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
974,12,51,"['none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2,not useful,
952,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.cumsum.yaml,2,dtype,the desired <dtype> of
976,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
977,12,51,"['none', 'default']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
954,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.zeros.yaml,2,dtype,the desired <dtype> of
979,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
955,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2,dtype,the desired <dtype> of
981,12,51,"['none', 'default']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2,not useful,
982,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2,not useful,
983,12,51,"['none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2,not useful,
957,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.randn.yaml,2,dtype,the desired <dtype> of
985,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
958,11,54,"['data', 'type']",the desired data type of returned Tensor.,torch.rand_like.yaml,2,dtype,the desired <dtype> of
959,11,54,"['data', 'type']",the desired data type of returned tensor.,torch.full.yaml,2,dtype,the desired <dtype> of
960,11,54,"['data', 'type']",quantization data type to use.,torch.nn.quantized.functional.conv3d.yaml,2,dtype (in varname),
1050,13,45,"['single', 'tuple']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2,tensor_t,q: how to represent single
622,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
991,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1012,13,45,"['single', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
993,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
994,12,51,"['none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
995,12,51,"['none', 'default']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2,not useful,
1013,13,45,"['single', 'tuple']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
997,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
996,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1014,13,45,"['single', 'tuple']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
1000,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
1015,13,45,"['single', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
1002,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1003,12,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2,not useful,
1016,13,45,"['single', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1429,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,
1463,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1007,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1499,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1009,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1538,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1011,12,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1017,13,45,"['single', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1018,13,45,"['single', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1019,13,45,"['single', 'tuple']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1020,13,45,"['single', 'tuple']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1021,13,45,"['single', 'tuple']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1022,13,45,"['single', 'tuple']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1023,13,45,"['single', 'tuple']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1024,13,45,"['single', 'tuple']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1025,13,45,"['single', 'tuple']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1026,13,45,"['single', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1027,13,45,"['single', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1028,13,45,"['single', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1029,13,45,"['single', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
1030,13,45,"['single', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
1031,13,45,"['single', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1032,13,45,"['single', 'tuple']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1033,13,45,"['single', 'tuple']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1034,13,45,"['single', 'tuple']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1035,13,45,"['single', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
1036,13,45,"['single', 'tuple']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1037,13,45,"['single', 'tuple']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1038,13,45,"['single', 'tuple']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1039,13,45,"['single', 'tuple']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1040,13,45,"['single', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1041,13,45,"['single', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1042,13,45,"['single', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1043,13,45,"['single', 'tuple']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1044,13,45,"['single', 'tuple']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1045,13,45,"['single', 'tuple']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1046,13,45,"['single', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1047,13,45,"['single', 'tuple']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1048,13,45,"['single', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1049,13,45,"['single', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
1051,13,45,"['single', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1052,13,45,"['single', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1053,13,45,"['single', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1054,13,45,"['single', 'tuple']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1055,13,45,"['single', 'tuple']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1056,13,45,"['single', 'tuple']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1057,14,43,"['number', 'tuple']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
1058,14,43,"['number', 'tuple']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
1059,14,43,"['number', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,2,structure + ndim +dtype + shape(len),
1060,14,43,"['number', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1061,14,43,"['number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1062,14,43,"['number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1063,14,43,"['number', 'tuple']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1064,14,43,"['number', 'tuple']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1065,14,43,"['number', 'tuple']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1066,14,43,"['number', 'tuple']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,2,structure + ndim +dtype + shape(len),
1067,14,43,"['number', 'tuple']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1068,14,43,"['number', 'tuple']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1069,14,43,"['number', 'tuple']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,2,structure + ndim +dtype + shape(len),
1070,14,43,"['number', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1071,14,43,"['number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1072,14,43,"['number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1073,14,43,"['number', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1074,14,43,"['number', 'tuple']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1075,14,43,"['number', 'tuple']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1076,14,43,"['number', 'tuple']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,2,structure + ndim +dtype + shape(len),
1078,14,43,"['number', 'tuple']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1079,14,43,"['number', 'tuple']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1080,14,43,"['number', 'tuple']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1081,14,43,"['number', 'tuple']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,2,structure + ndim +dtype + shape(len),
1082,14,43,"['number', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1083,14,43,"['number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1084,14,43,"['number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,2,structure + ndim +dtype + shape(len),
1085,14,43,"['number', 'tuple']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1086,14,43,"['number', 'tuple']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1087,14,43,"['number', 'tuple']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2,structure + ndim +dtype + shape(len),
1088,14,43,"['number', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1089,14,43,"['number', 'tuple']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1090,14,43,"['number', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1091,14,43,"['number', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.ones.yaml,2,structure + ndim +dtype + shape(len),
1092,14,43,"['number', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1093,14,43,"['number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1094,14,43,"['number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,2,structure + ndim +dtype + shape(len),
1097,14,43,"['number', 'tuple']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1098,14,43,"['number', 'tuple']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1099,14,43,"['number', 'tuple']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,2,structure + ndim +dtype + shape(len),
1077,14,43,"['number', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.rand.yaml,2,dtype+structure,can be a ..
1095,14,43,"['number', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.zeros.yaml,2,dtype+structure,can be a ..
1096,14,43,"['number', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.randn.yaml,2,dtype+structure,can be a ..
1100,15,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,not useful,the desired <dtype> of
1101,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,2,not useful,the desired <dtype> of
1102,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,2,not useful,the desired <dtype> of
1103,15,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,not useful,the desired <dtype> of
1104,15,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,not useful,the desired <dtype> of
1105,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,2,not useful,the desired <dtype> of
1106,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,2,not useful,the desired <dtype> of
1107,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,2,not useful,the desired <dtype> of
1108,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2,not useful,the desired <dtype> of
1109,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2,not useful,the desired <dtype> of
1110,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2,not useful,the desired <dtype> of
1111,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,2,not useful,the desired <dtype> of
1112,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,2,not useful,the desired <dtype> of
1113,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,2,not useful,the desired <dtype> of
1114,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2,not useful,the desired <dtype> of
1115,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,2,not useful,the desired <dtype> of
1116,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,2,not useful,the desired <dtype> of
1117,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,2,not useful,the desired <dtype> of
1118,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,2,not useful,the desired <dtype> of
1119,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,2,not useful,the desired <dtype> of
1120,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,2,not useful,the desired <dtype> of
1121,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2,not useful,the desired <dtype> of
1122,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,2,not useful,the desired <dtype> of
1123,15,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,not useful,the desired <dtype> of
1124,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,2,not useful,the desired <dtype> of
1125,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,2,not useful,the desired <dtype> of
1127,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,2,not useful,the desired <dtype> of
1128,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2,not useful,the desired <dtype> of
1129,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,2,not useful,the desired <dtype> of
1130,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2,not useful,the desired <dtype> of
1132,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,2,not useful,the desired <dtype> of
1133,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2,not useful,the desired <dtype> of
1126,15,42,"['data', 'tensor']",Initial data for the tensor.,torch.as_tensor.yaml,2,not useful,
1135,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,2,not useful,the desired <dtype> of
1136,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,2,not useful,the desired <dtype> of
1137,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2,not useful,the desired <dtype> of
1138,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,2,not useful,the desired <dtype> of
1131,15,42,"['data', 'tensor']",Initial data for the tensor.,torch.tensor.yaml,2,not useful,
1139,15,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,2,not useful,the desired <dtype> of
1140,15,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,2,not useful,the desired <dtype> of
1134,15,42,"['data', 'tensor']",Initial data for the tensor.,torch.sparse_coo_tensor.yaml,2,not useful,
1144,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1166,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1168,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1164,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1142,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1150,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1180,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
1568,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,6,not useful,
1143,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
1606,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,
1145,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
1665,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1147,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
1162,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1149,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
1146,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1151,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
1160,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1153,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
1170,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1155,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
1152,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1157,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
1148,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1159,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
1178,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1161,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
1154,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1163,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
1172,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1165,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
1158,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1167,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
1156,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1169,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
1176,16,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1171,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
980,12,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2,not useful,
1173,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
1185,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,uses the current device
1175,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
1207,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,uses the current device
1177,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
1526,26,35,"['device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2,not useful,
1179,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
1595,28,33,"['current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,3,not useful,
1181,16,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
1209,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,uses the current device
1205,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,uses the current device
1184,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3,not useful,
1183,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,uses the current device
1186,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3,not useful,
1191,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,uses the current device
1188,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3,not useful,
970,12,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2,not useful,
1190,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3,not useful,
1221,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,uses the current device
1192,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3,not useful,
1203,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,uses the current device
1194,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3,not useful,
1520,26,35,"['device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2,not useful,
1196,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3,not useful,
1589,28,33,"['current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,3,not useful,
1198,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3,not useful,
1187,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,uses the current device
1200,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3,not useful,
1201,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,uses the current device
1202,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3,not useful,
1211,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,uses the current device
1204,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3,not useful,
987,12,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2,not useful,
1206,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3,not useful,
1193,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,uses the current device
1208,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3,not useful,
1392,22,36,"['device', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2,not useful,
1210,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3,not useful,
1532,26,35,"['device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2,not useful,
1212,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3,not useful,
1600,28,33,"['current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,3,not useful,
1214,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3,not useful,
1189,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,uses the current device
1216,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3,not useful,
1219,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,uses the current device
1218,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3,not useful,
564,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1220,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3,not useful,
1195,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,uses the current device
1222,17,40,"['current', 'device', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3,not useful,
1213,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,uses the current device
1199,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,uses the current device
1197,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,uses the current device
1217,17,40,"['current', 'device', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,uses the current device
1223,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1224,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1225,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1226,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1227,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1228,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1229,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1230,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1231,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1232,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1233,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1234,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1235,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1236,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1237,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1238,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1239,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1240,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1241,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1242,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1243,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1244,18,39,"['single', 'number', 'tuple']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1245,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1246,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1247,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1248,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1249,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1250,18,39,"['single', 'number', 'tuple']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1251,18,39,"['single', 'number', 'tuple']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1252,18,39,"['single', 'number', 'tuple']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1253,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1254,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1255,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1256,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1257,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1258,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1259,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1260,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1261,18,39,"['single', 'number', 'tuple']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
1262,19,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,5,dtype,the desired <dtype> of
1263,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,5,dtype,the desired <dtype> of
1264,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,5,dtype,the desired <dtype> of
1265,19,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,5,dtype,the desired <dtype> of
1266,19,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,5,dtype,the desired <dtype> of
1267,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,5,dtype,the desired <dtype> of
1268,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,5,dtype,the desired <dtype> of
1269,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,5,dtype,the desired <dtype> of
1270,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,5,dtype,the desired <dtype> of
1271,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,5,dtype,the desired <dtype> of
1272,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,5,dtype,the desired <dtype> of
1273,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,5,dtype,the desired <dtype> of
1274,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,5,dtype,the desired <dtype> of
1275,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,5,dtype,the desired <dtype> of
1276,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,5,dtype,the desired <dtype> of
1277,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,5,dtype,the desired <dtype> of
1278,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,5,dtype,the desired <dtype> of
1279,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,5,dtype,the desired <dtype> of
1280,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,5,dtype,the desired <dtype> of
1281,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,5,dtype,the desired <dtype> of
1282,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,5,dtype,the desired <dtype> of
1283,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,5,dtype,the desired <dtype> of
1284,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,5,dtype,the desired <dtype> of
1285,19,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,5,dtype,the desired <dtype> of
1286,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,5,dtype,the desired <dtype> of
1287,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,5,dtype,the desired <dtype> of
1288,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,5,dtype,the desired <dtype> of
1289,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,5,dtype,the desired <dtype> of
1290,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,5,dtype,the desired <dtype> of
1291,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,5,dtype,the desired <dtype> of
1292,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,5,dtype,the desired <dtype> of
1293,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,5,dtype,the desired <dtype> of
1294,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,5,dtype,the desired <dtype> of
1295,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,5,dtype,the desired <dtype> of
1296,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,5,dtype,the desired <dtype> of
1297,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,5,dtype,the desired <dtype> of
961,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1298,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,5,dtype,the desired <dtype> of
1299,19,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,5,dtype,the desired <dtype> of
1302,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1324,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1325,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1322,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1409,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,
1311,20,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2,not useful,
1444,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1479,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1304,20,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2,not useful,
1514,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1549,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,6,not useful,
1583,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,
1648,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1315,20,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2,not useful,
1301,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1303,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
1321,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1306,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1323,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1300,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1308,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1326,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
1310,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1328,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1313,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1319,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1309,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1332,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1335,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
1334,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1320,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1336,20,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1337,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
1305,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1339,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
1318,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1327,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1312,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1307,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1333,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1345,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
1314,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1329,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1317,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1349,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
1350,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2013,44,22,"['tuple', 'tensor']","If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor.",torch.lu.yaml,2,not useful,
1316,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1353,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
1354,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
1355,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
1331,20,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1357,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
1358,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
1359,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
1389,22,36,"['device', 'device']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2,dtype,a <dtype>
1361,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
1362,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
1386,22,36,"['device', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2,device,
1364,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
1365,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
1366,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
1379,22,36,"['device', 'device']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2,device,
1368,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
1385,22,36,"['device', 'device']",device for which to return the device capability.,torch.cuda.get_device_capability.yaml,2,dtype,^<dtype>
1370,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
1371,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
1372,21,36,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
1373,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
1374,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2,not useful,
1375,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
1376,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2,not useful,
1377,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2,not useful,
1378,22,36,"['device', 'device']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2,not useful,
592,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1380,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2,not useful,
1381,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
1382,22,36,"['device', 'device']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2,not useful,
1383,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
1384,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
1338,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,not useful,
1340,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,not useful,
1387,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2,not useful,
1388,22,36,"['device', 'device']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2,not useful,
1341,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,not useful,
1390,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
1391,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
1342,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,not useful,
1393,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
1394,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
1395,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
1396,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
1397,22,36,"['device', 'device']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
1398,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
1399,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
1400,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
1401,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
1402,22,36,"['device', 'device']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2,not useful,
1403,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
1404,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
1405,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
1406,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
1407,22,36,"['device', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2,not useful,
1408,22,36,"['device', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
978,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1410,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,
1343,21,36,"['tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2,not useful,
1412,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,
1344,21,36,"['tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2,not useful,
1414,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,
1346,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,not useful,
1416,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,
1417,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,
1418,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,
1452,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1420,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,
1488,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1525,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1557,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,6,not useful,
1594,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,
1425,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,
1655,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1427,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,
1428,23,35,"['default', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3,not useful,
1347,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,not useful,
1430,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,
1363,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,not useful,
1367,21,36,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,not useful,
1433,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,
1352,21,36,"['tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2,tuple+ndim(1)+shape([2]),
1435,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,
1369,21,36,"['tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2,tuple+ndim(1)+shape([2]),
1616,29,32,"['default', 'true']",Default: `True`,torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype,
1617,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.avg_pool1d.yaml,2,dtype,
1439,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,
1618,29,32,"['default', 'true']",Default: `True`.,torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype,
1441,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,
1619,29,32,"['default', 'true']",Default: `True`,torch.irfft.yaml,2,dtype,
1443,23,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,
1620,29,32,"['default', 'true']",Default: `True`.,torch.triangular_solve.yaml,2,dtype,
1445,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1621,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.kl_div.yaml,2,dtype,
1447,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
647,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1449,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1622,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.kl_div.yaml,2,dtype,
1451,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1623,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.avg_pool2d.yaml,2,dtype,
1453,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1010,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1455,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1624,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout.yaml,2,dtype,
1625,29,32,"['default', 'true']",whether or not to display a progress bar to stderr Default: True,torch.hub.download_url_to_file.yaml,2,dtype,
1626,29,32,"['default', 'true']",Default is True.,torch.lobpcg.yaml,2,dtype,
1442,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,
1460,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1477,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1462,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1512,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1464,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1465,24,35,"['default', 'uses']",By default uses the same backend as the global group.,torch.distributed.new_group.yaml,2,not useful,
1548,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1581,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,6,not useful,
1468,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
1615,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,
1470,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1677,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1627,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy.yaml,2,dtype,
1628,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy.yaml,2,dtype,
1474,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1629,29,32,"['default', 'true']",Default: `True`.,torch.nn.utils.rnn.pack_sequence.yaml,2,dtype,
1476,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1630,29,32,"['default', 'true']",Default: `True`,torch.lu.yaml,2,dtype,
1478,24,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1631,29,32,"['default', 'true']",Default is True.,torch.hub.load.yaml,2,dtype,
1480,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,
1632,29,32,"['default', 'true']",Default: True,torch.utils.model_zoo.load_url.yaml,2,dtype,
1482,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,
1633,29,32,"['default', 'true']",Default: `True`,torch.stft.yaml,2,dtype,
1634,29,32,"['default', 'true']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2,dtype,
1485,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1635,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout2d.yaml,2,dtype,
1487,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1636,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.nll_loss.yaml,2,dtype,
1489,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,
1637,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.nll_loss.yaml,2,dtype,
1491,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1638,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout3d.yaml,2,dtype,
616,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1639,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.cross_entropy.yaml,2,dtype,
1640,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.cross_entropy.yaml,2,dtype,
1496,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
992,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1498,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1641,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,dtype,
1500,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,
1642,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,dtype,
1643,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2,dtype,
1503,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,
1426,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,
1505,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1461,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1497,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1536,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1509,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1566,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,6,not useful,
1511,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1604,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,
1513,25,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,
1663,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,
1515,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2,not useful,
1644,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2,dtype,
1517,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2,not useful,
1518,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2,not useful,
1519,26,35,"['device', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2,not useful,
1645,29,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2,dtype,
1521,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2,not useful,
1646,29,32,"['default', 'true']",Default: True,torch.hub.load_state_dict_from_url.yaml,2,dtype,
1523,26,35,"['device', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2,not useful,
1647,29,32,"['default', 'true']",Default: `True`,torch.rfft.yaml,2,dtype,
1657,30,30,"['default', 'device']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2,not useful,a <dtype>
1678,31,30,"['input', 'size']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
1527,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2,not useful,
1528,26,35,"['device', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2,not useful,
1529,26,35,"['device', 'default']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2,not useful,
1681,31,30,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
1684,31,30,"['input', 'size']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
1686,31,30,"['input', 'size']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
1688,31,30,"['input', 'size']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
1689,31,30,"['input', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
581,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1690,31,30,"['input', 'size']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
1537,26,35,"['device', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
1691,31,30,"['input', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
972,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1692,31,30,"['input', 'size']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
1693,31,30,"['input', 'size']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
1542,26,35,"['device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2,not useful,
1694,31,30,"['input', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
1413,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,
1448,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1484,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1547,26,35,"['device', 'default']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2,not useful,
1522,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1553,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,6,not useful,
1550,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,6,not useful,
1591,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,
1552,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,6,not useful,
1652,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,
1554,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,6,not useful,
1695,31,30,"['input', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
1556,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,6,not useful,
1696,31,30,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
1558,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,6,not useful,
1697,31,30,"['input', 'size']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
1560,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,6,not useful,
1698,31,30,"['input', 'size']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
1699,31,30,"['input', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
1700,31,30,"['input', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
1702,31,30,"['input', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
1565,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,6,not useful,
1703,31,30,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
1567,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,6,not useful,
1705,31,30,"['input', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
1569,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,6,not useful,
1707,31,30,"['input', 'size']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
1701,31,30,"['input', 'size']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2,shape dependency,
1572,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,6,not useful,
613,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1574,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,6,not useful,
1706,31,30,"['input', 'size']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2,shape dependency,
1683,31,30,"['input', 'size']",Second input (of size matching x1).,torch.nn.functional.cosine_similarity.yaml,2,shape dependency (x1),
990,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1578,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,6,not useful,
1680,31,30,"['input', 'size']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,shape,
1580,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,6,not useful,
1715,32,28,"['tensor', 'shape']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
1582,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,6,not useful,
1708,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
1584,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,3,not useful,
1424,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,
1586,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,3,not useful,
1587,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,3,not useful,
1588,28,33,"['current', 'device', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,3,not useful,
1459,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1590,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,3,not useful,
1495,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1592,28,33,"['current', 'device', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,3,not useful,
1535,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1564,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,6,not useful,
1603,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,
1596,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,3,not useful,
1597,28,33,"['current', 'device', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,3,not useful,
1662,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,
1709,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
1710,32,28,"['tensor', 'shape']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2,shape,
1711,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
1716,32,28,"['tensor', 'shape']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
1723,32,28,"['tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2,numeric,
1717,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
1605,28,33,"['current', 'device', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3,not useful,
1718,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
1719,32,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.kl_div.yaml,2,tensor,^(a|the)\s*tensor
1720,32,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.kl_div.yaml,2,shape(dependent,
1721,32,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.binary_cross_entropy.yaml,2,tensor,^(a|the)\s*tensor
1610,28,33,"['current', 'device', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,3,not useful,
1722,32,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy.yaml,2,shape(dependent,
628,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1733,32,28,"['tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,numeric,
1724,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
1001,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1725,32,28,"['tensor', 'shape']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2,shape,
1726,32,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
1728,32,28,"['tensor', 'shape']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2,shape,
1729,32,28,"['tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
1731,32,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,tensor,^(a|the)\s*tensor
1732,32,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,shape(dependent,
1734,32,28,"['tensor', 'shape']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2,shape,
1735,32,28,"['tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
1712,32,28,"['tensor', 'shape']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
1713,32,28,"['tensor', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,2,tensor_t,
1727,32,28,"['tensor', 'shape']",the divisor that may be either a number or a Tensor of the same shape as the dividend,torch.remainder.yaml,2,"ndim=0, same shape can't hadnle",
1714,32,28,"['tensor', 'shape']","the divisor, which may be either a number or a tensor of the same shape as the dividend",torch.fmod.yaml,2,"ndim=0, same shape can't hadnle",
1737,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1750,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1751,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1749,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1736,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1740,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1760,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
1748,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1738,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1746,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1752,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1747,33,26,"['tensor', 'type']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
1742,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1761,33,26,"['tensor', 'type']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
1739,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1758,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1743,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1753,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1756,33,26,"['tensor', 'type']",any number of tensors of the same type,torch.broadcast_tensors.yaml,2,not useful,any number of
1745,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1744,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1757,33,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1650,30,30,"['default', 'device']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2,not useful,
1651,30,30,"['default', 'device']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2,not useful,
1759,33,26,"['tensor', 'type']",the floating point tensor type or its name,torch.set_default_tensor_type.yaml,2,not useful,the <dtype>
1653,30,30,"['default', 'device']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2,not useful,
1434,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,
1469,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1656,30,30,"['default', 'device']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2,not useful,
1788,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,uses the current device
1504,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1541,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1573,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,6,not useful,
1609,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,
1668,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,
1801,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,uses the current device
1664,30,30,"['default', 'device']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2,not useful,
1792,35,25,"['default', 'current', 'device']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,3,not useful,a <dtype>
1802,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,uses the current device
1800,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,uses the current device
1787,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,uses the current device
1791,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,uses the current device
1811,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,uses the current device
1798,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,uses the current device
1672,30,30,"['default', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2,not useful,
1789,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,uses the current device
1674,30,30,"['default', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2,not useful,
1675,30,30,"['default', 'device']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2,not useful,
1676,30,30,"['default', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2,not useful,
1797,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,uses the current device
1803,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,uses the current device
1679,31,30,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
1793,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,uses the current device
1790,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,uses the current device
1682,31,30,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
1808,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,uses the current device
1794,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,uses the current device
1685,31,30,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
1804,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,uses the current device
1687,31,30,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
1796,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,uses the current device
1795,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,uses the current device
1806,35,25,"['default', 'current', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,uses the current device
1763,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,uses the current device
1778,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,uses the current device
1779,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,uses the current device
1777,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,uses the current device
1762,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,uses the current device
1769,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,uses the current device
1786,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,uses the current device
1776,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,uses the current device
1766,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,uses the current device
1775,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,uses the current device
1780,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,uses the current device
1771,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,uses the current device
1764,34,25,"['default', 'none', 'device']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,3,not useful,defaults to the <dtype> of 
1704,31,30,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
1765,34,25,"['default', 'none', 'device']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,3,not useful,defaults to the <dtype> of 
1767,34,25,"['default', 'none', 'device']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,3,not useful,defaults to the <dtype> of 
1770,34,25,"['default', 'none', 'device']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,3,not useful,defaults to the <dtype> of 
1785,34,25,"['default', 'none', 'device']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,3,not useful,defaults to the <dtype> of 
1768,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,uses the current device
1784,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,uses the current device
1772,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,uses the current device
1781,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,uses the current device
1774,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,uses the current device
1773,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,uses the current device
1783,34,25,"['default', 'none', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,uses the current device
1812,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,4,dtype,the desired <dtype> of
1813,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,4,dtype,the desired <dtype> of
1814,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,4,dtype,the desired <dtype> of
1815,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,4,dtype,the desired <dtype> of
1816,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,4,dtype,the desired <dtype> of
1817,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,4,dtype,the desired <dtype> of
1818,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,4,dtype,the desired <dtype> of
1819,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,4,dtype,the desired <dtype> of
1820,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,4,dtype,the desired <dtype> of
1821,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,4,dtype,the desired <dtype> of
1822,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,4,dtype,the desired <dtype> of
1823,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,4,dtype,the desired <dtype> of
1824,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,4,dtype,the desired <dtype> of
1825,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,4,dtype,the desired <dtype> of
1826,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,4,dtype,the desired <dtype> of
1827,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,4,dtype,the desired <dtype> of
1828,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,4,dtype,the desired <dtype> of
1829,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,4,dtype,the desired <dtype> of
1830,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,4,dtype,the desired <dtype> of
1831,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,4,dtype,the desired <dtype> of
1832,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,4,dtype,the desired <dtype> of
602,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1833,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,4,dtype,the desired <dtype> of
1834,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,4,dtype,the desired <dtype> of
984,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1741,33,26,"['tensor', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
1835,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,4,dtype,the desired <dtype> of
1836,36,25,"['desired', 'device', 'returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,4,dtype,the desired <dtype> of
1837,37,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,2,shape,
1419,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,
1454,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1838,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
1490,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1530,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1559,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,6,not useful,
1598,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,
1658,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,
1839,37,25,"['shape', '_channels']","filters of shape (in _channels , out _channels/groups , kH , kW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
1840,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
1841,37,25,"['shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2,shape,
1842,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
1843,37,25,"['shape', '_channels']","filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.functional.conv2d.yaml,2,shape,
1844,37,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,2,shape,
1845,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
1846,37,25,"['shape', '_channels']","filters of shape (in _channels , out _channels/groups , kT , kH , kW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
1847,37,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,2,shape,
1848,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
1849,37,25,"['shape', '_channels']","filters of shape (in _channels , out _channels/groups , kW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
1850,37,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,2,shape,
1851,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
1852,37,25,"['shape', '_channels']","filters of shape (out _channels , in _channels/groups , kW)",torch.nn.functional.conv1d.yaml,2,shape,
1853,37,25,"['shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2,shape,
1854,37,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
1855,37,25,"['shape', '_channels']","filters of shape (out _channels , in _channels/groups , kT , kH , kW)",torch.nn.functional.conv3d.yaml,2,shape,
1856,37,25,"['shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2,shape,
1857,37,25,"['shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
588,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1858,37,25,"['shape', '_channels']","quantized filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
1859,37,25,"['shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2,shape,
975,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1860,37,25,"['shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
1861,37,25,"['shape', '_channels']","quantized filters of shape (out _channels , in _channels/groups , kD , kH , kW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
1863,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1415,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,
1450,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1486,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1524,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1555,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,6,not useful,
1593,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,
1875,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1654,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,
1877,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1874,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1862,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1866,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1885,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
5361,382,6,"['represents', 'parameters']",a single vector represents the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2,not general,
1873,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1864,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1872,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1878,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1868,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1865,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1799,35,25,"['default', 'current', 'device']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3,not useful,
641,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1884,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1869,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1008,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1879,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1871,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1870,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1807,35,25,"['default', 'current', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3,not useful,
1440,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,
1809,35,25,"['default', 'current', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3,not useful,
1810,35,25,"['default', 'current', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,3,not useful,
1475,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1883,38,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1887,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1899,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1901,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1898,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1886,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1890,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1909,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
1897,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1888,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1896,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1902,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1891,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1889,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1908,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1894,39,24,"['none', 'type']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2,not useful,
1892,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1903,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1895,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1893,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1907,39,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1911,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,uses the current device
1922,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,uses the current device
1924,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,uses the current device
1921,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,uses the current device
1910,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,uses the current device
1914,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,uses the current device
1932,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,uses the current device
1920,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,uses the current device
1912,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,uses the current device
1919,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,uses the current device
1925,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,uses the current device
1915,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,uses the current device
1913,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,uses the current device
1931,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,uses the current device
1916,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,uses the current device
1926,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,uses the current device
1918,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,uses the current device
1917,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,uses the current device
1930,40,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,uses the current device
1934,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2,not useful,uses the current device
1948,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2,not useful,uses the current device
1949,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2,not useful,uses the current device
1947,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2,not useful,uses the current device
1933,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2,not useful,uses the current device
1937,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2,not useful,uses the current device
1955,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2,not useful,uses the current device
1945,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2,not useful,uses the current device
1935,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2,not useful,uses the current device
1943,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2,not useful,uses the current device
1510,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1546,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1579,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,6,not useful,
1614,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,
1673,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,
1867,38,24,"['default', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
1950,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2,not useful,uses the current device
1939,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2,not useful,uses the current device
1936,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2,not useful,uses the current device
1954,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2,not useful,uses the current device
1940,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,uses the current device
1951,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,uses the current device
1942,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,uses the current device
1941,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,uses the current device
1876,38,24,"['default', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2,not useful,
1953,41,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,uses the current device
1957,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,4,not useful,uses the current device
1971,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,4,not useful,uses the current device
1880,38,24,"['default', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2,not useful,
604,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1882,38,24,"['default', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2,not useful,
1972,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,4,not useful,uses the current device
1970,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,4,not useful,uses the current device
986,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1962,42,23,"['uses', 'current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,4,not useful,
1958,42,23,"['uses', 'current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,4,not useful,
1965,42,23,"['uses', 'current', 'device', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,4,not useful,
1421,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,
1456,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1492,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1531,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1561,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,6,not useful,
1956,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,4,not useful,uses the current device
1599,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,
1659,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2,not useful,
1961,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,4,not useful,uses the current device
1978,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,4,not useful,uses the current device
1969,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,4,not useful,uses the current device
1900,39,24,"['none', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2,not useful,
1959,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,4,not useful,uses the current device
1968,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,4,not useful,uses the current device
1973,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,4,not useful,uses the current device
1904,39,24,"['none', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2,not useful,
1963,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,4,not useful,uses the current device
1906,39,24,"['none', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2,not useful,
1960,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,4,not useful,uses the current device
1977,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,4,not useful,uses the current device
1964,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,4,not useful,uses the current device
1974,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,4,not useful,uses the current device
633,6,86,"['default', 'none']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
777,8,65,"['device', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
896,10,57,"['current', 'device']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1005,12,51,"['none', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1174,16,41,"['current', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1215,17,40,"['current', 'device', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1330,20,37,"['uses', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1437,23,35,"['default', 'none', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1472,24,35,"['default', 'uses']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1507,25,35,"['default', 'see']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1544,26,35,"['device', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1576,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,6,not useful,
1923,40,23,"['default', 'none', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,3,not useful,
1612,28,33,"['current', 'device', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1670,30,30,"['default', 'device']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1754,33,26,"['tensor', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1927,40,23,"['default', 'none', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,3,not useful,
1782,34,25,"['default', 'none', 'device']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1929,40,23,"['default', 'none', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,3,not useful,
1805,35,25,"['default', 'current', 'device']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1881,38,24,"['default', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1905,39,24,"['none', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1928,40,23,"['default', 'none', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1952,41,23,"['none', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2,not useful,
1975,42,23,"['uses', 'current', 'device', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,4,not useful,
2216,53,21,"['default', 'current', 'device', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,4,not useful,
2237,54,21,"['default', 'tensor', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3,not useful,
1967,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,4,not useful,uses the current device
2383,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,10,not useful,
631,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
1966,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,4,not useful,uses the current device
1976,42,23,"['uses', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,4,not useful,uses the current device
1004,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
2050,46,22,"['device', 'current', 'device']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,3,dtype,a <dtype>
2023,45,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
129,2,200,"['input', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
2024,45,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
2025,45,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
1436,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,
1471,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
1506,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
1543,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
1575,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,6,not useful,
1611,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,
1669,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2,not useful,
2026,45,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2027,45,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2028,45,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2029,45,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2030,45,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2031,45,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2033,45,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2034,45,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2035,45,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2042,45,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
2043,45,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
611,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
2044,45,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
1981,43,22,"['tensor', 'current']",Tensor to be broadcast from current process.,torch.distributed.all_gather.yaml,2,tensor,^(a|the)\s*tensor
989,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
1985,43,22,"['tensor', 'current']",List of tensors(on different GPUs) to be broadcast from current process.,torch.distributed.all_gather_multigpu.yaml,2,list,
2022,44,22,"['tuple', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2,structure+dtype,a ... or ..
2002,44,22,"['tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,dtype(callable),
1423,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,
1458,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
1494,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
1534,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
1563,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,6,not useful,
1979,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
1980,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
2003,44,22,"['tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,dtype(callable),
1982,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
1983,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
1984,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2004,44,22,"['tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,dtype(callable),
1986,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
1987,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
1988,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
1989,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
1990,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
1991,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
1992,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
1993,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
1994,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
1995,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
1996,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
1997,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
1998,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
1999,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
2000,43,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
3684,154,10,"['*', 'm', '*']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
2007,44,22,"['tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,dtype(callable),
2018,44,22,"['tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,dtype(callable),
2016,44,22,"['tuple', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,tuple+ndim(1)+shape([2]),
3694,155,10,"['*', 'm', 'm']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
2006,44,22,"['tuple', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,tuple+ndim(1)+shape([2]),
2014,44,22,"['tuple', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,tuple+ndim(1)+shape([2]),
2010,44,22,"['tuple', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2,structure,tuple of 
2905,92,15,"['tensor', 'inputs']","Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2,not useful,
4181,208,9,"['m', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
2015,44,22,"['tuple', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2,structure,tuple of 
4029,191,9,"['false', 'tensor']","If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor.",torch.lu.yaml,2,not useful,
2012,44,22,"['tuple', 'tensor']","If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor.",torch.lu.yaml,2,not useful,
2001,44,22,"['tuple', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
4494,247,7,"['*', 'm', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
2005,44,22,"['tuple', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
5328,377,6,"['model', 'model']","Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2,not useful,
2019,44,22,"['tuple', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
4758,285,7,"['n', 'k']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
2011,44,22,"['tuple', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2,tuple+ndim(1)+shape([2]),
2020,44,22,"['tuple', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2,tuple+ndim(1)+shape([2]),
2021,44,22,"['tuple', 'tensor']",the output tuple of tensors,torch.svd.yaml,2,tuple+ndim(1)+tensor,
2009,44,22,"['tuple', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2,structure,tuple of 
2017,44,22,"['tuple', 'tensor']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2,structure,tuple of 
2200,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,4,not useful,uses the current device
2212,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,4,not useful,uses the current device
2213,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,4,not useful,uses the current device
2211,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,4,not useful,uses the current device
2199,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,4,not useful,uses the current device
2203,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,4,not useful,uses the current device
2219,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,4,not useful,uses the current device
2032,45,22,"['none', 'input']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2,not useful,
2209,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,4,not useful,uses the current device
2201,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,4,not useful,uses the current device
2208,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,4,not useful,uses the current device
2036,45,22,"['none', 'input']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2,not useful,
2214,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,4,not useful,uses the current device
2204,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,4,not useful,uses the current device
1360,21,36,"['tensor', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
2040,45,22,"['none', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2,not useful,
2041,45,22,"['none', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2,not useful,
2202,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,4,not useful,uses the current device
2218,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,4,not useful,uses the current device
2205,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,4,not useful,uses the current device
2045,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3,not useful,
2046,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3,not useful,
2047,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3,not useful,
2048,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3,not useful,
2049,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3,not useful,
2215,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,4,not useful,uses the current device
2051,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3,not useful,
2052,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3,not useful,
2053,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3,not useful,
2054,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3,not useful,
2055,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3,not useful,
2056,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3,not useful,
2057,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3,not useful,
2058,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3,not useful,
2059,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3,not useful,
2060,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3,not useful,
2061,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3,not useful,
2062,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3,not useful,
2063,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3,not useful,
2064,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3,not useful,
2065,46,22,"['device', 'current', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,3,not useful,
2066,46,22,"['device', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3,not useful,
2067,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
2068,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
2069,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
2070,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
2071,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2072,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
2073,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
2074,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
2075,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
2076,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
2077,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
2078,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
2079,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
2080,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
2081,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
2082,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
2083,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
2084,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
2085,47,22,"['current', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2,not useful,
2086,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
2087,47,22,"['current', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2,not useful,
2088,47,22,"['current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
2089,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3,not useful,
2090,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3,not useful,
2091,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3,not useful,
2092,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3,not useful,
2093,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3,not useful,
2094,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3,not useful,
2095,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3,not useful,
2096,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3,not useful,
2097,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3,not useful,
2098,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3,not useful,
2099,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3,not useful,
2100,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3,not useful,
2101,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3,not useful,
2102,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3,not useful,
2103,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3,not useful,
2104,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3,not useful,
2105,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3,not useful,
2106,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3,not useful,
2107,48,22,"['current', 'cuda', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3,not useful,
2108,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3,not useful,
2109,48,22,"['current', 'cuda', 'device']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3,not useful,
2110,48,22,"['current', 'cuda', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3,not useful,
2111,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
2112,49,22,"['cpu', 'tensor']",Works only for CPU tensors.,torch.empty_strided.yaml,2,not useful,
2113,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
2114,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
2115,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
2116,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2117,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
2118,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
2119,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
2120,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
2121,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
2122,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
2123,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
2124,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
2125,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
2126,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
2127,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
2128,49,22,"['cpu', 'tensor']",Works only for CPU tensors.,torch.tensor.yaml,2,not useful,
2129,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
2130,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
2131,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
2132,49,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
2133,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
2134,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
2135,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
2136,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
2137,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2138,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
2139,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
2140,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
2141,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
2142,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
2143,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
2144,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
2145,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
2146,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
2147,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
2148,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
2149,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
2150,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
2151,50,22,"['cuda', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2,not useful,
2152,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
2153,50,22,"['cuda', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2,not useful,
2154,50,22,"['cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
2155,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_strided.yaml,5,not useful,
2156,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.arange.yaml,5,not useful,
2157,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones_like.yaml,5,not useful,
2158,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_like.yaml,5,not useful,
2159,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hann_window.yaml,5,not useful,
2160,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros_like.yaml,5,not useful,
2161,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand.yaml,5,not useful,
2162,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.eye.yaml,5,not useful,
2163,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn_like.yaml,5,not useful,
2164,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones.yaml,5,not useful,
2165,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randperm.yaml,5,not useful,
2166,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.linspace.yaml,5,not useful,
2167,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hamming_window.yaml,5,not useful,
2168,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.blackman_window.yaml,5,not useful,
2169,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.bartlett_window.yaml,5,not useful,
2170,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.logspace.yaml,5,not useful,
2171,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.tensor.yaml,5,not useful,
2172,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.sparse_coo_tensor.yaml,5,not useful,
2173,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros.yaml,5,not useful,
2174,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn.yaml,5,not useful,
2175,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand_like.yaml,5,not useful,
2176,51,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.full.yaml,5,not useful,
2177,52,22,"['process', 'group']",The process group to work on.,torch.distributed.get_backend.yaml,2,not useful,
2178,52,22,"['process', 'group']",The default is the general main process group.,torch.distributed.get_backend.yaml,2,not useful,
1755,33,26,"['tensor', 'type']",any python sequence of tensors of the same type.,torch.cat.yaml,2,not useful,
2180,52,22,"['process', 'group']",The process group to work on,torch.distributed.all_reduce.yaml,2,not useful,
2181,52,22,"['process', 'group']",The process group to work on,torch.distributed.isend.yaml,2,not useful,
2182,52,22,"['process', 'group']",The process group to work on,torch.distributed.barrier.yaml,2,not useful,
2183,52,22,"['process', 'group']",The process group to work on,torch.distributed.all_gather.yaml,2,not useful,
2184,52,22,"['process', 'group']",The process group to work on,torch.distributed.gather.yaml,2,not useful,
4113,200,9,"['python', 'tensor']",any python sequence of tensors of the same type.,torch.cat.yaml,2,not useful,
2186,52,22,"['process', 'group']",Timeout for operations executed against the process group.,torch.distributed.init_process_group.yaml,2,not useful,
2187,52,22,"['process', 'group']",The process group to work on,torch.distributed.recv.yaml,2,not useful,
2188,52,22,"['process', 'group']",The process group to work on,torch.distributed.broadcast_multigpu.yaml,2,not useful,
2189,52,22,"['process', 'group']",The process group to work on,torch.distributed.reduce.yaml,2,not useful,
2190,52,22,"['process', 'group']",The process group to work on,torch.distributed.all_gather_multigpu.yaml,2,not useful,
2191,52,22,"['process', 'group']",The process group to work on,torch.distributed.send.yaml,2,not useful,
2192,52,22,"['process', 'group']",The process group to work on,torch.distributed.get_world_size.yaml,2,not useful,
2193,52,22,"['process', 'group']",Timeout for operations executed against the process group.,torch.distributed.new_group.yaml,2,not useful,
2194,52,22,"['process', 'group']",The process group to work on,torch.distributed.scatter.yaml,2,not useful,
2195,52,22,"['process', 'group']",The process group to work on,torch.distributed.irecv.yaml,2,not useful,
2196,52,22,"['process', 'group']",The process group to work on,torch.distributed.reduce_multigpu.yaml,2,not useful,
2197,52,22,"['process', 'group']",The process group to work on,torch.distributed.broadcast.yaml,2,not useful,
2198,52,22,"['process', 'group']",The process group to work on,torch.distributed.get_rank.yaml,2,not useful,
1602,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,
1661,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2,not useful,
2207,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,4,not useful,uses the current device
2206,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,4,not useful,uses the current device
2217,53,21,"['default', 'current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,4,not useful,uses the current device
2221,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,uses the current device
2233,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3,not useful,uses the current device
2234,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,uses the current device
2232,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,uses the current device
2220,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,uses the current device
2224,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,uses the current device
2210,53,21,"['default', 'current', 'device', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,4,not useful,
2240,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,uses the current device
2231,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,uses the current device
606,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
2222,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,uses the current device
2230,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,uses the current device
988,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
2235,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,uses the current device
2226,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,uses the current device
2223,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,uses the current device
1422,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,
1457,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
1493,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
1533,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
1562,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,6,not useful,
2225,54,21,"['default', 'tensor', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,3,not useful,
1601,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,
1660,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2,not useful,
2239,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,uses the current device
2227,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3,not useful,uses the current device
2236,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3,not useful,uses the current device
2229,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3,not useful,uses the current device
2228,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3,not useful,uses the current device
2238,54,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,uses the current device
2241,55,21,"['input', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,not useful,
2242,55,21,"['input', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,not useful,
2243,55,21,"['input', '*']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,not useful,if `true/false`
2244,55,21,"['input', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,not useful,
2245,55,21,"['input', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,not useful,
638,6,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
2246,55,21,"['input', '*']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,not useful,
2247,55,21,"['input', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,not useful,
2248,55,21,"['input', '*']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,not useful,
2249,55,21,"['input', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,not useful,
2250,55,21,"['input', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,not useful,
2251,55,21,"['input', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,not useful,
2252,55,21,"['input', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,not useful,
2253,55,21,"['input', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,not useful,
2254,55,21,"['input', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,not useful,
2255,55,21,"['input', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,not useful,
2256,55,21,"['input', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,not useful,
2257,55,21,"['input', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,not useful,
2258,55,21,"['input', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,not useful,
2259,55,21,"['input', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,not useful,
2260,55,21,"['input', '*']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
2261,55,21,"['input', '*']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,not useful,
2262,56,21,"['input', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2263,56,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2264,56,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,ndim,
2265,56,21,"['input', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2266,56,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,ndim,
2267,56,21,"['input', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2268,56,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,ndim,
2269,56,21,"['input', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2270,56,21,"['input', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2271,56,21,"['input', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2272,56,21,"['input', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2273,56,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2276,56,21,"['input', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2277,56,21,"['input', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2279,56,21,"['input', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2280,56,21,"['input', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2281,56,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2282,56,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2,ndim,
1946,41,23,"['none', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
4409,236,8,"['sequence', 'tensor']",any python sequence of tensors of the same type.,torch.cat.yaml,2,not useful,
2278,56,21,"['input', 'dimensions']","m-elements tuple, where m/2 <= input dimensions and m is even.",torch.nn.functional.pad.yaml,2,structure+shape,
2304,58,21,"['size', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
1356,21,36,"['tensor', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2,not useful,
2305,58,21,"['size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2306,58,21,"['size', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2307,58,21,"['size', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2308,58,21,"['size', '*']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2,shape,
2309,58,21,"['size', '*']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
2310,58,21,"['size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2311,58,21,"['size', '*']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
4129,202,9,"['returns', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2,not useful,
5240,362,6,"['returns', 'tensor', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,3,not useful,
2312,58,21,"['size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2313,58,21,"['size', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2314,58,21,"['size', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2315,58,21,"['size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2316,58,21,"['size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2317,58,21,"['size', '*']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
2318,58,21,"['size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2319,58,21,"['size', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2320,58,21,"['size', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2321,58,21,"['size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2322,58,21,"['size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2323,58,21,"['size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2324,58,21,"['size', '*']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
2283,57,21,"['tensor', 'size']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2284,57,21,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2285,57,21,"['tensor', 'size']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2288,57,21,"['tensor', 'size']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
2289,57,21,"['tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2290,57,21,"['tensor', 'size']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
2291,57,21,"['tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2292,57,21,"['tensor', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2293,57,21,"['tensor', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2294,57,21,"['tensor', 'size']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
2295,57,21,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2296,57,21,"['tensor', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2297,57,21,"['tensor', 'size']","If given, has to be a Tensor of size C",torch.nn.functional.nll_loss.yaml,2,shape,
2298,57,21,"['tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
2299,57,21,"['tensor', 'size']","If given, has to be a Tensor of size C",torch.nn.functional.cross_entropy.yaml,2,shape,
2300,57,21,"['tensor', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2301,57,21,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2302,57,21,"['tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
2303,57,21,"['tensor', 'size']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
2286,57,21,"['tensor', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2,ndim+dtype,
2287,57,21,"['tensor', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2,dtype+ndim,
2368,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,10,not useful,uses the current device
2379,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,10,not useful,uses the current device
2380,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,10,not useful,uses the current device
2325,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,4,not useful,
2326,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,4,not useful,
2327,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,4,not useful,
2328,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,4,not useful,
2329,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,4,not useful,
2330,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,4,not useful,
2331,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,4,not useful,
2332,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,4,not useful,
2333,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,4,not useful,
2334,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,4,not useful,
2335,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,4,not useful,
2336,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,4,not useful,
2337,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,4,not useful,
2338,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,4,not useful,
2339,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,4,not useful,
2340,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,4,not useful,
2341,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,4,not useful,
2342,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,4,not useful,
2343,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,4,not useful,
2344,59,21,"['device', 'cpu', 'current', 'device']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,4,not useful,
2345,59,21,"['device', 'cpu', 'current', 'device']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,4,not useful,
2346,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2,not useful,
2347,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2,not useful,
2348,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2,not useful,
2349,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2,not useful,
2350,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2,not useful,
2351,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2,not useful,
2352,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2,not useful,
2353,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2,not useful,
5704,450,5,"['func', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2,not useful,
2355,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2,not useful,
2356,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2,not useful,
2357,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2,not useful,
2358,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2,not useful,
2359,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2,not useful,
2360,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2,not useful,
2361,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2,not useful,
2362,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2,not useful,
2363,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2,not useful,
2364,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2,not useful,
2365,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2,not useful,
2366,60,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2,not useful,
2378,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,10,not useful,uses the current device
1006,12,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
2367,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,10,not useful,uses the current device
2371,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,10,not useful,uses the current device
2386,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,10,not useful,uses the current device
1438,23,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,
1473,24,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1508,25,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1545,26,35,"['device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
1577,27,34,"['default', 'none', 'uses', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,6,not useful,
1613,28,33,"['current', 'device', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,
1671,30,30,"['default', 'device']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2,not useful,
2377,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,10,not useful,uses the current device
2369,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,10,not useful,uses the current device
2376,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,10,not useful,uses the current device
2381,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,10,not useful,uses the current device
2372,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,10,not useful,uses the current device
2370,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,10,not useful,uses the current device
2385,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,10,not useful,uses the current device
2373,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,10,not useful,uses the current device
2382,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,10,not useful,uses the current device
2375,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,10,not useful,uses the current device
2389,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2,can't handle,
2374,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,10,not useful,uses the current device
2384,61,20,"['default', 'none', 'uses', 'current', 'device', 'default', 'tensor', 'type', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,10,not useful,uses the current device
2392,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2,can't handle,
2407,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,4,dtype,the desired <dtype> of
2394,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2,can't handle,
2408,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,4,dtype,the desired <dtype> of
2396,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2,can't handle,
2409,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,4,dtype,the desired <dtype> of
2410,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,4,dtype,the desired <dtype> of
2399,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2,can't handle,
2411,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,4,dtype,the desired <dtype> of
2401,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2,can't handle,
2412,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,4,dtype,the desired <dtype> of
2403,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2,can't handle,
2413,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,4,dtype,the desired <dtype> of
2405,62,20,"['input', '_channels']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2,can't handle,
2414,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,4,dtype,the desired <dtype> of
2415,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,4,dtype,the desired <dtype> of
2416,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,4,dtype,the desired <dtype> of
2417,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,4,dtype,the desired <dtype> of
2418,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,4,dtype,the desired <dtype> of
2419,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,4,dtype,the desired <dtype> of
2420,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,4,dtype,the desired <dtype> of
2421,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,4,dtype,the desired <dtype> of
2422,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,4,dtype,the desired <dtype> of
2423,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,4,dtype,the desired <dtype> of
2424,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,4,dtype,the desired <dtype> of
2425,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,4,dtype,the desired <dtype> of
2426,63,20,"['desired', 'layout', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,4,dtype,the desired <dtype> of
2387,62,20,"['input', '_channels']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,2,shape,
2388,62,20,"['input', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,shape,
2390,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
2391,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
2393,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
2395,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
2397,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
2398,62,20,"['input', '_channels']","input tensor (minibatch , in _channels , iH , iW)",torch.nn.functional.avg_pool2d.yaml,2,shape,
2427,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,11,not useful,
2428,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,11,not useful,
2429,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,11,not useful,
2430,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,11,not useful,
2431,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,11,not useful,
2432,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,11,not useful,
2433,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,11,not useful,
2434,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,11,not useful,
2435,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,11,not useful,
2436,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,11,not useful,
2437,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,11,not useful,
2438,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,11,not useful,
2439,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,11,not useful,
2440,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,11,not useful,
2441,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,11,not useful,
2442,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,11,not useful,
2443,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,11,not useful,
2444,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,11,not useful,
2445,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,11,not useful,
2446,64,20,"['device', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'device', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,11,not useful,
2400,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
2402,62,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
2404,62,20,"['input', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
2406,62,20,"['input', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
2461,65,20,"['n', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*3,
2447,65,20,"['n', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2448,65,20,"['n', 'n']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
2449,65,20,"['n', 'n']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
2463,65,20,"['n', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*3,
2451,65,20,"['n', 'n']",the square matrix of shape (n times n) for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2,shape,
2452,65,20,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2453,65,20,"['n', 'n']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2,shape,
2454,65,20,"['n', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
5399,389,5,"['*', 'm', 'k']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
2455,65,20,"['n', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
2456,65,20,"['n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2457,65,20,"['n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2458,65,20,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2459,65,20,"['n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2465,65,20,"['n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
3625,149,11,"['set', 'true']","At the moment, ONNX is oriented towards exporting models for inference only, so you will generally not need to set this to True.",torch.onnx.export.yaml,2,not useful,
2466,65,20,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2462,65,20,"['n', 'n']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*2,
2464,65,20,"['n', 'n']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*2,
2450,65,20,"['n', 'n']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,shape,
2468,66,19,"['*', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2469,66,19,"['*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2470,66,19,"['*', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2471,66,19,"['*', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2472,66,19,"['*', '*']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2,shape,
2473,66,19,"['*', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2474,66,19,"['*', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2475,66,19,"['*', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2476,66,19,"['*', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
5403,390,5,"['true', 'otherwise']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
2477,66,19,"['*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2478,66,19,"['*', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2484,66,19,"['*', '*']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
2479,66,19,"['*', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2480,66,19,"['*', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2482,66,19,"['*', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2483,66,19,"['*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2485,66,19,"['*', '*']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,dtype(bool),
2486,67,19,"['input', 'size', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
2487,67,19,"['input', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
2488,67,19,"['input', 'size', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
2489,67,19,"['input', 'size', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,3,shape,
2490,67,19,"['input', 'size', '*']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,shape,
2491,67,19,"['input', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2492,67,19,"['input', 'size', '*']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
2493,67,19,"['input', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2494,67,19,"['input', 'size', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,3,shape,
2495,67,19,"['input', 'size', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3,shape,
2496,67,19,"['input', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
2497,67,19,"['input', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
2498,67,19,"['input', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
2499,67,19,"['input', 'size', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,3,shape,
2500,67,19,"['input', 'size', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,3,shape,
2501,67,19,"['input', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
2502,67,19,"['input', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
2503,67,19,"['input', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
2504,67,19,"['input', 'size', '*']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
2505,68,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool3d.yaml,2,dtype(bool),
2506,68,19,"['whether', 'return']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2,dtype(bool),
2507,68,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool1d.yaml,2,dtype(bool),
2508,68,19,"['whether', 'return']",controls whether to return normalized results.,torch.irfft.yaml,2,dtype(bool),
2509,68,19,"['whether', 'return']",controls whether to return largest or smallest elements,torch.topk.yaml,2,dtype(bool),
2510,68,19,"['whether', 'return']",controls whether to return the elements in sorted order,torch.topk.yaml,2,dtype(bool),
2511,68,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool2d.yaml,2,dtype(bool),
2512,68,19,"['whether', 'return']",controls whether to return normalized results.,torch.fft.yaml,2,dtype(bool),
2513,68,19,"['whether', 'return']",controls whether to return normalized results.,torch.ifft.yaml,2,dtype(bool),
2514,68,19,"['whether', 'return']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2,dtype,
2515,68,19,"['whether', 'return']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2,dtype(bool),
2516,68,19,"['whether', 'return']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2,dtype(bool),
2517,68,19,"['whether', 'return']",Whether to also return the counts for each unique element.,torch.unique.yaml,2,dtype(bool),
2518,68,19,"['whether', 'return']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2,dtype(bool),
2519,68,19,"['whether', 'return']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2,dtype(bool),
2520,68,19,"['whether', 'return']",Whether to also return the counts for each unique element.,torch.unique_consecutive.yaml,2,dtype(bool),
2521,68,19,"['whether', 'return']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2,dtype(bool),
2522,68,19,"['whether', 'return']",controls whether to return normalized results.,torch.rfft.yaml,2,dtype(bool),
2523,68,19,"['whether', 'return']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,2,dtype(bool),
2524,69,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
2525,69,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
2530,69,18,"['default', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,2,not useful,
2526,69,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2,not useful,defaults to the <dtype> of 
2527,69,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2528,69,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2529,69,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2,not useful,defaults to the <dtype> of 
2531,69,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2532,69,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2537,69,18,"['default', 'input']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
2533,69,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2,not useful,defaults to the <dtype> of 
2534,69,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2535,69,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2536,69,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2,not useful,defaults to the <dtype> of 
2538,69,18,"['default', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2,not useful,
2539,69,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
2540,69,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
2541,69,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2,not useful,defaults to the <dtype> of 
2577,72,17,"['input', 'm']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
2578,72,17,"['input', 'm']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
2579,72,17,"['input', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2580,72,17,"['input', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2581,72,17,"['input', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
2582,72,17,"['input', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2583,72,17,"['input', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
2584,72,17,"['input', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2585,72,17,"['input', 'm']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2586,72,17,"['input', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2587,72,17,"['input', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
4907,307,6,"['true', 'tensor']","If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor.",torch.lu.yaml,2,not useful,
2588,72,17,"['input', 'm']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2590,72,17,"['input', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2591,72,17,"['input', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2592,72,17,"['input', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
2576,72,17,"['input', 'm']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
2589,72,17,"['input', 'm']","m-elements tuple, where m/2 <= input dimensions and m is even.",torch.nn.functional.pad.yaml,2,structure+shape,
2565,71,17,"['input', 'shape']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
2566,71,17,"['input', 'shape']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
2570,71,17,"['input', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2,numeric,
2574,71,17,"['input', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,numeric,
2559,71,17,"['input', 'shape']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
2560,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
5842,478,5,"['original', 'module']",correspondence between original module types and quantized counterparts,torch.quantization.quantize.yaml,2,not useful,
2561,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
2562,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
2567,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
1141,15,42,"['data', 'tensor']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2,not useful,
2568,71,17,"['input', 'shape']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
2569,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
2571,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
2572,71,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
2573,71,17,"['input', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
2575,71,17,"['input', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
2563,71,17,"['input', 'shape']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
2564,71,17,"['input', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,2,tensor_t,
2542,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2,dtype(bool),if `true/false`
2543,70,17,"['true', 'input']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
2544,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2,dtype(bool),if `true/false`
2545,70,17,"['true', 'input']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype(bool),if `true/false`
2546,70,17,"['true', 'input']","if `True`, the input is expected to contain sequences sorted by length in a decreasing order.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype(bool),if `true/false`
2547,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2,dtype(bool),if `true/false`
1182,16,41,"['current', 'tensor']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2,not useful,
2548,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2,dtype(bool),if `true/false`
2549,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),if `true/false`
2550,70,17,"['true', 'input']","if `True`, checks that the input contains sequences sorted by length in a decreasing order.",torch.nn.utils.rnn.pack_sequence.yaml,2,dtype(bool),if `true/false`
2551,70,17,"['true', 'input']","if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.",torch.autograd.gradcheck.yaml,2,bool,
2552,70,17,"['true', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
2553,70,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2,dtype(bool),if `true/false`
2554,70,17,"['true', 'input']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,dtype(bool),
2555,70,17,"['true', 'input']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2,dtype(bool),if `true/false`
2556,70,17,"['true', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
2558,70,17,"['true', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,bool,
2593,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,2,dtype(bool),
2594,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,2,dtype(bool),
2595,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,2,dtype(bool),
2596,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,2,dtype(bool),
2597,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,2,dtype(bool),
2598,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,2,dtype(bool),
2599,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,2,dtype(bool),
2600,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,2,dtype(bool),
2601,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,2,dtype(bool),
2602,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,2,dtype(bool),
2610,74,17,"['process', 'group', 'work']",The process group to work on.,torch.distributed.get_backend.yaml,3,not useful,
2611,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_reduce.yaml,3,not useful,
2612,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.isend.yaml,3,not useful,
2613,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.barrier.yaml,3,not useful,
2614,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_gather.yaml,3,not useful,
2615,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.gather.yaml,3,not useful,
2616,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.recv.yaml,3,not useful,
2617,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.broadcast_multigpu.yaml,3,not useful,
2618,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.reduce.yaml,3,not useful,
2619,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_gather_multigpu.yaml,3,not useful,
2620,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.send.yaml,3,not useful,
2621,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.get_world_size.yaml,3,not useful,
2622,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.scatter.yaml,3,not useful,
2623,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.irecv.yaml,3,not useful,
2624,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.reduce_multigpu.yaml,3,not useful,
2625,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.broadcast.yaml,3,not useful,
2626,74,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.get_rank.yaml,3,not useful,
2627,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,2,deprecated,
2628,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,2,deprecated,
2629,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,2,deprecated,
2630,75,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
2631,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,2,deprecated,
2632,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,2,deprecated,
2633,75,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.nll_loss.yaml,2,not useful,
2634,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,2,deprecated,
2635,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,2,deprecated,
2636,75,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
2637,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,2,deprecated,
2638,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,deprecated,
2639,75,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
2640,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,deprecated,
2641,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,2,deprecated,
2642,75,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
2643,75,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,2,deprecated,
2603,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,2,dtype(bool),
2604,73,17,"['whether', 'output']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,2,dtype(bool),
2605,73,17,"['whether', 'output']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2,dtype(bool),
2606,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,2,dtype(bool),
2607,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,2,dtype(bool),
2608,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,2,dtype(bool),
2609,73,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,2,dtype(bool),
2644,76,16,"['*', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2645,76,16,"['*', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2646,76,16,"['*', 'm']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2,shape,
2647,76,16,"['*', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
2648,76,16,"['*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2649,76,16,"['*', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
5725,455,5,"['n', 'n', 'k']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,3,can't handle,
2650,76,16,"['*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
2651,76,16,"['*', 'm']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2660,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.std2.yaml,2,not useful,
2652,76,16,"['*', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2662,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.mean2.yaml,2,not useful,
2663,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.sum2.yaml,2,not useful,
2653,76,16,"['*', 'm']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
2654,76,16,"['*', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2655,76,16,"['*', 'm']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2656,76,16,"['*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2668,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,2,not useful,
2658,76,16,"['*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2670,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,2,not useful,
2659,76,16,"['*', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
2676,78,16,"['dimension', 'along']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz.yaml,2,"dtype(int)+range(-1,inf)+ndim",
2673,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.var2.yaml,2,not useful,
2677,78,16,"['dimension', 'along']",A dimension along which softmax will be computed.,torch.nn.functional.gumbel_softmax.yaml,2,"dtype(int)+range(-1,inf)",
2675,77,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,2,not useful,
2678,78,16,"['dimension', 'along']",the dimension to sort along,torch.topk.yaml,2,"dtype(int)+range(-1,inf)",
2680,78,16,"['dimension', 'along']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz2.yaml,2,"dtype(int)+range(-1,inf)+ndim",
2681,78,16,"['dimension', 'along']",A dimension along which log_softmax will be computed.,torch.nn.functional.log_softmax.yaml,2,"dtype(int)+range(-1,inf)",
5499,409,5,"['tensor', 'used']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2,not useful,
2682,78,16,"['dimension', 'along']",A dimension along which to chunk the tensor.,torch.cuda.comm.scatter.yaml,2,"dtype(int)+range(-1,inf)",
2683,78,16,"['dimension', 'along']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2,"dtype(int)+range(-1,inf)+ndim",
2684,78,16,"['dimension', 'along']",the dimension to sort along,torch.argsort.yaml,2,"dtype(int)+range(-1,inf)",
2685,78,16,"['dimension', 'along']",the dimension to sort along,torch.sort.yaml,2,"dtype(int)+range(-1,inf)",
2686,78,16,"['dimension', 'along']",the dimension along which to narrow,torch.narrow.yaml,2,"dtype(int)+range(-1,inf)+ndim",
2687,78,16,"['dimension', 'along']",the dimension to find the kth value along,torch.kthvalue.yaml,2,"dtype(int)+range(-1,inf)",
2688,78,16,"['dimension', 'along']",A dimension along which softmin will be computed (so every slice along dim will sum to 1).,torch.nn.functional.softmin.yaml,2,"dtype(int)+range(-1,inf)",
2690,78,16,"['dimension', 'along']",A dimension along which softmax will be computed.,torch.nn.functional.softmax.yaml,2,"dtype(int)+range(-1,inf)",
2691,78,16,"['dimension', 'along']",a dimension along which the tensors will be concatenated.,torch.cuda.comm.gather.yaml,2,"dtype(int)+range(-1,inf)",
5504,410,5,"['tensor', 'data']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2,not useful,
2689,78,16,"['dimension', 'along']",dimension along which to split the tensor,torch.chunk.yaml,2,"int+range[-1,inf)",
2679,78,16,"['dimension', 'along']",dimension along which to split the tensor.,torch.split.yaml,2,"int+range[-1,inf)",
2665,77,16,"['dimension', 'reduce']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,2,"list+dtype+ndim(0,1)",
2661,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.prod2.yaml,2,"dtype(int)+range(-1,inf)",
2664,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.nn.functional.normalize.yaml,2,"dtype(int)+range(-1,inf)",
2666,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.min2.yaml,2,"dtype(int)+range(-1,inf)",
2667,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.argmin2.yaml,2,"dtype(int)+range(-1,inf)",
2669,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.median2.yaml,2,"dtype(int)+range(-1,inf)",
2671,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.max2.yaml,2,"dtype(int)+range(-1,inf)",
2672,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.argmax2.yaml,2,"dtype(int)+range(-1,inf)",
2674,77,16,"['dimension', 'reduce']",the dimension to reduce.,torch.mode.yaml,2,"dtype(int)+range(-1,inf)",
2708,80,16,"['input', 'batch']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,not useful,
2710,80,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,not useful,
2711,80,16,"['input', 'batch']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,not useful,
2712,80,16,"['input', 'batch']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,not useful,
2713,80,16,"['input', 'batch']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,not useful,
2714,80,16,"['input', 'batch']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,not useful,
2715,80,16,"['input', 'batch']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,not useful,
2716,80,16,"['input', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,not useful,
2717,80,16,"['input', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,not useful,
2718,80,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,not useful,
2719,80,16,"['input', 'batch']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,not useful,
2720,80,16,"['input', 'batch']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,not useful,
2721,80,16,"['input', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,not useful,
2722,80,16,"['input', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,not useful,
2723,80,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,not useful,
2709,80,16,"['input', 'batch']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,not useful,
2692,79,16,"['input', 'tensor', 'size']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
2693,79,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
2694,79,16,"['input', 'tensor', 'size']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
2695,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,shape,
2696,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2697,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
2698,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2699,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
2700,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
2701,79,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
2702,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
2703,79,16,"['input', 'tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,dtype(bool),
2704,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
2705,79,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
2706,79,16,"['input', 'tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,dtype(bool),
2707,79,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
2789,85,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2,dtype+range(0-1),
2790,85,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2,dtype+range(non-negative),
2792,85,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2,dtype+range(0-1),
2793,85,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2,dtype+range(non-negative),
2795,85,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2,dtype+range(0-1),
2796,85,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2,dtype+range(non-negative),
2798,85,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,dtype+range(0-1),
2799,85,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,dtype+range(non-negative),
2802,85,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2,dtype+range(0-1),
2803,85,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2,dtype+range(non-negative),
2772,84,16,"['size', 'batch']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2773,84,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2775,84,16,"['size', 'batch']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2776,84,16,"['size', 'batch']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
2777,84,16,"['size', 'batch']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2,shape,
2778,84,16,"['size', 'batch']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
2039,45,22,"['none', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
5331,377,6,"['model', 'model']",describes what to run in the forward pass of the model or part of the model.,torch.utils.checkpoint.checkpoint.yaml,2,not useful,
2779,84,16,"['size', 'batch']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2274,56,21,"['input', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
2780,84,16,"['size', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2781,84,16,"['size', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2782,84,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2783,84,16,"['size', 'batch']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2784,84,16,"['size', 'batch']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
2785,84,16,"['size', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2786,84,16,"['size', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2787,84,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2724,81,16,"['tensor', '_channels']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,2,shape,
2725,81,16,"['tensor', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,shape,
2726,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
2727,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,2,shape,
2728,81,16,"['tensor', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2,shape,
2729,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,2,shape,
2730,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
2731,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,2,shape,
2732,81,16,"['tensor', '_channels']","input tensor (minibatch , in _channels , iH , iW)",torch.nn.functional.avg_pool2d.yaml,2,shape,
2733,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,2,shape,
2734,81,16,"['tensor', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2,shape,
2735,81,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,2,shape,
2736,81,16,"['tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2,shape,
2774,84,16,"['size', 'batch']",the size of the original signal (without batch dimension).,torch.irfft.yaml,2,not useful,
2737,81,16,"['tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
2738,81,16,"['tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2,shape,
2739,81,16,"['tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
2740,82,16,"['tensor', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
2741,82,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
2742,82,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,ndim,
2743,82,16,"['tensor', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
2744,82,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,ndim,
2745,82,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,ndim,
2746,82,16,"['tensor', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
2747,82,16,"['tensor', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
2748,82,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
2751,82,16,"['tensor', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
2788,85,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,2,not useful,
2753,82,16,"['tensor', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
2754,82,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
2791,85,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,2,not useful,
2755,82,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2,ndim,
2756,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,5,dtype(bool),
2794,85,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,2,not useful,
2757,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,5,dtype(bool),
2758,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,5,dtype(bool),
2797,85,16,"['parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2,can't handle ,
2759,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,5,dtype(bool),
2760,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,5,dtype(bool),
2761,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,5,dtype(bool),
2801,85,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,2,not useful,
2762,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,5,dtype(bool),
2763,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,5,dtype(bool),
2764,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,5,dtype(bool),
2765,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,5,dtype(bool),
2766,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,5,dtype(bool),
2767,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,5,dtype(bool),
2768,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,5,dtype(bool),
2769,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,5,dtype(bool),
2770,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,5,dtype(bool),
2771,83,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,5,dtype(bool),
2804,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,4,not useful,defaults to the <dtype> of 
2805,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,4,not useful,defaults to the <dtype> of 
2806,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,4,not useful,defaults to the <dtype> of 
2807,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,4,not useful,defaults to the <dtype> of 
2808,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,4,not useful,defaults to the <dtype> of 
2809,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,4,not useful,defaults to the <dtype> of 
2810,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,4,not useful,defaults to the <dtype> of 
2819,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3,not useful,
2820,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3,not useful,
2821,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3,not useful,
2822,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3,not useful,
2823,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3,not useful,
2824,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3,not useful,
2825,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3,not useful,
2826,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3,not useful,
2827,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3,not useful,
2828,87,15,"['default', 'uses', 'global']",By default uses the same backend as the global group.,torch.distributed.new_group.yaml,3,not useful,
2829,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3,not useful,
2830,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3,not useful,
2831,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3,not useful,
2832,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3,not useful,
2833,87,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,3,not useful,
2811,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,4,not useful,defaults to the <dtype> of 
2812,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,4,not useful,defaults to the <dtype> of 
2813,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,4,not useful,defaults to the <dtype> of 
2814,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,4,not useful,defaults to the <dtype> of 
2815,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,4,not useful,defaults to the <dtype> of 
2816,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,4,not useful,defaults to the <dtype> of 
2817,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,4,not useful,defaults to the <dtype> of 
2818,86,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,4,not useful,defaults to the <dtype> of 
2864,90,15,"['input', '*', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,not useful,
2749,82,16,"['tensor', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
4464,243,8,"['name', 'module']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2,not useful,
2865,90,15,"['input', '*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,not useful,
2866,90,15,"['input', '*', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,not useful,
2867,90,15,"['input', '*', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,3,not useful,
2868,90,15,"['input', '*', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,3,not useful,
2869,90,15,"['input', '*', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3,not useful,
2870,90,15,"['input', '*', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,not useful,
2871,90,15,"['input', '*', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,not useful,
2872,90,15,"['input', '*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,not useful,
2873,90,15,"['input', '*', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,3,not useful,
2874,90,15,"['input', '*', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,3,not useful,
2875,90,15,"['input', '*', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
2876,90,15,"['input', '*', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
2877,90,15,"['input', '*', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,not useful,
2878,90,15,"['input', '*', '*']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
2879,91,15,"['input', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,not useful,
2880,91,15,"['input', 'n']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,2,not useful,
2883,91,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,not useful,
2884,91,15,"['input', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,not useful,
2885,91,15,"['input', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,not useful,
2886,91,15,"['input', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,not useful,
2887,91,15,"['input', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,not useful,
2888,91,15,"['input', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,not useful,
2889,91,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,not useful,
2890,91,15,"['input', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,not useful,
2891,91,15,"['input', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,not useful,
2892,91,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,not useful,
2893,91,15,"['input', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,not useful,
2882,91,15,"['input', 'n']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,not useful,
2881,91,15,"['input', 'n']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,not useful,
2834,88,15,"['input', 'tensor', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
2835,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
2836,88,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,3,ndim,
2837,88,15,"['input', 'tensor', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
2838,88,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,3,ndim,
2839,88,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,3,ndim,
2840,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
2841,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
2842,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
2845,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
2846,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
2847,88,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
2848,88,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,3,ndim,
2858,89,15,"['input', 'tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,3,numeric,
2862,89,15,"['input', 'tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,numeric,
2854,89,15,"['input', 'tensor', 'shape']",input tensor of shape B times P times M .,torch.cdist.yaml,3,shape,
2855,89,15,"['input', 'tensor', 'shape']",input tensor of shape B times R times M .,torch.cdist.yaml,3,shape,
2849,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,3,shape,
2850,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,3,shape,
2851,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,3,shape,
2856,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,3,shape,
2857,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,3,shape,
2859,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,3,shape,
2860,89,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,3,shape,
2861,89,15,"['input', 'tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,3,shape,
2863,89,15,"['input', 'tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,3,shape,
2852,89,15,"['input', 'tensor', 'shape']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,3,shape,
2853,89,15,"['input', 'tensor', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,3,tensor_t,
2924,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,6,shape,
2925,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,6,shape,
2926,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,6,shape,
2927,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,6,shape,
2928,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,6,shape,
2929,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,6,shape,
2930,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,6,shape,
2931,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,6,shape,
2932,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,6,shape,
2933,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,6,shape,
2934,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,6,shape,
2935,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,6,shape,
2936,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,6,shape,
2937,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,6,shape,
2938,94,15,"['size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,6,shape,
2939,95,15,"['size', '*', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
2940,95,15,"['size', '*', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,3,shape,
2941,95,15,"['size', '*', 'm']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,3,shape,
2942,95,15,"['size', '*', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,shape,
2943,95,15,"['size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2944,95,15,"['size', '*', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
2945,95,15,"['size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2946,95,15,"['size', '*', 'm']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,3,shape,
2947,95,15,"['size', '*', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3,shape,
2948,95,15,"['size', '*', 'm']","the tensor to factor of size (*, m, n)",torch.lu.yaml,3,shape,
2949,95,15,"['size', '*', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,3,shape,
2950,95,15,"['size', '*', 'm']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,3,shape,
2951,95,15,"['size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
2952,95,15,"['size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
2953,95,15,"['size', '*', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
2894,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,dtype(callable),
2895,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2,dtype(bool),
2896,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,dtype(callable),
2897,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2,dtype(bool),
2898,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,dtype(callable),
2899,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,dtype(callable),
2900,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2,dtype(bool),
2901,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,dtype(callable),
2902,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2,dtype(bool),
2903,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,dtype(callable),
2904,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),
2906,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,dtype(callable),
2907,92,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,dtype(callable),
2908,92,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2,dtype(bool),
2909,93,15,"['tensor', 'size', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
2910,93,15,"['tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
2911,93,15,"['tensor', 'size', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
2912,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,shape,
2913,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2914,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
2915,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,shape,
2916,93,15,"['tensor', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
2954,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,7,not useful,
2955,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,7,not useful,
2956,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,7,not useful,
2957,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,7,not useful,
2958,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,7,not useful,
2959,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,7,not useful,
2960,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,7,not useful,
2961,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,7,not useful,
2962,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,7,not useful,
2963,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,7,not useful,
2964,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,7,not useful,
2965,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,7,not useful,
2966,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,7,not useful,
2967,96,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch.set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,7,not useful,
2917,93,15,"['tensor', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
2918,93,15,"['tensor', 'size', '*']","the tensor to factor of size (*, m, n)",torch.lu.yaml,3,shape,
2919,93,15,"['tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
2920,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
2921,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
2922,93,15,"['tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
2923,93,15,"['tensor', 'size', '*']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
3150,110,14,"['controls', 'whether']",controls whether to return normalized results.,torch.irfft.yaml,2,dtype(bool),
3151,110,14,"['controls', 'whether']","controls whether `input` was halfed to avoid redundancy, e.g., by `rfft()`.",torch.irfft.yaml,2,dtype(bool),
3152,110,14,"['controls', 'whether']",controls whether to return largest or smallest elements,torch.topk.yaml,2,dtype(bool),
3153,110,14,"['controls', 'whether']",controls whether to return the elements in sorted order,torch.topk.yaml,2,dtype(bool),
3154,110,14,"['controls', 'whether']",controls whether to return normalized results.,torch.fft.yaml,2,dtype(bool),
3155,110,14,"['controls', 'whether']",controls whether to return normalized results.,torch.ifft.yaml,2,dtype(bool),
3156,110,14,"['controls', 'whether']",controls whether eigenvectors have to be computed,torch.symeig.yaml,2,dtype(bool),
3157,110,14,"['controls', 'whether']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,2,dtype(bool),
3158,110,14,"['controls', 'whether']",controls whether pivoting is done.,torch.lu.yaml,2,dtype(bool),
3159,110,14,"['controls', 'whether']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2,dtype(bool),
3160,110,14,"['controls', 'whether']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2,dtype(bool),
3161,110,14,"['controls', 'whether']",Controls whether to enable flush denormal mode or not,torch.set_flush_denormal.yaml,2,dtype(bool),
3162,110,14,"['controls', 'whether']",controls whether to return normalized results.,torch.rfft.yaml,2,dtype(bool),
3163,110,14,"['controls', 'whether']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,2,dtype(bool),
3094,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hvp.yaml,2,dtype(bool),defualts to `false/true`
3095,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hvp.yaml,2,dtype(bool),defualts to `false/true`
3096,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vjp.yaml,2,dtype(bool),defualts to `false/true`
3097,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vjp.yaml,2,dtype(bool),defualts to `false/true`
3098,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jvp.yaml,2,dtype(bool),defualts to `false/true`
3099,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jvp.yaml,2,dtype(bool),defualts to `false/true`
3100,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vhp.yaml,2,dtype(bool),defualts to `false/true`
2996,99,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
3101,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vhp.yaml,2,dtype(bool),defualts to `false/true`
2998,99,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
2999,99,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
3000,99,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
3102,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jacobian.yaml,2,dtype(bool),defualts to `false/true`
3002,99,14,"['input', 'output']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
3103,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jacobian.yaml,2,dtype(bool),defualts to `false/true`
3104,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.backward.yaml,2,dtype(bool),defualts to `false/true`
3105,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.grad.yaml,2,dtype(bool),defualts to `false/true`
3006,99,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
3007,99,14,"['input', 'output']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2,not useful,
3106,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hessian.yaml,2,dtype(bool),defualts to `false/true`
3107,106,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hessian.yaml,2,dtype(bool),defualts to `false/true`
3039,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2,dtype(bool),
3042,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2,dtype(bool),
3044,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2,dtype(bool),
3046,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2,dtype(bool),
3048,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),
3049,102,14,"['false', 'inputs']","If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error.",torch.autograd.grad.yaml,2,dtype(bool),
3051,102,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2,dtype(bool),
3003,99,14,"['input', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3004,99,14,"['input', 'output']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3008,99,14,"['input', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3009,99,14,"['input', 'output']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
2982,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,7,shape,
2983,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,7,shape,
2984,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,7,shape,
2985,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,7,shape,
2986,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,7,shape,
2987,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,7,shape,
2988,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,7,shape,
2989,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,7,shape,
2990,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,7,shape,
2991,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,7,shape,
2992,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,7,shape,
2993,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,7,shape,
2994,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,7,shape,
2995,98,14,"['input', 'size', '*', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,7,shape,
2968,97,14,"['input', 'tensor', 'size', '*']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,4,shape,
2969,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,4,shape,
2970,97,14,"['input', 'tensor', 'size', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,4,shape,
3038,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hvp.yaml,2,not useful,
2971,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,4,shape,
3040,102,14,"['false', 'inputs']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2,not useful,
3041,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vjp.yaml,2,not useful,
2972,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
3043,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jvp.yaml,2,not useful,
2973,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,4,shape,
3045,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vhp.yaml,2,not useful,
2974,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
3047,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jacobian.yaml,2,not useful,
2975,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,4,shape,
2976,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,4,shape,
3050,102,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hessian.yaml,2,not useful,
2977,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,4,shape,
2978,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,4,shape,
3053,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
2979,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,4,shape,
3055,103,14,"['none', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
2980,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,4,shape,
3057,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
2981,97,14,"['input', 'tensor', 'size', '*']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,4,shape,
3059,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
3062,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,enum,
3061,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3056,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,enum,
3063,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3060,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,enum,
3065,103,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3052,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,enum,
3067,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
3054,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,enum,
3069,104,14,"['none', 'sum']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
3058,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,enum,
3071,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3064,103,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,enum,
3073,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
3076,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,enum,
3075,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3070,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,enum,
3077,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3074,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,enum,
3079,104,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3066,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,enum,
3068,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,enum,
3072,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,enum,
3083,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2,not useful,
3078,104,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,enum,
4499,248,7,"['true', 'outputs']","if `grad_outputs` is `None` and `gen_non_contig_grad_outputs` is `True`, the randomly generated gradient outputs are made to be noncontiguous",torch.autograd.gradgradcheck.yaml,2,not useful,
3146,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,enum,
3087,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2,not useful,
3140,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,enum,
3144,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,enum,
3090,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2,not useful,
3091,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2,not useful,
3092,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2,not useful,
3093,105,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2,not useful,
3136,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,enum,
3138,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,enum,
3142,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,enum,
3148,109,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,enum,
3132,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,enum,
3126,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3,enum,
3130,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3,enum,
3122,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3,enum,
3124,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,3,enum,
3128,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3,enum,
3134,108,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3,enum,
3118,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,enum,
3112,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,enum,
3116,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,enum,
3108,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,enum,
3109,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
3110,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,enum,
3111,107,14,"['reduction', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
3114,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,enum,
3113,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3120,107,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,enum,
3115,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
3080,105,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype,the desired <dtype> of
3117,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3081,105,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,the desired <dtype> of
3119,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3082,105,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,the desired <dtype> of
3121,107,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3089,105,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,dtype,the desired <dtype> of
3123,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3,not useful,
3024,101,14,"['tensor', 'compare']",the tensor to compare,torch.le.yaml,2,tensor,^(a|the)\s*tensor
3125,108,14,"['reduction', 'output', 'sum']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,3,not useful,
3025,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.le.yaml,2,tensor,^(a|the)\s*tensor
3127,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3026,101,14,"['tensor', 'compare']",the tensor to compare,torch.gt.yaml,2,tensor,^(a|the)\s*tensor
3129,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3,not useful,
3027,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.gt.yaml,2,tensor,^(a|the)\s*tensor
3131,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3028,101,14,"['tensor', 'compare']",the tensor to compare,torch.ne.yaml,2,tensor,^(a|the)\s*tensor
3133,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3029,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.ne.yaml,2,tensor,^(a|the)\s*tensor
3135,108,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3030,101,14,"['tensor', 'compare']",the tensor to compare,torch.eq.yaml,2,tensor,^(a|the)\s*tensor
3137,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
3031,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.eq.yaml,2,tensor,^(a|the)\s*tensor
3139,109,14,"['reduction', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
3032,101,14,"['tensor', 'compare']",the tensor to compare,torch.ge.yaml,2,tensor,^(a|the)\s*tensor
3141,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3033,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.ge.yaml,2,tensor,^(a|the)\s*tensor
3143,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
3034,101,14,"['tensor', 'compare']",the tensor to compare,torch.lt.yaml,2,tensor,^(a|the)\s*tensor
3145,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3035,101,14,"['tensor', 'compare']",the tensor or value to compare,torch.lt.yaml,2,tensor,^(a|the)\s*tensor
3147,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3036,101,14,"['tensor', 'compare']",first tensor to compare,torch.allclose.yaml,2,tensor_t,
3149,109,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3037,101,14,"['tensor', 'compare']",second tensor to compare,torch.allclose.yaml,2,tensor,
3010,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2,dtype(bool),
3011,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2,dtype(bool),
3012,100,14,"['tensor', 'value']",the tensor or value to compare,torch.le.yaml,2,tensor,^(a|the)\s*tensor
3013,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2,dtype(bool),
3014,100,14,"['tensor', 'value']",the tensor or value to compare,torch.gt.yaml,2,tensor,^(a|the)\s*tensor
3015,100,14,"['tensor', 'value']",the tensor or value to compare,torch.ne.yaml,2,tensor,^(a|the)\s*tensor
3016,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2,dtype(bool),
3017,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),
3018,100,14,"['tensor', 'value']",the tensor or value to compare,torch.eq.yaml,2,tensor,^(a|the)\s*tensor
3019,100,14,"['tensor', 'value']",the tensor or value to compare,torch.ge.yaml,2,tensor,^(a|the)\s*tensor
3020,100,14,"['tensor', 'value']",the tensor or value to compare,torch.lt.yaml,2,tensor,^(a|the)\s*tensor
3021,100,14,"['tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3022,100,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2,dtype(bool),
3023,100,14,"['tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3177,112,13,"['*', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
3178,112,13,"['*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
3179,112,13,"['*', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
3180,112,13,"['*', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
3181,112,13,"['*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
3182,112,13,"['*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
3183,112,13,"['*', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
3184,112,13,"['*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
3185,112,13,"['*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
3187,112,13,"['*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
3188,112,13,"['*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
3189,112,13,"['*', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
3275,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,7,device,
3272,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,7,device,
3278,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,7,device,
3242,117,13,"['false', 'instead']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
3203,114,13,"['input', 'size', '*', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,4,shape,
3204,114,13,"['input', 'size', '*', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,4,shape,
3205,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,4,shape,
3206,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
3207,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,4,shape,
5758,461,5,"['n', 'm']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
3208,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
3209,114,13,"['input', 'size', '*', 'm']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,4,shape,
3210,114,13,"['input', 'size', '*', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,4,shape,
3190,113,13,"['default', 'losses']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2,not useful,
3191,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3192,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2,not useful,
3193,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3194,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3195,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3196,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3197,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3198,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3199,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3200,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3201,113,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3202,113,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3211,114,13,"['input', 'size', '*', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,4,shape,
3212,114,13,"['input', 'size', '*', 'm']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,4,shape,
3213,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,4,shape,
3214,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,4,shape,
3215,114,13,"['input', 'size', '*', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,4,shape,
3256,118,13,"['number', 'SOME_VALUE']",any number of 1 dimensional tensors.,torch.cartesian_prod.yaml,2,not useful,any number of
3265,118,13,"['number', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
3266,118,13,"['number', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3257,118,13,"['number', 'SOME_VALUE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2,not useful,
3255,118,13,"['number', 'SOME_VALUE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,not useful,
3260,118,13,"['number', 'SOME_VALUE']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,not useful,
3261,118,13,"['number', 'SOME_VALUE']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2,not useful,
3262,118,13,"['number', 'SOME_VALUE']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,not useful,
3263,118,13,"['number', 'SOME_VALUE']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,not useful,
3264,118,13,"['number', 'SOME_VALUE']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2,not useful,
3267,118,13,"['number', 'SOME_VALUE']","the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.",torch.pca_lowrank.yaml,2,not useful,
3164,111,13,"['output', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3165,111,13,"['output', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3166,111,13,"['output', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3173,111,13,"['output', 'tuple']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,tuple+ndim(1)+shape([2]),
3168,111,13,"['output', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3169,111,13,"['output', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3171,111,13,"['output', 'tuple']",optional output tuple.,torch.lu.yaml,2,tuple,
3174,111,13,"['output', 'tuple']",optional output tuple.,torch.solve.yaml,2,tuple,
3167,111,13,"['output', 'tuple']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,tuple+ndim(1)+shape([2]),
3172,111,13,"['output', 'tuple']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,tuple+ndim(1)+shape([2]),
3170,111,13,"['output', 'tuple']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2,tuple+ndim(1)+shape([2]),
3175,111,13,"['output', 'tuple']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2,tuple+ndim(1)+shape([2]),
3176,111,13,"['output', 'tuple']",the output tuple of tensors,torch.svd.yaml,2,tuple+ndim(1)+tensor,
3216,115,13,"['tensor', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
3218,115,13,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2,shape,
3219,115,13,"['tensor', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
3220,115,13,"['tensor', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
3221,115,13,"['tensor', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
3222,115,13,"['tensor', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,2,shape,
3223,115,13,"['tensor', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
3224,115,13,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2,shape,
3225,115,13,"['tensor', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
3226,115,13,"['tensor', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
3227,115,13,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2,shape,
3243,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3244,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2,not useful,
3245,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3246,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3247,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3248,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3249,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3250,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3251,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3252,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3253,117,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3254,117,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3228,115,13,"['tensor', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
3217,115,13,"['tensor', 'n']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
3229,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,3,shape,
2843,88,15,"['input', 'tensor', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,3,not useful,
3698,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
3230,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,3,shape,
3231,116,13,"['tensor', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,3,shape,
3232,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,3,shape,
3233,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,3,shape,
3234,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,3,shape,
3235,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,3,shape,
3236,116,13,"['tensor', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,3,shape,
3237,116,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,3,shape,
3268,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,7,not useful,
3269,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,7,not useful,
3270,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,7,not useful,
3271,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,7,not useful,
3238,116,13,"['tensor', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3,shape,
3273,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,7,not useful,
3274,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,7,not useful,
3239,116,13,"['tensor', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,3,shape,
3276,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,7,not useful,
3277,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,7,not useful,
3240,116,13,"['tensor', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3,shape,
3279,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,7,not useful,
3280,119,13,"['current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,7,not useful,
3281,120,13,"['losses', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
3282,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3283,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2,not useful,
3284,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3285,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3286,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3287,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3288,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3289,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3290,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3291,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3292,120,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3293,120,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3241,116,13,"['tensor', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,3,shape,
3354,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,5,shape,
3355,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,5,shape,
3356,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,5,shape,
3357,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,5,shape,
4078,197,9,"['number', 'dimensions']",Ellipses u2026 represent a fixed number of dimensions.,torch.einsum.yaml,2,not useful,
3358,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,5,shape,
3359,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,5,shape,
3360,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,5,shape,
4639,268,7,"['first', 'input']",First input.,torch.nn.functional.cosine_similarity.yaml,2,not useful,
3361,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor (minibatch , in _channels , iH , iW)",torch.nn.functional.avg_pool2d.yaml,5,shape,
3362,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,5,shape,
3363,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,5,shape,
3364,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,5,shape,
3365,126,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,5,shape,
3366,127,12,"['input', 'tensor', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
3368,127,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
3369,127,12,"['input', 'tensor', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
3370,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
3371,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
3372,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
3373,127,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
1351,21,36,"['tensor', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2,not useful,
3374,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
3318,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,3,not useful,
3319,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,3,not useful,
3320,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3321,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3322,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,3,not useful,
3323,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,3,not useful,
3324,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3325,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3326,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3327,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3328,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3329,123,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3375,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
3376,127,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
3377,127,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
3954,183,9,"['tensor', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
3367,127,12,"['input', 'tensor', 'n']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,3,shape,
3446,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,enum,
3440,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3,enum,
3444,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3,enum,
3438,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3,enum,
3442,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3,enum,
3448,133,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3,enum,
3294,121,12,"['output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3342,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3343,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.kl_div.yaml,2,not useful,
3344,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3345,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3346,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3347,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.nll_loss.yaml,2,not useful,
3348,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3349,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.cross_entropy.yaml,2,not useful,
3350,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3351,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3352,125,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3353,125,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3295,121,12,"['output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2,dtype(int) + ndim,
3296,121,12,"['output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3297,121,12,"['output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2,dtype(int) + ndim,
3298,121,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3300,121,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3301,121,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
3302,121,12,"['output', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3304,121,12,"['output', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3494,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,enum,
3488,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3,enum,
3492,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3,enum,
3486,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3,enum,
3490,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3,enum,
3496,137,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3,enum,
3426,132,12,"['selected', 'device']",selected device.,torch.cuda.max_memory_reserved.yaml,2,dtype(device),
3427,132,12,"['selected', 'device']",selected device.,torch.cuda.memory_reserved.yaml,2,dtype(device),
3428,132,12,"['selected', 'device']",selected device.,torch.cuda.reset_max_memory_allocated.yaml,2,dtype(device),
3429,132,12,"['selected', 'device']",selected device.,torch.cuda.memory_stats.yaml,2,dtype(device),
3430,132,12,"['selected', 'device']",selected device.,torch.cuda.memory_allocated.yaml,2,dtype(device),
3431,132,12,"['selected', 'device']",selected device.,torch.cuda.current_stream.yaml,2,dtype(device),
3433,132,12,"['selected', 'device']",selected device.,torch.cuda.set_device.yaml,2,dtype(device),
3434,132,12,"['selected', 'device']",selected device.,torch.cuda.max_memory_allocated.yaml,2,dtype(device),
3435,132,12,"['selected', 'device']",selected device.,torch.cuda.memory_summary.yaml,2,dtype(device),
3436,132,12,"['selected', 'device']",selected device.,torch.cuda.default_stream.yaml,2,dtype(device),
3437,132,12,"['selected', 'device']",selected device.,torch.cuda.reset_max_memory_cached.yaml,2,dtype(device),
3462,135,12,"['set', 'false']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
3469,135,12,"['set', 'false']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3473,135,12,"['set', 'false']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3467,135,12,"['set', 'false']",Set to `True` for reduced QR decomposition and `False` for complete QR decomposition.,torch.qr.yaml,2,bool,
3452,134,12,"['set', 'points']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
3453,134,12,"['set', 'points']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
3458,134,12,"['set', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3459,134,12,"['set', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3460,134,12,"['set', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3461,134,12,"['set', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3402,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3403,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3404,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3405,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3406,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3407,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3408,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3409,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3410,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3411,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3412,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3413,130,12,"['single', 'number', 'tuple', 'padw']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3414,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3415,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3416,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3417,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3418,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3419,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3420,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3421,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3422,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3423,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3424,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3425,131,12,"['single', 'number', 'tuple', 'sw']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
3331,124,12,"['SOME_VALUE', 'SOME_VALUE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2,"int+range[-1,inf)",
3330,124,12,"['SOME_VALUE', 'SOME_VALUE']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
3332,124,12,"['SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.irfft.yaml,2,value(enum),
3334,124,12,"['SOME_VALUE', 'SOME_VALUE']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2,shape,
3335,124,12,"['SOME_VALUE', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
3336,124,12,"['SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.fft.yaml,2,value(enum),
3337,124,12,"['SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.ifft.yaml,2,value(enum),
3341,124,12,"['SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.rfft.yaml,2,value(enum),
3339,124,12,"['SOME_VALUE', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*2,
3340,124,12,"['SOME_VALUE', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*2,
3391,129,12,"['tensor', 'm']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
3392,129,12,"['tensor', 'm']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
3393,129,12,"['tensor', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
3394,129,12,"['tensor', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,2,shape,
3395,129,12,"['tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
3396,129,12,"['tensor', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
3397,129,12,"['tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2,shape,
3398,129,12,"['tensor', 'm']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
3399,129,12,"['tensor', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
3432,132,12,"['selected', 'device']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2,not useful,
3400,129,12,"['tensor', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
3401,129,12,"['tensor', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
3390,129,12,"['tensor', 'm']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
3378,128,12,"['tensor', 'size', '*', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,4,shape,
3379,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,4,shape,
3380,128,12,"['tensor', 'size', '*', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,4,shape,
3439,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3,not useful,
3381,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,4,shape,
3441,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3382,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,4,shape,
3443,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3,not useful,
3383,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,4,shape,
3445,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3384,128,12,"['tensor', 'size', '*', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,4,shape,
3447,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3385,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,4,shape,
3449,133,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3450,134,12,"['set', 'points']",the ending value for the set of points,torch.arange.yaml,2,not useful,
3451,134,12,"['set', 'points']",the starting value for the set of points.,torch.arange.yaml,2,not useful,
3386,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,4,shape,
3387,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,4,shape,
3454,134,12,"['set', 'points']",the ending value for the set of points,torch.linspace.yaml,2,not useful,
3455,134,12,"['set', 'points']",the starting value for the set of points,torch.linspace.yaml,2,not useful,
3456,134,12,"['set', 'points']",the ending value for the set of points,torch.logspace.yaml,2,not useful,
3457,134,12,"['set', 'points']",the starting value for the set of points,torch.logspace.yaml,2,not useful,
3388,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,4,shape,
3389,128,12,"['tensor', 'size', '*', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,4,shape,
3306,122,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2,dtype(bool),if `true/false`
3307,122,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2,dtype(bool),
3308,122,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype(bool),
5199,355,6,"['set', 'SOME_VALUE']","For `nccl`, this is applicable only if the environment variable `NCCL_BLOCKING_WAIT` is set to 1.",torch.distributed.init_process_group.yaml,2,not useful,
3464,135,12,"['set', 'false']",Set this to False if you want to export an untrained model.,torch.onnx.export.yaml,2,not useful,
3465,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2,not useful,
3466,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3785,164,10,"['second', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2,not useful,
3468,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3309,122,12,"['true', 'output']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2,dtype(bool),
3470,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3471,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3472,135,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3310,122,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2,dtype(bool),if `true/false`
3474,136,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,2,not useful,
3475,136,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
3476,136,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
3477,136,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,2,not useful,
3478,136,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv2d.yaml,2,not useful,
3479,136,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
3480,136,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
3481,136,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,2,not useful,
3482,136,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv1d.yaml,2,not useful,
3483,136,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv3d.yaml,2,not useful,
3484,136,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
3485,136,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
3311,122,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2,dtype(bool),if `true/false`
3487,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3,not useful,
3312,122,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2,dtype(bool),if `true/false`
3489,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3313,122,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2,dtype(bool),
3491,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3,not useful,
3314,122,12,"['true', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3493,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3315,122,12,"['true', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3495,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3317,122,12,"['true', 'output']","if `True`, the output will be in `B x T x *` format.",torch.nn.utils.rnn.pad_packed_sequence.yaml,2,dtype(bool),if `true/false`
3497,137,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3498,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,3,not useful,
3499,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,3,not useful,
3500,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3501,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
3502,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,3,not useful,
3503,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,3,not useful,
3504,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3505,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,3,not useful,
3506,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3507,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
3508,138,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3509,138,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
3510,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,3,deprecated,
3511,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,3,deprecated,
3512,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,3,deprecated,
3513,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,3,deprecated,
3514,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,3,deprecated,
3515,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,3,deprecated,
3516,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,3,deprecated,
3517,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,3,deprecated,
3518,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,deprecated,
3519,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,deprecated,
3520,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,3,deprecated,
3521,139,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,3,deprecated,
3522,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3523,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2,not useful,
3524,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3525,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3526,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3527,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3528,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3529,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3530,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3531,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3532,140,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3533,140,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3534,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
3535,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2,not useful,
3536,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3537,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3538,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
3539,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2,not useful,
3540,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3541,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3542,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3543,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3544,141,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3545,141,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3546,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.empty_strided.yaml,2,not useful,
3547,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.arange.yaml,2,not useful,
3548,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.rand.yaml,2,not useful,
3549,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.eye.yaml,2,not useful,
3550,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.ones.yaml,2,not useful,
3551,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.randperm.yaml,2,not useful,
3552,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.linspace.yaml,2,not useful,
3553,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.logspace.yaml,2,not useful,
3554,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.zeros.yaml,2,not useful,
3555,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.randn.yaml,2,not useful,
3556,142,11,"['default', 'torch.strided']",Default: `torch.strided`.,torch.full.yaml,2,not useful,
3634,150,11,"['batch', 'matrices']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2,shape,
3635,150,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,2,matrix->numeric,
3636,150,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,2,matrix->numeric,
3637,150,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.addbmm.yaml,2,matrix->numeric,
3638,150,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.addbmm.yaml,2,matrix->numeric,
3639,150,11,"['batch', 'matrices']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
3640,150,11,"['batch', 'matrices']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2,shape,
3641,150,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.bmm.yaml,2,matrix->numeric,
3642,150,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.bmm.yaml,2,matrix->numeric,
3643,150,11,"['batch', 'matrices']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
3644,150,11,"['batch', 'matrices']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
3579,145,11,"['input', 'n', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,not useful,
3580,145,11,"['input', 'n', 'n']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,3,not useful,
3581,145,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,not useful,
3582,145,11,"['input', 'n', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,not useful,
3583,145,11,"['input', 'n', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,not useful,
3584,145,11,"['input', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,not useful,
3585,145,11,"['input', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,not useful,
3586,145,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,not useful,
3587,145,11,"['input', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3588,145,11,"['input', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3589,145,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,not useful,
3569,144,11,"['input', 'tensor', 'm']",input tensor of shape B times P times M .,torch.cdist.yaml,3,not useful,
3570,144,11,"['input', 'tensor', 'm']",input tensor of shape B times R times M .,torch.cdist.yaml,3,not useful,
3571,144,11,"['input', 'tensor', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,not useful,
3572,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,not useful,
3573,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,not useful,
3574,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,not useful,
3575,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,not useful,
3576,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3577,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3578,144,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,not useful,
3568,144,11,"['input', 'tensor', 'm']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,3,not useful,
3557,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,5,shape,
3558,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,5,shape,
3559,143,11,"['input', 'tensor', 'size', '*', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,5,shape,
3560,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,5,shape,
3561,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,5,shape,
3562,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,5,shape,
4564,257,7,"['input', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2,not useful,
4643,268,7,"['first', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2,not useful,
3563,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,5,shape,
3564,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,5,shape,
3565,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,5,shape,
3566,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,5,shape,
3567,143,11,"['input', 'tensor', 'size', '*', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,5,shape,
3650,151,11,"['matrix', 'multiplied']",a dense matrix be multiplied,torch.sparse.addmm.yaml,2,matrix->numeric,
3604,147,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2,not useful,
3605,147,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2,not useful,
3649,151,11,"['matrix', 'multiplied']",a sparse matrix to be multiplied,torch.sparse.addmm.yaml,2,"sparsetensor, matrix->numeric",
3607,147,11,"['specified', 'input']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2,not useful,
3608,147,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2,not useful,
3609,147,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2,not useful,
3610,147,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2,not useful,
3611,147,11,"['specified', 'input']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2,not useful,
3647,151,11,"['matrix', 'multiplied']",the first matrix to be multiplied,torch.addmm.yaml,2,matrix->numeric,
3613,148,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
3614,148,11,"['set', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
3615,148,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
3616,148,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
3654,151,11,"['matrix', 'multiplied']",the first matrix to be multiplied,torch.mm.yaml,2,matrix->numeric,
3651,151,11,"['matrix', 'multiplied']",the first sparse matrix to be multiplied,torch.sparse.mm.yaml,2,"sparsetensor, matrix->numeric",
3646,151,11,"['matrix', 'multiplied']",the matrix to be multiplied.,torch.ormqr.yaml,2,matrix->numeric,
3652,151,11,"['matrix', 'multiplied']",the second dense matrix to be multiplied,torch.sparse.mm.yaml,2,matrix->numeric,
3648,151,11,"['matrix', 'multiplied']",the second matrix to be multiplied,torch.addmm.yaml,2,matrix->numeric,
3655,151,11,"['matrix', 'multiplied']",the second matrix to be multiplied,torch.mm.yaml,2,matrix->numeric,
3590,146,11,"['second', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3591,146,11,"['second', 'tensor']",the second input tensor,torch.min22.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3592,146,11,"['second', 'tensor']",the second input tensor,torch.max22.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3593,146,11,"['second', 'tensor']",the second input tensor,torch.add.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3594,146,11,"['second', 'tensor']",the second input tensor,torch.bitwise_and.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3595,146,11,"['second', 'tensor']",the second input tensor,torch.cross.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3598,146,11,"['second', 'tensor']",the second input tensor,torch.atan2.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3600,146,11,"['second', 'tensor']",the second input tensor,torch.bitwise_or.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
3599,146,11,"['second', 'tensor']",second tensor to compare,torch.allclose.yaml,2,tensor,
3596,146,11,"['second', 'tensor']",the second multiplicand tensor,torch.mul.yaml,2,tensor_t (tensor),
3597,146,11,"['second', 'tensor']",the second tensor to be multiplied,torch.matmul.yaml,2,tensor_t (tensor),
3612,148,11,"['set', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.empty_strided.yaml,2,dtype(bool),
3617,148,11,"['set', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3618,148,11,"['set', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3619,148,11,"['set', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,"range (-1,inf)",
3620,148,11,"['set', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.tensor.yaml,2,dtype(bool),
3621,148,11,"['set', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3622,148,11,"['set', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3623,149,11,"['set', 'true']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
3626,149,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout.yaml,2,dtype(bool),
3627,149,11,"['set', 'true']","if set to `True`, returns an info IntTensor.",torch.lu.yaml,2,dtype(bool),
3628,149,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout2d.yaml,2,dtype(bool),
4518,251,7,"['value', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
4560,257,7,"['input', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
4740,282,7,"['two', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
4773,287,7,"['matrix', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
5039,329,6,"['tensor', 'dimension']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
5819,473,5,"['one', 'dimension']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2,not useful,
3645,151,11,"['matrix', 'multiplied']",matrix to be multiplied,torch.mv.yaml,2,not useful,
3653,151,11,"['matrix', 'multiplied']",matrix to be multiplied,torch.addmv.yaml,2,not useful,
3258,118,13,"['number', 'SOME_VALUE']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2,can't handle,
5188,354,6,"['number', 'columns']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2,can't handle,
5290,371,6,"['matrix', 'size']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2,can't handle,
3656,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
3631,149,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout3d.yaml,2,dtype(bool),
3658,152,10,"['output', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
3659,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
3632,149,11,"['set', 'true']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3633,149,11,"['set', 'true']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3662,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
3663,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3664,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
3665,152,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3624,149,11,"['set', 'true']",Set it to `True` to force CUDA headers and libraries to be included.,torch.utils.cpp_extension.load_inline.yaml,2,bool,
3629,149,11,"['set', 'true']",Set it to True` to force CUDA headers and libraries to be included.,torch.utils.cpp_extension.load.yaml,2,bool,
3630,149,11,"['set', 'true']",Set to `True` for reduced QR decomposition and `False` for complete QR decomposition.,torch.qr.yaml,2,bool,
3601,147,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype,the desired <dtype> of
3602,147,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,the desired <dtype> of
3603,147,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,the desired <dtype> of
3606,147,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,dtype,the desired <dtype> of
5799,469,5,"['=', '=']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
3676,154,10,"['*', 'm', '*']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,not useful,
3677,154,10,"['*', 'm', '*']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,3,not useful,
3678,154,10,"['*', 'm', '*']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,3,not useful,
3679,154,10,"['*', 'm', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,3,not useful,
3680,154,10,"['*', 'm', '*']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3,not useful,
3681,154,10,"['*', 'm', '*']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,3,not useful,
3682,154,10,"['*', 'm', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,3,not useful,
3683,154,10,"['*', 'm', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3685,154,10,"['*', 'm', '*']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3686,155,10,"['*', 'm', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,3,not useful,
5801,470,5,"['=', 'n']","The dimensions of Q and R are (*, m, k) and (*, k, n) respectively, where k = min(m, n) if `some:` is `True` and k = m otherwise.",torch.qr.yaml,2,can't handle,
3687,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,3,not useful,
3688,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,not useful,
3689,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,not useful,
3690,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3,not useful,
3691,155,10,"['*', 'm', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3,not useful,
3692,155,10,"['*', 'm', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,3,not useful,
3693,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3695,155,10,"['*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3666,153,10,"['*', 'n', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,not useful,
3667,153,10,"['*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,not useful,
3668,153,10,"['*', 'n', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,not useful,
3696,156,10,"['default', 'SOME_VALUE.SOME_VALUE']","When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.",torch.autograd.gradgradcheck.yaml,2,not useful,
3697,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: 0.5,torch.nn.functional.dropout.yaml,2,not useful,
3798,166,10,"['number', 'tensor']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
3699,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: 0.5,torch.nn.functional.dropout2d.yaml,2,not useful,
3700,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: 1.0,torch.nn.quantized.functional.conv2d.yaml,2,not useful,
3701,156,10,"['default', 'SOME_VALUE.SOME_VALUE']","When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.",torch.autograd.gradcheck.yaml,2,not useful,
3702,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: 0.5,torch.nn.functional.dropout3d.yaml,2,not useful,
3703,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: `10.0`.,torch.logspace.yaml,2,not useful,
3704,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3705,156,10,"['default', 'SOME_VALUE.SOME_VALUE']",Default: 1.0,torch.nn.quantized.functional.conv3d.yaml,2,not useful,
3669,153,10,"['*', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,not useful,
3670,153,10,"['*', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,not useful,
3671,153,10,"['*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,not useful,
3672,153,10,"['*', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3674,153,10,"['*', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3675,153,10,"['*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,not useful,
3766,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2,dtype(bool),
3767,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2,dtype(bool),
3768,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2,dtype(bool),
3769,163,10,"['false', 'return']","If False, return a symmetric window.",torch.hann_window.yaml,2,dtype(bool),
3716,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2,can't handle,
3717,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2,can't handle,
3718,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2,can't handle,
3719,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2,can't handle,
3720,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2,can't handle,
3721,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2,can't handle,
3722,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2,can't handle,
3770,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2,dtype(bool),
3771,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),
3725,158,10,"['input', 'number']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2,can't handle,
3726,159,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3,not useful,
3727,159,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3,not useful,
3728,159,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3,not useful,
3729,159,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3,not useful,
3772,163,10,"['false', 'return']","If False, return a symmetric window.",torch.hamming_window.yaml,2,dtype(bool),
3773,163,10,"['false', 'return']","If False, return a symmetric window.",torch.blackman_window.yaml,2,dtype(bool),
3774,163,10,"['false', 'return']","If False, return a symmetric window.",torch.bartlett_window.yaml,2,dtype(bool),
3733,159,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3,not useful,
3775,163,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2,dtype(bool),
3706,157,10,"['input', 'n', 'batch']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,not useful,
3708,157,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,not useful,
3709,157,10,"['input', 'n', 'batch']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,not useful,
3710,157,10,"['input', 'n', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,not useful,
3711,157,10,"['input', 'n', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,not useful,
3712,157,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,not useful,
3713,157,10,"['input', 'n', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,not useful,
3714,157,10,"['input', 'n', 'batch']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,not useful,
3715,157,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,not useful,
3707,157,10,"['input', 'n', 'batch']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3,not useful,
3723,158,10,"['input', 'number']",an input tensor or number,torch.result_type.yaml,2,dtype ,
3724,158,10,"['input', 'number']",an input tensor or number,torch.result_type.yaml,2,dtype ,
3730,159,10,"['input', 'output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
3731,159,10,"['input', 'output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
3734,159,10,"['input', 'output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3,not useful,
3735,159,10,"['input', 'output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,not useful,
3812,167,10,"['inputs', 'function']",A tuple of example inputs that will be passed to the function while tracing.,torch.jit.trace.yaml,2,not useful,a <dtype>
3796,166,10,"['number', 'tensor']",any number of 1 dimensional tensors.,torch.cartesian_prod.yaml,2,sturecture+tensor_t,any number of
3803,166,10,"['number', 'tensor']",any number of tensors of the same type,torch.broadcast_tensors.yaml,2,sturecture+tensor_t,any number of
3800,166,10,"['number', 'tensor']",Has to be between 0 and the number of dimensions of concatenated tensors (inclusive),torch.stack.yaml,2,"dimension (-1,inf)",
3801,166,10,"['number', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,"range (-1,inf)",
3804,166,10,"['number', 'tensor']",The number of places by which the elements of the tensor are shifted.,torch.roll.yaml,2,"dtype(int)+range(0,inf)",
3799,166,10,"['number', 'tensor']",the divisor that may be either a number or a Tensor of the same shape as the dividend,torch.remainder.yaml,2,"ndim=0, same shape can't hadnle",
3797,166,10,"['number', 'tensor']","the divisor, which may be either a number or a tensor of the same shape as the dividend",torch.fmod.yaml,2,"ndim=0, same shape can't hadnle",
5696,449,5,"['function', 'function']","If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",torch.utils.cpp_extension.load_inline.yaml,2,not general,
3805,166,10,"['number', 'tensor']",the number to fill the output tensor with.,torch.full.yaml,2,int,the number to
3661,152,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,not useful,
3657,152,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,not useful,
3660,152,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,not useful,
3776,164,10,"['second', 'input']",the second input tensor,torch.bitwise_xor.yaml,2,not useful,the (fist/second) input/output tensor
5258,365,6,"['argument', 'optional']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.backward.yaml,2,not useful,
3777,164,10,"['second', 'input']",the second input tensor,torch.min22.yaml,2,not useful,the (fist/second) input/output tensor
3778,164,10,"['second', 'input']",the second input tensor,torch.max22.yaml,2,not useful,the (fist/second) input/output tensor
3779,164,10,"['second', 'input']",the second input tensor,torch.add.yaml,2,not useful,the (fist/second) input/output tensor
3781,164,10,"['second', 'input']",the second input tensor,torch.bitwise_and.yaml,2,not useful,the (fist/second) input/output tensor
3782,164,10,"['second', 'input']",the second input tensor,torch.cross.yaml,2,not useful,the (fist/second) input/output tensor
3783,164,10,"['second', 'input']",the second input tensor,torch.atan2.yaml,2,not useful,the (fist/second) input/output tensor
3784,164,10,"['second', 'input']",the second input tensor,torch.bitwise_or.yaml,2,not useful,the (fist/second) input/output tensor
3780,164,10,"['second', 'input']",Second input (of size matching x1).,torch.nn.functional.cosine_similarity.yaml,2,not useful,
3786,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,5,not useful,the desired <dtype> of
3787,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,5,not useful,the desired <dtype> of
3788,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,5,not useful,the desired <dtype> of
3791,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,5,not useful,the desired <dtype> of
3736,160,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.le.yaml,2,bool+tensor,
3737,160,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.gt.yaml,2,bool+tensor,
5259,365,6,"['argument', 'optional']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.grad.yaml,2,not useful,
3738,160,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.ne.yaml,2,bool+tensor,
3741,160,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.ge.yaml,2,bool+tensor,
3742,160,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.lt.yaml,2,bool+tensor,
3740,160,10,"['tensor', 'must']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2,dtype+tensor,
5572,424,5,"['none', 'value']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.backward.yaml,2,not useful,
3745,160,10,"['tensor', 'must']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2,dtype+tensor,
3739,160,10,"['tensor', 'must']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,not useful,
3744,160,10,"['tensor', 'must']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2,not useful,
3789,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,5,not useful,
3790,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,5,not useful,
3765,162,10,"['tuple', 'output']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2,structure+dtype,a ... or ..
3792,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,5,not useful,
3793,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,5,not useful,
3794,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,5,not useful,
3795,165,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,5,not useful,
5573,424,5,"['none', 'value']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.grad.yaml,2,not useful,
2179,52,22,"['process', 'group']","If another specific group is specified, the calling process must be part of `group`.",torch.distributed.get_backend.yaml,2,not useful,
5501,410,5,"['tensor', 'data']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2,not useful,
1483,25,35,"['default', 'see']","If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see `get_default_dtype()`.",torch.arange.yaml,2,not useful,
4532,253,7,"['default', 'dtype']","If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see `get_default_dtype()`.",torch.arange.yaml,2,not useful,
3763,162,10,"['tuple', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,tuple+ndim(1)+shape([2]),
5866,483,5,"['+', 'SOME_VALUE']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2,can't handle,
1348,21,36,"['tensor', 'tensor']","If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2,not useful,
3758,162,10,"['tuple', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,tuple+ndim(1)+shape([2]),
2008,44,22,"['tuple', 'tensor']","If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2,not useful,
3806,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.hvp.yaml,2,not useful,
3807,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.vjp.yaml,2,not useful,
3808,167,10,"['inputs', 'function']",inputs to the function,torch.autograd.gradgradcheck.yaml,2,not useful,
3809,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.jvp.yaml,2,not useful,
3810,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.vhp.yaml,2,not useful,
3811,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.jacobian.yaml,2,not useful,
4578,259,7,"['tensor', 'tuple']","If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2,not useful,
3813,167,10,"['inputs', 'function']",inputs to the function,torch.autograd.gradcheck.yaml,2,not useful,
3814,167,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.hessian.yaml,2,not useful,
5048,330,6,"['tensor', 'tuple', 'tensor']","If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,3,not useful,
3816,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,8,not useful,
3817,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,8,not useful,
3818,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,8,not useful,
3819,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,8,not useful,
3820,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,8,not useful,
3821,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,8,not useful,
3822,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,8,not useful,
3823,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,8,not useful,
3824,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,8,not useful,
3825,168,10,"['returns', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,8,not useful,
3826,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2,can't handle,
3827,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2,can't handle,
911,11,54,"['data', 'type']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,2,not useful,
3829,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2,can't handle,
3830,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2,can't handle,
3831,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2,can't handle,
3832,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2,can't handle,
3833,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2,can't handle,
3834,169,9,"['split', 'input']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2,can't handle,
3761,162,10,"['tuple', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,tuple+ndim(1)+shape([2]),
4707,278,7,"['dtype', 'input']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,2,not useful,
3760,162,10,"['tuple', 'output']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2,structure,tuple of 
3762,162,10,"['tuple', 'output']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2,structure,tuple of 
3756,162,10,"['tuple', 'output']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
3757,162,10,"['tuple', 'output']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
3764,162,10,"['tuple', 'output']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
3759,162,10,"['tuple', 'output']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2,structure,tuple of 
3085,105,14,"['specified', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2,not useful,
5601,430,5,"['data', 'type', 'input']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,3,not useful,
3746,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_reduce.yaml,4,dtype(bool),
3747,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.barrier.yaml,4,dtype(bool),
3748,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_gather.yaml,4,dtype(bool),
3749,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.gather.yaml,4,dtype(bool),
3750,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.broadcast_multigpu.yaml,4,dtype(bool),
3751,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.reduce.yaml,4,dtype(bool),
4247,216,8,"['default', 'specified']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2,not useful,
3752,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_gather_multigpu.yaml,4,dtype(bool),
3753,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.scatter.yaml,4,dtype(bool),
3754,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.reduce_multigpu.yaml,4,dtype(bool),
3755,161,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.broadcast.yaml,4,dtype(bool),
4165,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,enum,
4162,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,enum,
4164,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,enum,
4159,206,9,"['|', '|']",padding mode for outside grid values `'zeros'` | `'border'` | `'reflection'`.,torch.nn.functional.grid_sample.yaml,2,enum,
4160,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,enum,
4939,312,6,"['default', 'value']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2,not useful,
3862,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.std2.yaml,2,not useful,
3863,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.mean2.yaml,2,not useful,
3864,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.sum2.yaml,2,not useful,
4167,206,9,"['|', '|']",algorithm used for upsampling: `'nearest'` | `'linear'` | `'bilinear'` | `'bicubic'` | `'trilinear'` | `'area'`.,torch.nn.functional.interpolate.yaml,2,enum,
3866,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,2,not useful,
3867,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,2,not useful,
3868,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.var2.yaml,2,not useful,
3869,173,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,2,not useful,
4161,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,enum,
4163,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,enum,
4166,206,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,enum,
4195,210,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.irfft.yaml,3,dtype(bool),
4196,210,9,"['controls', 'whether', 'return']",controls whether to return largest or smallest elements,torch.topk.yaml,3,dtype(bool),
4197,210,9,"['controls', 'whether', 'return']",controls whether to return the elements in sorted order,torch.topk.yaml,3,dtype(bool),
4198,210,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.fft.yaml,3,dtype(bool),
4199,210,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.ifft.yaml,3,dtype(bool),
4200,210,9,"['controls', 'whether', 'return']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,3,dtype(bool),
4201,210,9,"['controls', 'whether', 'return']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,3,dtype(bool),
4202,210,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.rfft.yaml,3,dtype(bool),
4203,210,9,"['controls', 'whether', 'return']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,3,dtype(bool),
3865,173,9,"['dimension', 'dimensions']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,2,"list+dtype+ndim(0,1)",
4024,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2,dtype(bool),
4025,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2,dtype(bool),
4026,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2,dtype(bool),
4027,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2,dtype(bool),
4028,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),
4030,191,9,"['false', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
4031,191,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2,dtype(bool),
4032,191,9,"['false', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4039,192,9,"['first', 'tensor']",first tensor to compare,torch.allclose.yaml,2,tensor_t,
4033,192,9,"['first', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
4034,192,9,"['first', 'tensor']",the first input tensor,torch.add.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
4035,192,9,"['first', 'tensor']",the first input tensor,torch.bitwise_and.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
4038,192,9,"['first', 'tensor']",the first input tensor,torch.atan2.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
4040,192,9,"['first', 'tensor']",the first input tensor,torch.bitwise_or.yaml,2,tensor_t (tensor),the (fist/second) input/output tensor
4037,192,9,"['first', 'tensor']",the first tensor to be multiplied,torch.matmul.yaml,2,tensor_t(tensor),
4122,201,9,"['function', 'takes']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2,dtype,^dtype
4114,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,dtype(callable),
4115,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,dtype(callable),
4116,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,dtype(callable),
4117,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,dtype(callable),
4118,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,dtype(callable),
4119,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,dtype(callable),
4120,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,dtype(callable),
4121,201,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,dtype(callable),
3907,178,9,"['input', 'size', '*', 'm', 'm']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,5,shape,
3908,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,5,shape,
3909,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5,shape,
3910,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,5,shape,
3911,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5,shape,
3912,178,9,"['input', 'size', '*', 'm', 'm']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,5,shape,
3913,178,9,"['input', 'size', '*', 'm', 'm']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,5,shape,
3914,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,5,shape,
3915,178,9,"['input', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,5,shape,
5571,424,5,"['none', 'value']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2,not useful,
3871,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,6,shape,
3872,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,6,shape,
3873,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,6,shape,
3874,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,6,shape,
3875,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,6,shape,
3876,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor (minibatch , in _channels , iH , iW)",torch.nn.functional.avg_pool2d.yaml,6,shape,
3877,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,6,shape,
3878,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,6,shape,
3879,174,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,6,shape,
3926,180,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
3927,180,9,"['tensor', 'single']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
3928,180,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
3898,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,6,shape,
3899,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.avg_pool1d.yaml,6,shape,
3900,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,6,shape,
3932,180,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
3901,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,6,shape,
3934,181,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
3902,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv_transpose1d.yaml,6,shape,
3936,181,9,"['tensor', 'containing']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
3937,181,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
3938,181,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
3903,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW)",torch.nn.functional.conv1d.yaml,6,shape,
3904,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,6,shape,
3905,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,6,shape,
5576,425,5,"['none', 'specified']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2,not useful,
3906,177,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,6,shape,
3889,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,9,shape,
3890,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,9,shape,
3891,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,9,shape,
3892,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,9,shape,
3893,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,9,shape,
3894,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,9,shape,
3895,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,9,shape,
3896,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,9,shape,
3897,176,9,"['input', 'tensor', 'size', '*', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,9,shape,
3880,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,6,shape,
3881,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,6,shape,
2037,45,22,"['none', 'input']","If None, derived from the input scale",torch.nn.quantized.functional.linear.yaml,2,not useful,
3882,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,6,shape,
3883,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,6,shape,
3884,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,6,shape,
3885,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,6,shape,
3886,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,6,shape,
3961,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2,not useful,
3887,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,6,shape,
3963,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2,not useful,
3964,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2,not useful,
3888,175,9,"['input', 'tensor', 'size', '*', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,6,shape,
3966,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2,not useful,
3967,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2,not useful,
3968,184,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2,not useful,
4141,204,9,"['kh', 'kw']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2,"dtype(int)+ndim(0,1)+structure",
4142,204,9,"['kh', 'kw']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,"dtype(int)+ndim(0,1)+structure",
3971,185,9,"['target', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
4143,204,9,"['kh', 'kw']","filters of shape (in _channels , out _channels/groups , kH , kW)",torch.nn.functional.conv_transpose2d.yaml,2,shape,
4144,204,9,"['kh', 'kw']","filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.functional.conv2d.yaml,2,shape,
4145,204,9,"['kh', 'kw']","filters of shape (in _channels , out _channels/groups , kT , kH , kW)",torch.nn.functional.conv_transpose3d.yaml,2,shape,
4146,204,9,"['kh', 'kw']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2,"dtype(int)+ndim(0,1)+structure",
2038,45,22,"['none', 'input']","If None, derived from the input zero_point",torch.nn.quantized.functional.linear.yaml,2,not useful,
4147,204,9,"['kh', 'kw']","filters of shape (out _channels , in _channels/groups , kT , kH , kW)",torch.nn.functional.conv3d.yaml,2,shape,
4148,204,9,"['kh', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
3979,186,9,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.hvp.yaml,2,not useful,
3980,186,9,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
3981,186,9,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.jvp.yaml,2,not useful,
3982,186,9,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
3983,186,9,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.vhp.yaml,2,not useful,
3984,186,9,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
3985,186,9,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
4149,204,9,"['kh', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kD , kH , kW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
3987,186,9,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
3855,172,9,"['list', 'tuple']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.as_tensor.yaml,2,dtype+structure,can be a ..
3856,172,9,"['list', 'tuple']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.tensor.yaml,2,dtype+structure,can be a ..
3857,172,9,"['list', 'tuple']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2,dtype+structure,can be a ..
3861,172,9,"['list', 'tuple']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2,structure+dtype,a ... or ..
3858,172,9,"['list', 'tuple']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2,dtype+structure,can be a ..
3853,172,9,"['list', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.rand.yaml,2,dtype+structure,can be a ..
3854,172,9,"['list', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.ones.yaml,2,dtype+structure,can be a ..
3859,172,9,"['list', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.zeros.yaml,2,dtype+structure,can be a ..
3860,172,9,"['list', 'tuple']",Can be a variable number of arguments or a collection like a list or tuple.,torch.randn.yaml,2,dtype+structure,can be a ..
3997,188,9,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
3998,188,9,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
3999,188,9,"['size', 'output']",Must be the same size as the output of `func`.,torch.autograd.functional.vjp.yaml,2,not useful,
4000,188,9,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
4001,188,9,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
4002,188,9,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
4003,188,9,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
4004,188,9,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
4005,188,9,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
4177,208,9,"['m', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,2,shape,
4178,208,9,"['m', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
4179,208,9,"['m', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,2,shape,
4180,208,9,"['m', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
4183,208,9,"['m', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
4184,208,9,"['m', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,2,shape,
4186,209,9,"['n-dimensional', 'torch.tensor']","an n-dimensional torch.Tensor, where n >= 2",torch.nn.init.orthogonal_.yaml,2,ndim,>=2
4187,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,2,ndim,
4188,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,2,ndim,
4189,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,2,ndim,
4190,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,2,ndim,
4191,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,2,ndim,
4192,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,2,ndim,
4193,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,2,ndim,
4194,209,9,"['n-dimensional', 'torch.tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,2,ndim,
4061,195,9,"['number', 'default']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2,not useful,
4060,195,9,"['number', 'default']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,"dtype(int)+ndim(0,1)+structure",
4063,195,9,"['number', 'default']",the number of columns with default being `n`,torch.eye.yaml,2,"dtype(int)+range(0,inf)",
4064,195,9,"['number', 'default']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4065,195,9,"['number', 'default']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4066,195,9,"['number', 'default']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4067,195,9,"['number', 'default']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4068,195,9,"['number', 'default']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2,"dtype(int)+range(0,inf)",
5252,364,6,"['element', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2,not useful,
4083,197,9,"['number', 'dimensions']",Has to be between 0 and the number of dimensions of concatenated tensors (inclusive),torch.stack.yaml,2,"dimension (-1,inf)",
4079,197,9,"['number', 'dimensions']",number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively,torch.tensordot.yaml,2,"dtype(int)+range(0,inf)",
4080,197,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.irfft.yaml,2,"dtype(int)+range(0,inf)",
4081,197,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.fft.yaml,2,"dtype(int)+range(0,inf)",
4082,197,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.ifft.yaml,2,"dtype(int)+range(0,inf)",
4084,197,9,"['number', 'dimensions']",the number of dimensions,torch.mvlgamma.yaml,2,"dtype(int)+range(0,inf)",
4355,229,8,"['none', 'output']","if not `None`, the output will be padded to have length `total_length`.",torch.nn.utils.rnn.pad_packed_sequence.yaml,2,not useful,
5084,336,6,"['size', 'size']",If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.,torch.sparse_coo_tensor.yaml,2,not useful,
4086,197,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.rfft.yaml,2,"dtype(int)+range(0,inf)",
3084,105,14,"['specified', 'tensor']","If not specified, the tensor will be divided into equal chunks.",torch.cuda.comm.scatter.yaml,2,not useful,
4087,198,9,"['number', 'elements']",number of elements to combine,torch.combinations.yaml,2,"dtype(int)+range(0,inf)",
4090,198,9,"['number', 'elements']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4095,198,9,"['number', 'elements']",The number of places by which the elements of the tensor are shifted.,torch.roll.yaml,2,"dtype(int)+range(0,inf)",
4075,196,9,"['number', 'groups']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2,"dtype(int)+range(0,inf)",
4096,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,dtype(callable),
4045,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,6,not useful,
4046,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,6,not useful,
4097,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,dtype(callable),
4048,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,6,not useful,
4049,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,6,not useful,
4050,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,6,not useful,
4051,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.prod2.yaml,5,not useful,
4052,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.sum2.yaml,5,not useful,
4053,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.sum.yaml,5,not useful,
4054,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.log_softmax.yaml,5,not useful,
4055,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.cumprod.yaml,5,not useful,
4056,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.prod.yaml,5,not useful,
4057,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.softmin.yaml,5,not useful,
4058,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.cumsum.yaml,5,not useful,
4059,194,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.softmax.yaml,5,not useful,
4098,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,dtype(callable),
5073,334,6,"['size', 'dimension']","If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",torch.roll.yaml,2,can't handle,
4062,195,9,"['number', 'default']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4099,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,dtype(callable),
4100,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,dtype(callable),
4101,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,dtype(callable),
4102,199,9,"['python', 'function']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2,dtype(callable),
4103,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,dtype(callable),
4104,199,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,dtype(callable),
4069,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2,can't handle,
4070,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2,can't handle,
4071,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2,can't handle,
4072,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2,can't handle,
4073,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2,can't handle,
4074,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2,can't handle,
4105,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,not useful,
4076,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2,can't handle,
4077,196,9,"['number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2,can't handle,
5709,451,5,"['must', 'size']","If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",torch.roll.yaml,2,can't handle,
4106,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,not useful,
4107,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,not useful,
4108,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,not useful,
4109,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,not useful,
130,2,200,"['input', 'tensor']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2,not useful,
4110,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,not useful,
4111,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,not useful,
4112,200,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,not useful,
4175,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,enum,
4088,198,9,"['number', 'elements']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4089,198,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4172,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3,enum,
4091,198,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
4092,198,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4093,198,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4094,198,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4174,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3,enum,
4168,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3,enum,
4170,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,3,enum,
4173,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3,enum,
4176,207,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3,enum,
4132,203,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,not useful,
4133,203,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,not useful,
4138,203,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,not useful,
4123,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,not useful,
4124,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,not useful,
4125,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,not useful,
4126,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,not useful,
4127,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,not useful,
4128,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,not useful,
4130,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,not useful,
4131,202,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,not useful,
4006,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4007,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
2275,56,21,"['input', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2,not useful,
4008,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4009,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4010,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4011,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4012,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4013,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4014,189,9,"['single', 'number', 'tuple', 'padh', 'padw']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4015,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
2750,82,16,"['tensor', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2,not useful,
4016,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4017,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4018,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4019,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4020,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4021,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
2844,88,15,"['input', 'tensor', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,3,not useful,
4022,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
4023,190,9,"['single', 'number', 'tuple', 'sh', 'sw']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
3988,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,7,shape,
3989,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,7,shape,
4134,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
4135,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4136,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
4137,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
3990,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,7,shape,
4139,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4140,203,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3991,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,7,shape,
3992,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,7,shape,
3993,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,7,shape,
3994,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,7,shape,
3995,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,7,shape,
3996,187,9,"['size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,7,shape,
3986,186,9,"['size', 'input']",Should be of same size as input tensor.,torch.bincount.yaml,2,not useful,
3844,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,3,"int+range[-1,inf)",
3845,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.irfft.yaml,3,value(enum),
4150,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,4,not useful,
4151,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,4,not useful,
4152,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,4,not useful,
4153,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv2d.yaml,4,not useful,
4154,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,4,not useful,
4155,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv1d.yaml,4,not useful,
4156,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv3d.yaml,4,not useful,
4157,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv2d.yaml,4,not useful,
4158,205,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv3d.yaml,4,not useful,
3847,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
3848,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,shape,
3849,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.fft.yaml,3,value(enum),
3850,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.ifft.yaml,3,value(enum),
3852,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.rfft.yaml,3,value(enum),
4042,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,6,not useful,the desired <dtype> of
4043,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,6,not useful,the desired <dtype> of
4044,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,6,not useful,the desired <dtype> of
4047,193,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,6,not useful,the desired <dtype> of
3828,169,9,"['split', 'input']",dimension on which to split the input.,torch.nn.functional.glu.yaml,2,not useful,
4169,207,9,"['reduction', 'output', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3,not useful,
3970,185,9,"['target', 'output']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,not useful,
4171,207,9,"['reduction', 'output', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,3,not useful,
3972,185,9,"['target', 'output']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2,not useful,
3973,185,9,"['target', 'output']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,not useful,
3974,185,9,"['target', 'output']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2,not useful,
3975,185,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,not useful,
3977,185,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,not useful,
3978,185,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,not useful,
3935,181,9,"['tensor', 'containing']",the output tensor containing indices,torch.nonzero.yaml,2,tensor_t,
3939,181,9,"['tensor', 'containing']",Tensor containing indices into the embedding matrix,torch.nn.functional.embedding.yaml,2,tensor,^(a|the)\s*tensor
3940,181,9,"['tensor', 'containing']",the tensor containing the binary mask to index with,torch.masked_select.yaml,2,tensor,^(a|the)\s*tensor
3941,181,9,"['tensor', 'containing']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2,ndim+dtype,
2752,82,16,"['tensor', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
3942,181,9,"['tensor', 'containing']",the input tensor containing probabilities,torch.multinomial.yaml,2,tensor_t,
3952,183,9,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.kl_div.yaml,2,shape(dependent,
4185,208,9,"['m', 'n']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2,not useful,
3953,183,9,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy.yaml,2,shape(dependent,
3956,183,9,"['tensor', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3957,183,9,"['tensor', 'input']",A Tensor that is input to `functions`,torch.utils.checkpoint.checkpoint_sequential.yaml,2,tensor,^(a|the)\s*tensor
3958,183,9,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,shape(dependent,
3959,183,9,"['tensor', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3960,183,9,"['tensor', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,bool,
3955,183,9,"['tensor', 'input']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2,structure,tuple of 
3962,184,9,"['tensor', 'operation']",Tensors that participate in the collective operation.,torch.distributed.broadcast_multigpu.yaml,2,tensor,^(a|the)\s*tensor
3965,184,9,"['tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
3969,184,9,"['tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
3925,180,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,dtype(callable),
3929,180,9,"['tensor', 'single']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2,structure,
3930,180,9,"['tensor', 'single']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2,structure,
3931,180,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,dtype(callable),
3933,180,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,dtype(callable),
3943,182,9,"['tensor', 'size', '*', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,4,shape,
3944,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,4,shape,
3945,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
4204,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,7,can't handle,
4205,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,7,can't handle,
4206,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,7,can't handle,
4207,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,7,can't handle,
4208,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,7,can't handle,
4209,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,7,can't handle,
4210,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,7,can't handle,
4211,211,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,7,can't handle,
3946,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,4,shape,
3947,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4,shape,
4214,212,8,"['output', 'single']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
3948,182,9,"['tensor', 'size', '*', 'm']","the tensor to factor of size (*, m, n)",torch.lu.yaml,4,shape,
3949,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,4,shape,
3950,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,4,shape,
3951,182,9,"['tensor', 'size', '*', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,4,shape,
3920,179,9,"['tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
3921,179,9,"['tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
3923,179,9,"['tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,not useful,
3924,179,9,"['tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
3917,179,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,not useful,tuple of 
3918,179,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,not useful,tuple of 
3919,179,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,not useful,tuple of 
3916,179,9,"['tensor', 'values']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,2,not useful,
3835,170,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2,dtype(bool),if `true/false`
3837,170,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2,dtype(bool),if `true/false`
3838,170,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2,dtype(bool),if `true/false`
3839,170,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2,dtype(bool),if `true/false`
3840,170,9,"['true', 'computed']","If `True`, the Jacobian will be computed in a differentiable manner.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),if `true/false`
3841,170,9,"['true', 'computed']","If `True`, the Hessian will be computed in a differentiable manner.",torch.autograd.functional.hessian.yaml,2,dtype(bool),if `true/false`
3842,170,9,"['true', 'computed']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,dtype(bool),
4433,239,8,"['=', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4434,239,8,"['=', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4236,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
5040,329,6,"['tensor', 'dimension']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2,not useful,
4238,215,8,"['output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4239,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4240,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
4241,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4242,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4243,215,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4429,239,8,"['=', 'SOME_VALUE']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,not useful,
683,7,75,"['default', 'SOME_VALUE']",Default is the number of X columns (when specified) or 1.,torch.lobpcg.yaml,2,not useful,
3259,118,13,"['number', 'SOME_VALUE']",Default is the number of X columns (when specified) or 1.,torch.lobpcg.yaml,2,not useful,
5822,474,5,"['side', 'output']","If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.",torch.einsum.yaml,2,not useful,
4248,216,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.load.yaml,2,not useful,
4249,216,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.help.yaml,2,not useful,
4430,239,8,"['=', 'SOME_VALUE']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2,not useful,
4251,216,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.list.yaml,2,not useful,
4252,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.std2.yaml,3,not useful,
4253,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.mean2.yaml,3,not useful,
4254,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.sum2.yaml,3,not useful,
4431,239,8,"['=', 'SOME_VALUE']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,not useful,
4256,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,3,not useful,
4257,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,3,not useful,
4258,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.var2.yaml,3,not useful,
4259,217,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,3,not useful,
4432,239,8,"['=', 'SOME_VALUE']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,not useful,
4436,240,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,3,shape,
4437,240,8,"['bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,3,shape,
4438,240,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,3,shape,
4439,240,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,3,shape,
4440,240,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,3,shape,
4441,240,8,"['bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,3,shape,
4442,240,8,"['bias', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3,shape,
4268,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,3,not useful,
4269,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,3,not useful,
4270,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,3,not useful,
4443,240,8,"['bias', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3,shape,
4272,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,3,not useful,
4273,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,3,not useful,
4274,219,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,3,not useful,
4244,216,8,"['default', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,not useful,
4250,216,8,"['default', 'specified']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2,not useful,
4255,217,8,"['dimension', 'dimensions', 'reduce']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,3,not useful,
4332,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,3,dtype(bool),
4333,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,3,dtype(bool),
4334,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,3,dtype(bool),
4335,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,3,dtype(bool),
4336,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,3,dtype(bool),
4337,227,8,"['false', 'tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,dtype(bool),
4338,227,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,3,dtype(bool),
4339,227,8,"['false', 'tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,dtype(bool),
4452,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (in _channels , out _channels/groups , kH , kW)",torch.nn.functional.conv_transpose2d.yaml,5,shape,
4453,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.functional.conv2d.yaml,5,shape,
4454,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (in _channels , out _channels/groups , kT , kH , kW)",torch.nn.functional.conv_transpose3d.yaml,5,shape,
4455,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (in _channels , out _channels/groups , kW)",torch.nn.functional.conv_transpose1d.yaml,5,shape,
4456,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (out _channels , in _channels/groups , kW)",torch.nn.functional.conv1d.yaml,5,shape,
4457,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","filters of shape (out _channels , in _channels/groups , kT , kH , kW)",torch.nn.functional.conv3d.yaml,5,shape,
4458,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.quantized.functional.conv2d.yaml,5,shape,
4459,242,8,"['filters', 'shape', '_channels', '_channels/groups', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kD , kH , kW)",torch.nn.quantized.functional.conv3d.yaml,5,shape,
4470,244,8,"['floating', 'point']",A floating point value to determine the cutoff for small singular values.,torch.pinverse.yaml,2,dtype,a <dtype>
4471,244,8,"['floating', 'point']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4468,244,8,"['floating', 'point']",the floating point dtype to make the default,torch.set_default_dtype.yaml,2,dtype,the <dtype>
4475,244,8,"['floating', 'point']",the floating point tensor type or its name,torch.set_default_tensor_type.yaml,2,dtype,the <dtype>
4276,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2,not useful,if `true/false`
4277,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2,not useful,if `true/false`
4278,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2,not useful,if `true/false`
4279,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2,not useful,if `true/false`
4280,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2,not useful,if `true/false`
4281,220,8,"['input', 'independent']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4282,220,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2,not useful,if `true/false`
4283,220,8,"['input', 'independent']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
4292,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,8,shape,
4293,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,8,shape,
4294,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,8,shape,
4295,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,8,shape,
4296,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,8,shape,
4297,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,8,shape,
4298,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,8,shape,
4877,302,7,"['multiple', 'per']","If using multiple processes per machine with `nccl` backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks.",torch.distributed.init_process_group.yaml,2,not useful,
4299,222,8,"['input', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,8,shape,
4271,219,8,"['input', 'tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
4275,219,8,"['input', 'tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,not useful,
4260,218,8,"['input', 'tensor', 'size', '*', 'm']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,5,shape,
4261,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,5,shape,
4262,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5,shape,
4263,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,5,shape,
4264,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5,shape,
4265,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,5,shape,
4266,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,5,shape,
4267,218,8,"['input', 'tensor', 'size', '*', 'm']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,5,shape,
4286,221,8,"['input', 'times']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
4287,221,8,"['input', 'times']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
4284,221,8,"['input', 'times']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,2,shape,
4288,221,8,"['input', 'times']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
4289,221,8,"['input', 'times']",whether to pad `input` on both sides so that the t -th frame is centered at time t times hop _length .,torch.stft.yaml,2,dtype(bool),
4290,221,8,"['input', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
4291,221,8,"['input', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
4285,221,8,"['input', 'times']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
4424,238,8,"['n', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4426,238,8,"['n', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4420,238,8,"['n', 'SOME_VALUE']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,2,not useful,
4421,238,8,"['n', 'SOME_VALUE']","an n-dimensional torch.Tensor, where n >= 2",torch.nn.init.orthogonal_.yaml,2,not useful,>=2
4422,238,8,"['n', 'SOME_VALUE']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2,not useful,
4423,238,8,"['n', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,not useful,
4425,238,8,"['n', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4427,238,8,"['n', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4460,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.l1_unstructured.yaml,2,dtype,
4461,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_unstructured.yaml,2,dtype,
4462,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.ln_structured.yaml,2,dtype,
4463,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_structured.yaml,2,dtype,
4465,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.remove.yaml,2,dtype,
4466,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.custom_from_mask.yaml,2,dtype,
4467,243,8,"['name', 'module']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.identity.yaml,2,dtype,
4348,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
4349,229,8,"['none', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4350,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4351,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
4352,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4353,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4354,229,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5191,354,6,"['number', 'columns']","If X is specifed, the value of n (when specified) must be the number of X columns.",torch.lobpcg.yaml,2,not useful,
4366,231,8,"['number', 'output']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
4371,231,8,"['number', 'output']",the number to fill the output tensor with.,torch.full.yaml,2,int,the number to
4397,235,8,"['optional', 'tensor']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2,shape,
4401,235,8,"['optional', 'tensor']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2,shape,
4403,235,8,"['optional', 'tensor']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2,numeric,weight of/for
4402,235,8,"['optional', 'tensor']",the optional destination tensor,torch.lstsq.yaml,2,dtype,the optional <> <>
4234,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4231,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4364,231,8,"['number', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4365,231,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4233,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2,not useful,
4367,231,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
4368,231,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4369,231,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4370,231,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
347,3,163,"['output', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2,not useful,
4229,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2,not useful,
4228,214,8,"['output', '|']",interpolation mode to calculate output values `'bilinear'` | `'nearest'`.,torch.nn.functional.grid_sample.yaml,2,not useful,
4230,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2,not useful,
4232,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2,not useful,
4235,214,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4212,212,8,"['output', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4213,212,8,"['output', 'single']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2,dtype(int) + ndim,
3005,99,14,"['input', 'output']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2,not useful,
4215,212,8,"['output', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4216,212,8,"['output', 'single']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2,dtype(int) + ndim,
4217,212,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4218,212,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4219,212,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4221,213,8,"['output', 'values']",interpolation mode to calculate output values `'bilinear'` | `'nearest'`.,torch.nn.functional.grid_sample.yaml,2,not useful,
4224,213,8,"['output', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4225,213,8,"['output', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4226,213,8,"['output', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,not useful,
4227,213,8,"['output', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
4220,213,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,not useful,tuple of 
3732,159,10,"['input', 'output', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,3,not useful,
4222,213,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,not useful,tuple of 
4223,213,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,not useful,tuple of 
2997,99,14,"['input', 'output']",Input and output of the collective.,torch.distributed.all_reduce.yaml,2,not useful,
3001,99,14,"['input', 'output']",Input and output of the collective.,torch.distributed.reduce.yaml,2,not useful,
4396,235,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
4380,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,7,dtype(callable),
4398,235,8,"['optional', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
4399,235,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
4400,235,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
4381,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,7,dtype(callable),
4382,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,7,dtype(callable),
4383,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,7,dtype(callable),
4384,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,7,dtype(callable),
4385,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,7,dtype(callable),
4386,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,7,dtype(callable),
4387,233,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,7,dtype(callable),
4356,230,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.hann_window.yaml,2,not useful,the desired <dtype> of
4357,230,8,"['returned', 'window']",the size of returned window,torch.hann_window.yaml,2,not useful,
4358,230,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2,not useful,the desired <dtype> of
4359,230,8,"['returned', 'window']",the size of returned window,torch.hamming_window.yaml,2,not useful,
4412,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose2d.yaml,3,not useful,
4413,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv2d.yaml,3,not useful,
4414,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose3d.yaml,3,not useful,
4415,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose1d.yaml,3,not useful,
4416,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv1d.yaml,3,not useful,
4417,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv3d.yaml,3,not useful,
4418,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.quantized.functional.conv2d.yaml,3,not useful,
4419,237,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.quantized.functional.conv3d.yaml,3,not useful,
4360,230,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2,not useful,the desired <dtype> of
4361,230,8,"['returned', 'window']",the size of returned window,torch.blackman_window.yaml,2,not useful,
4362,230,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2,not useful,the desired <dtype> of
4363,230,8,"['returned', 'window']",the size of returned window,torch.bartlett_window.yaml,2,not useful,
4340,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4341,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.min22.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4342,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.max22.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4343,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.add.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
3802,166,10,"['number', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4344,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_and.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4345,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.cross.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4346,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.atan2.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4347,228,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_or.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
4404,236,8,"['sequence', 'tensor']",a sequence of 2 or more 2-D tensors whose product is to be determined.,torch.chain_matmul.yaml,2,ndim+dtype,
4405,236,8,"['sequence', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2,structure(sequence),
4435,239,8,"['=', 'SOME_VALUE']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2,not useful,
4406,236,8,"['sequence', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2,structure(sequence),
4408,236,8,"['sequence', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2,structure(sequence),
4410,236,8,"['sequence', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2,structure(sequence),
4411,236,8,"['sequence', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2,structure(sequence),
4407,236,8,"['sequence', 'tensor']",sequence of tensors to concatenate,torch.stack.yaml,2,structure,sequence of
4379,232,8,"['shape', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3,structure+dtype,a ... or ..
4372,232,8,"['shape', 'output', 'tensor']",the shape of the output tensor,torch.empty_strided.yaml,3,dtype(torch.Size)+ndim(1),
4373,232,8,"['shape', 'output', 'tensor']",the shape of the output tensor,torch.as_strided.yaml,3,dtype(torch.Size)+ndim(1),
4444,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose2d.yaml,3,not useful,
4445,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv2d.yaml,3,not useful,
4446,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose3d.yaml,3,not useful,
4447,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose1d.yaml,3,not useful,
4448,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv1d.yaml,3,not useful,
4449,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv3d.yaml,3,not useful,
4450,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.quantized.functional.conv2d.yaml,3,not useful,
4451,241,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.quantized.functional.conv3d.yaml,3,not useful,
4374,232,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,3,structure(sequence),
4375,232,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,3,structure(sequence),
4376,232,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,3,structure(sequence),
4377,232,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,3,structure(sequence),
4378,232,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,3,structure(sequence),
4324,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4325,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4326,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4327,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4328,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4329,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4330,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4331,226,8,"['single', 'number', 'tuple', 'dw']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,4,"dtype(int)+ndim(0,1)+structure",
4316,225,8,"['size', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4317,225,8,"['size', 'single']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2,dtype(int) + ndim,
4318,225,8,"['size', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4319,225,8,"['size', 'single']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2,dtype(int) + ndim,
4469,244,8,"['floating', 'point']",Only floating point types are supported.,torch.hann_window.yaml,2,can't handle,
4320,225,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4321,225,8,"['size', 'single']",size of a single chunk or list of sizes for each chunk,torch.split.yaml,2,doc_dtype,
4472,244,8,"['floating', 'point']",Only floating point types are supported.,torch.hamming_window.yaml,2,can't handle,
4473,244,8,"['floating', 'point']",Only floating point types are supported.,torch.blackman_window.yaml,2,can't handle,
4474,244,8,"['floating', 'point']",Only floating point types are supported.,torch.bartlett_window.yaml,2,can't handle,
5235,361,6,"['inputs', 'tuple']",It should also know how to handle the inputs passed as the tuple.,torch.utils.checkpoint.checkpoint.yaml,2,not useful,
4322,225,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4323,225,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4308,224,8,"['target', 'output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,3,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4309,224,8,"['target', 'output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,3,dtype(int) + ndim,
4310,224,8,"['target', 'output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,3,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4311,224,8,"['target', 'output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,3,dtype(int) + ndim,
4312,224,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4314,224,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4315,224,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4485,246,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
4388,234,8,"['vector', 'product']",The vector for which the Hessian vector product is computed.,torch.autograd.functional.hvp.yaml,2,ndim=1,
4487,246,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
4488,246,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
4389,234,8,"['vector', 'product']",The vector for which the vector Jacobian product is computed.,torch.autograd.functional.vjp.yaml,2,ndim=1,
4390,234,8,"['vector', 'product']",the first vector of the outer product,torch.addr.yaml,2,ndim=1,
4392,234,8,"['vector', 'product']",The vector for which the Jacobian vector product is computed.,torch.autograd.functional.jvp.yaml,2,ndim=1,
4393,234,8,"['vector', 'product']",The vector for which the vector Hessian product is computed.,torch.autograd.functional.vhp.yaml,2,ndim=1,
4394,234,8,"['vector', 'product']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,2,ndim=1,
4395,234,8,"['vector', 'product']","The ""vector"" in the Jacobian-vector product.",torch.autograd.grad.yaml,2,ndim=1,
4391,234,8,"['vector', 'product']",the second vector of the outer product,torch.addr.yaml,2,tensor_t (tensor),
4300,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std2.yaml,4,dtype(bool),
4301,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var_mean.yaml,4,dtype(bool),
4302,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var.yaml,4,dtype(bool),
2557,70,17,"['true', 'input']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2,not useful,
4303,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std_mean.yaml,4,dtype(bool),
4304,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var_mean2.yaml,4,dtype(bool),
4305,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var2.yaml,4,dtype(bool),
4306,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std_mean2.yaml,4,dtype(bool),
4307,223,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std.yaml,4,dtype(bool),
4490,247,7,"['*', 'm', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,3,shape,
4491,247,7,"['*', 'm', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
5495,409,5,"['tensor', 'used']",It should contain correctly-sized tensors to be used for output of the collective.,torch.distributed.all_gather.yaml,2,not useful,
4492,247,7,"['*', 'm', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,3,shape,
4493,247,7,"['*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
4495,247,7,"['*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
4496,247,7,"['*', 'm', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,3,shape,
4763,286,7,"['batch', 'element']",list of sequences lengths of each batch element.,torch.nn.utils.rnn.pack_padded_sequence.yaml,2,list,
4513,250,7,"['value', 'SOME_VALUE']",Default value equals 30 minutes.,torch.distributed.init_process_group.yaml,2,can't handle,
4514,250,7,"['value', 'SOME_VALUE']",Default value equals 30 minutes.,torch.distributed.new_group.yaml,2,can't handle,
4868,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.irfft.yaml,4,dtype(bool),
4869,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.fft.yaml,4,dtype(bool),
5634,436,5,"['number', 'classes']",Must be a vector with length equal to the number of classes.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not general,
3870,173,9,"['dimension', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4519,251,7,"['value', 'input']",Specifies a target value that is ignored and does not contribute to the input gradient.,torch.nn.functional.nll_loss.yaml,2,not useful,
1730,32,28,"['tensor', 'shape']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2,can't handle,
4870,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.ifft.yaml,4,dtype(bool),
4871,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,4,dtype(bool),
4523,251,7,"['value', 'input']",Specifies a target value that is ignored and does not contribute to the input gradient.,torch.nn.functional.cross_entropy.yaml,2,not useful,
4872,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,4,dtype(bool),
4525,252,7,"['elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4526,252,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4873,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.rfft.yaml,4,dtype(bool),
4528,252,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2,not useful,
4529,252,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4530,252,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4531,252,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
3743,160,10,"['tensor', 'must']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2,can't handle,
4533,253,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2,not useful,
4534,253,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2,not useful,
4535,253,7,"['default', 'dtype']",Default: dtype of `input`.,torch.sparse.sum.yaml,2,not useful,
4536,253,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2,not useful,
4537,253,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2,not useful,
4538,253,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2,not useful,
4874,301,7,"['controls', 'whether', 'return', 'results']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,4,dtype(bool),
4540,254,7,"['default', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2,not useful,
4541,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4542,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.nll_loss.yaml,2,not useful,
4543,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.cross_entropy.yaml,2,not useful,
4544,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4545,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4546,255,7,"['input', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
4547,255,7,"['input', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
4777,288,7,"['corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,not useful,
4549,255,7,"['input', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
4778,288,7,"['corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,not useful,
4779,288,7,"['corner', 'pixels']","if `True`, consider `-1` and `1` to refer to the centers of the corner pixels rather than the image corners.",torch.nn.functional.affine_grid.yaml,2,not useful,if `true/false`
4780,288,7,"['corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4553,256,7,"['input', 'points']","Geometrically, we consider the pixels of the input as squares rather than points.",torch.nn.functional.grid_sample.yaml,2,not useful,
4554,256,7,"['input', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4781,288,7,"['corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4782,288,7,"['corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,not useful,
4557,256,7,"['input', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2,not useful,
4783,288,7,"['corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
4539,254,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.ctc_loss.yaml,2,not useful,
3922,179,9,"['tensor', 'values']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4828,295,7,"['destination', 'rank']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,list,
4826,295,7,"['destination', 'rank']",Destination rank.,torch.distributed.isend.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4827,295,7,"['destination', 'rank']",Destination rank (default is 0),torch.distributed.gather.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
5043,329,6,"['tensor', 'dimension']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2,can't handle,
4829,295,7,"['destination', 'rank']",Destination rank,torch.distributed.reduce.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4830,295,7,"['destination', 'rank']",Destination rank.,torch.distributed.send.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4567,258,7,"['tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
4568,258,7,"['tensor', 'SOME_VALUE']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
4569,258,7,"['tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
4831,295,7,"['destination', 'rank']",Destination rank,torch.distributed.reduce_multigpu.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4571,258,7,"['tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
4832,295,7,"['destination', 'rank']",Destination tensor rank within `tensor_list`,torch.distributed.reduce_multigpu.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4527,252,7,"['elements', 'output']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2,dtype(bool),
4635,267,7,"['false', 'loss']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,dtype(bool),
4637,268,7,"['first', 'input']",the first input tensor,torch.bitwise_xor.yaml,2,not useful,the (fist/second) input/output tensor
4638,268,7,"['first', 'input']",the first input tensor,torch.add.yaml,2,not useful,the (fist/second) input/output tensor
4640,268,7,"['first', 'input']",the first input tensor,torch.bitwise_and.yaml,2,not useful,the (fist/second) input/output tensor
1938,41,23,"['none', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2,not useful,
1944,41,23,"['none', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2,not useful,
4641,268,7,"['first', 'input']",the first input tensor,torch.atan2.yaml,2,not useful,the (fist/second) input/output tensor
4642,268,7,"['first', 'input']",the first input tensor,torch.bitwise_or.yaml,2,not useful,the (fist/second) input/output tensor
4646,269,7,"['first', 'multiplied']",the first matrix to be multiplied,torch.addmm.yaml,2,not useful,
4650,269,7,"['first', 'multiplied']",the first matrix to be multiplied,torch.mm.yaml,2,not useful,
4648,269,7,"['first', 'multiplied']",the first sparse matrix to be multiplied,torch.sparse.mm.yaml,2,not useful,
4644,269,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,2,not useful,
4645,269,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.addbmm.yaml,2,not useful,
4649,269,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.bmm.yaml,2,not useful,
4647,269,7,"['first', 'multiplied']",the first tensor to be multiplied,torch.matmul.yaml,2,not useful,
4561,257,7,"['input', 'input']","if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.",torch.autograd.gradcheck.yaml,2,not useful,
4562,257,7,"['input', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4563,257,7,"['input', 'input']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4565,257,7,"['input', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
3086,105,14,"['specified', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2,not useful,
3088,105,14,"['specified', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2,not useful,
4595,262,7,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
4596,262,7,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
4597,262,7,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
4598,262,7,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
4566,257,7,"['input', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,not useful,
4555,256,7,"['input', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4601,262,7,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
4556,256,7,"['input', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
4603,263,7,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
4604,263,7,"['single', 'element']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
4605,263,7,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
4558,256,7,"['input', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,not useful,
4607,263,7,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
4559,256,7,"['input', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
4609,264,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
4610,264,7,"['single', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
4611,264,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
4548,255,7,"['input', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,not useful,
4550,255,7,"['input', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,not useful,
4614,264,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5577,425,5,"['none', 'specified']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2,not useful,
4551,255,7,"['input', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,not useful,
4552,255,7,"['input', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,not useful,
5578,425,5,"['none', 'specified']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2,not useful,
4770,287,7,"['matrix', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
4771,287,7,"['matrix', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
4772,287,7,"['matrix', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
5310,374,6,"['rng', 'state']",Omit stashing and restoring the RNG state during each checkpoint.,torch.utils.checkpoint.checkpoint_sequential.yaml,2,not useful,
4623,266,7,"['false', 'losses']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2,not useful,
4624,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2,not useful,
4625,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4626,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2,not useful,
4627,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4628,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4629,266,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4630,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
4631,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4632,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
4633,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4634,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4774,287,7,"['matrix', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
4636,267,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4775,287,7,"['matrix', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
4798,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.l1_unstructured.yaml,4,dtype(module),
5313,374,6,"['rng', 'state']",Omit stashing and restoring the RNG state during each checkpoint.,torch.utils.checkpoint.checkpoint.yaml,2,not useful,
4799,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_unstructured.yaml,4,dtype(module),
4800,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.ln_structured.yaml,4,dtype(module),
4801,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_structured.yaml,4,dtype(module),
4802,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.remove.yaml,4,dtype(module),
4803,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.custom_from_mask.yaml,4,dtype(module),
4804,291,7,"['module', 'containing', 'tensor', 'prune']",module containing the tensor to prune.,torch.nn.utils.prune.identity.yaml,4,dtype(module),
4041,192,9,"['first', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4759,285,7,"['n', 'k']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4085,197,9,"['number', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4761,285,7,"['n', 'k']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4776,287,7,"['matrix', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4756,285,7,"['n', 'k']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,not useful,
4760,285,7,"['n', 'k']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4957,315,6,"['dimension', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
3303,121,12,"['output', 'size']",output spatial size.,torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5042,329,6,"['tensor', 'dimension']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2,not useful,
4762,285,7,"['n', 'k']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
5007,323,6,"['input', 'matrix']",the input matrix,torch.geqrf.yaml,2,not useful,
4658,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,4,not useful,
4659,271,7,"['none', 'reduction', 'applied', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,4,not useful,
4660,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,4,not useful,
4661,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,4,not useful,
4662,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,4,not useful,
4663,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,4,not useful,
4664,271,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,4,not useful,
4665,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,5,not useful,
4666,272,7,"['none', 'reduction', 'applied', 'mean', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,5,not useful,
4667,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,5,not useful,
4668,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,5,not useful,
4669,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,5,not useful,
4670,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,5,not useful,
4671,272,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,5,not useful,
4672,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7,not useful,
4673,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7,not useful,
4674,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7,not useful,
4675,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7,not useful,
4676,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7,not useful,
4677,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,not useful,
4678,273,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7,not useful,
4679,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7,not useful,
4680,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7,not useful,
4681,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7,not useful,
4682,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7,not useful,
4683,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7,not useful,
4684,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,not useful,
4685,274,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7,not useful,
4686,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,8,not useful,
4687,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8,not useful,
4688,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
4689,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8,not useful,
4690,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8,not useful,
4691,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
4692,275,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
4693,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,6,not useful,
4694,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,6,not useful,
4695,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,6,not useful,
4696,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,6,not useful,
4697,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,6,not useful,
4698,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6,not useful,
4699,276,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,6,not useful,
4700,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7,not useful,
4701,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7,not useful,
4702,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7,not useful,
4703,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7,not useful,
4704,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7,not useful,
4705,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,not useful,
4706,277,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7,not useful,
3305,121,12,"['output', 'size']",output spatial size.,torch.nn.functional.interpolate.yaml,2,not useful,
4708,278,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2,not useful,
4709,278,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2,not useful,
4710,278,7,"['dtype', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,2,not useful,
4711,278,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2,not useful,
4712,278,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2,not useful,
4713,278,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2,not useful,
4742,283,7,"['n', 'n', 'batch']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
4744,283,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3,shape,
4745,283,7,"['n', 'n', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
4746,283,7,"['n', 'n', 'batch']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,3,shape,
4747,283,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3,shape,
4748,283,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3,shape,
4743,283,7,"['n', 'n', 'batch']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3,shape,
4752,284,7,"['n', 'n', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*3,
4754,284,7,"['n', 'n', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*3,
4749,284,7,"['n', 'n', 'SOME_VALUE']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,3,shape,
4750,284,7,"['n', 'n', 'SOME_VALUE']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
4751,284,7,"['n', 'n', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,shape,
4753,284,7,"['n', 'n', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*2,
4755,284,7,"['n', 'n', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*2,
4715,279,7,"['number', 'default', 'SOME_VALUE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,3,"int+range[-1,inf)",
4729,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,3,not useful,
4730,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
4731,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,3,not useful,
4732,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,3,not useful,
4733,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
4734,281,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
4714,279,7,"['number', 'default', 'SOME_VALUE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,3,"dtype(int)+ndim(0,1)+structure",
4716,279,7,"['number', 'default', 'SOME_VALUE']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,3,"dtype(int)+range(0,inf)",
2467,66,19,"['*', '*']","output will be in `B x T x *` if True, or in `T x B x *` otherwise",torch.nn.utils.rnn.pad_sequence.yaml,2,not useful,
4717,279,7,"['number', 'default', 'SOME_VALUE']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,3,"dtype(int)+range(0,inf)",
4718,279,7,"['number', 'default', 'SOME_VALUE']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,3,"dtype(int)+range(0,inf)",
4182,208,9,"['m', 'n']",the m by n matrix A,torch.lstsq.yaml,2,not useful,
4719,279,7,"['number', 'default', 'SOME_VALUE']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,3,"dtype(int)+range(0,inf)",
4720,279,7,"['number', 'default', 'SOME_VALUE']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,3,"dtype(int)+range(0,inf)",
4483,246,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2,not useful,
4484,246,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
4486,246,7,"['output', 'shape']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2,not useful,
4489,246,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2,not useful,
4479,245,7,"['output', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
4480,245,7,"['output', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
4481,245,7,"['output', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3,not useful,
4482,245,7,"['output', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,not useful,
4476,245,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,3,not useful,tuple of 
4477,245,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,3,not useful,tuple of 
4478,245,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,3,not useful,tuple of 
4854,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.l1_unstructured.yaml,6,dtype,
4855,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_unstructured.yaml,6,dtype,
4856,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.ln_structured.yaml,6,dtype,
4757,285,7,"['n', 'k']",Default value for n is k.,torch.lobpcg.yaml,2,not useful,
4857,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_structured.yaml,6,dtype,
4858,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.remove.yaml,6,dtype,
4859,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.custom_from_mask.yaml,6,dtype,
4860,299,7,"['parameter', 'name', 'within', 'module', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.identity.yaml,6,dtype,
4838,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,4,enum,
4835,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,4,enum,
4764,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2,not useful,
4765,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4766,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2,not useful,
4767,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4768,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4769,286,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4837,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,4,enum,
4833,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,4,enum,
4836,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,4,enum,
4839,296,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,4,enum,
4651,270,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,2,matrix->numeric,
4652,270,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.addbmm.yaml,2,matrix->numeric,
4656,270,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.bmm.yaml,2,matrix->numeric,
4655,270,7,"['second', 'multiplied']",the second dense matrix to be multiplied,torch.sparse.mm.yaml,2,matrix->numeric,
4653,270,7,"['second', 'multiplied']",the second matrix to be multiplied,torch.addmm.yaml,2,matrix->numeric,
4657,270,7,"['second', 'multiplied']",the second matrix to be multiplied,torch.mm.yaml,2,matrix->numeric,
4654,270,7,"['second', 'multiplied']",the second tensor to be multiplied,torch.matmul.yaml,2,tensor_t (tensor),
4728,281,7,"['set', 'false', 'instead']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,3,dtype(bool),
4721,280,7,"['set', 'input']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
4722,280,7,"['set', 'input']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
4723,280,7,"['set', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
4724,280,7,"['set', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
4725,280,7,"['set', 'input']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,"range (-1,inf)",
4726,280,7,"['set', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4727,280,7,"['set', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4861,300,7,"['signal_ndim', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,ndim,
4862,300,7,"['signal_ndim', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.irfft.yaml,2,value(enum),
4863,300,7,"['signal_ndim', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,ndim,
4864,300,7,"['signal_ndim', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.fft.yaml,2,value(enum),
4865,300,7,"['signal_ndim', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,ndim,
5385,387,5,"['b', '*']","output will be in `B x T x *` if True, or in `T x B x *` otherwise",torch.nn.utils.rnn.pad_sequence.yaml,2,not useful,
5400,390,5,"['true', 'otherwise']","output will be in `B x T x *` if True, or in `T x B x *` otherwise",torch.nn.utils.rnn.pad_sequence.yaml,2,not useful,
4866,300,7,"['signal_ndim', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.ifft.yaml,2,value(enum),
4867,300,7,"['signal_ndim', 'SOME_VALUE']","`signal_ndim` can only be 1, 2 or 3",torch.rfft.yaml,2,value(enum),
4602,263,7,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2,dtype(callable),
4606,263,7,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2,dtype(callable),
4608,263,7,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2,dtype(callable),
4615,264,7,"['single', 'tensor']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2,tensor_t,q: how to represent single
4612,264,7,"['single', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2,structure,
4613,264,7,"['single', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2,structure,
4599,262,7,"['size', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,2,dependency(shape),
4600,262,7,"['size', 'tensor']",Size of the sparse tensor.,torch.sparse_coo_tensor.yaml,2,info in doc_dtype,
4593,261,7,"['size', 'tuple']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2,shape dependency,
4594,261,7,"['size', 'tuple']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2,shape dependency,
4588,261,7,"['size', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4589,261,7,"['size', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4590,261,7,"['size', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4591,261,7,"['size', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4592,261,7,"['size', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4847,298,7,"['source', 'rank']",Source rank.,torch.distributed.recv.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4848,298,7,"['source', 'rank']",Source rank.,torch.distributed.broadcast_multigpu.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4849,298,7,"['source', 'rank']",Source tensor rank within `tensor_list`,torch.distributed.broadcast_multigpu.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4851,298,7,"['source', 'rank']",Source rank (default is 0),torch.distributed.scatter.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4852,298,7,"['source', 'rank']",Source rank.,torch.distributed.irecv.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4853,298,7,"['source', 'rank']",Source rank.,torch.distributed.broadcast.yaml,2,"dtype(int)+range(0,inf)",(sorce|destination) (tenosr)* rank
4850,298,7,"['source', 'rank']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2,list,
4824,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,enum,
4821,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,8,enum,
4823,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,8,enum,
4819,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,8,enum,
4820,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,8,enum,
4822,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,8,enum,
4825,294,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,8,enum,
4817,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,enum,
4814,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,7,enum,
4816,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,7,enum,
4812,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,7,enum,
4813,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,7,enum,
4815,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,7,enum,
4818,293,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,7,enum,
4834,296,7,"['reduction', 'output', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,4,not useful,
4810,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,enum,
4807,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,7,enum,
4809,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,7,enum,
4806,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,7,enum,
4808,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,7,enum,
4840,297,7,"['losses', 'batch']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2,not useful,
4841,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2,not useful,
4842,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4843,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2,not useful,
4844,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4845,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4846,297,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4811,292,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,7,enum,
4785,289,7,"['string', 'containing']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,2,"list+dtype+ndim(0,1)",
4786,289,7,"['string', 'containing']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2,"list+dtype+ndim(0,1)",
4784,289,7,"['string', 'containing']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2,string,
4787,289,7,"['string', 'containing']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2,string,
4788,289,7,"['string', 'containing']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2,string,
4789,289,7,"['string', 'containing']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2,string,
4790,289,7,"['string', 'containing']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2,string,
4794,290,7,"['string', 'name']",a string of entrypoint name defined in repo's hubconf.py,torch.hub.load.yaml,2,dtype,a <dtype>
4795,290,7,"['string', 'name']",a string of entrypoint name defined in repo's hubconf.py,torch.hub.help.yaml,2,dtype,a <dtype>
4791,290,7,"['string', 'name']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2,string,
4792,290,7,"['string', 'name']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2,string,
4793,290,7,"['string', 'name']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2,string,
4796,290,7,"['string', 'name']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2,string,
4797,290,7,"['string', 'name']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2,string,
4581,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,5,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4582,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,5,dtype(int) + ndim,
4583,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,5,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4584,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,5,dtype(int) + ndim,
4585,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,5,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4586,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,5,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4587,260,7,"['target', 'output', 'size', 'single', 'integer']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,5,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
4570,258,7,"['tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,not useful,
4572,258,7,"['tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,not useful,
4573,258,7,"['tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,not useful,
4579,259,7,"['tensor', 'tuple']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2,tensor_t,q: how to represent single
4574,259,7,"['tensor', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,dtype(callable),
4575,259,7,"['tensor', 'tuple']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,dtype(callable),
4875,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.kl_div.yaml,2,not useful,
4876,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
4576,259,7,"['tensor', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,dtype(callable),
4878,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.nll_loss.yaml,2,not useful,
4879,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.cross_entropy.yaml,2,not useful,
4880,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
4881,302,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
4577,259,7,"['tensor', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,dtype(callable),
4580,259,7,"['tensor', 'tuple']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,dtype(callable),
4504,249,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2,dtype(bool),
4505,249,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype(bool),
4506,249,7,"['true', 'compute']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2,dtype(bool),
4508,249,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2,dtype(bool),
4509,249,7,"['true', 'compute']","If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products.",torch.autograd.backward.yaml,2,dtype(bool),if `true/false`
4510,249,7,"['true', 'compute']","If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products.",torch.autograd.grad.yaml,2,dtype(bool),if `true/false`
4497,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2,dtype(bool),if `true/false`
4498,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2,dtype(bool),if `true/false`
4500,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2,dtype(bool),if `true/false`
4501,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2,dtype(bool),if `true/false`
4502,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2,dtype(bool),if `true/false`
4503,248,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2,dtype(bool),if `true/false`
4622,265,7,"['tuple', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3,structure+dtype,a ... or ..
4619,265,7,"['tuple', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,3,structure,tuple of 
4898,305,6,"['true', 'way']",Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way.,torch.autograd.backward.yaml,2,not useful,
4899,305,6,"['true', 'way']",Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way.,torch.autograd.grad.yaml,2,not useful,
4620,265,7,"['tuple', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,3,structure,tuple of 
4616,265,7,"['tuple', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,3,structure,tuple of 
4617,265,7,"['tuple', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,3,structure,tuple of 
4621,265,7,"['tuple', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,3,structure,tuple of 
4618,265,7,"['tuple', 'output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,3,structure,tuple of 
4738,282,7,"['two', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2,structure,tuple of 
4739,282,7,"['two', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2,structure,tuple of 
3316,122,12,"['true', 'output']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2,not useful,
4735,282,7,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
4736,282,7,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
4741,282,7,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
4737,282,7,"['two', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2,structure,tuple of 
4912,308,6,"['value', 'set', 'points']",the ending value for the set of points,torch.arange.yaml,3,not useful,
4913,308,6,"['value', 'set', 'points']",the starting value for the set of points.,torch.arange.yaml,3,not useful,
4914,308,6,"['value', 'set', 'points']",the ending value for the set of points,torch.linspace.yaml,3,not useful,
4915,308,6,"['value', 'set', 'points']",the starting value for the set of points,torch.linspace.yaml,3,not useful,
4916,308,6,"['value', 'set', 'points']",the ending value for the set of points,torch.logspace.yaml,3,not useful,
4917,308,6,"['value', 'set', 'points']",the starting value for the set of points,torch.logspace.yaml,3,not useful,
4521,251,7,"['value', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
4522,251,7,"['value', 'input']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,"range (-1,inf)",
4920,309,6,"['default', 'dtype', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,3,not useful,
4524,251,7,"['value', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4520,251,7,"['value', 'input']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2,numeric,weight of/for
4511,250,7,"['value', 'SOME_VALUE']","p value for the p-norm distance to calculate between each vector pair in [0, infty] .",torch.nn.functional.pdist.yaml,2,not useful,
4924,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,8,not useful,
4925,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
4926,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,8,not useful,
4927,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,8,not useful,
4928,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
4929,310,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
4930,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,6,not useful,
4931,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,6,not useful,
4932,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,6,not useful,
4933,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,6,not useful,
4934,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6,not useful,
4935,311,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,6,not useful,
4512,250,7,"['value', 'SOME_VALUE']","p value for the p-norm distance to calculate between each vector pair in [0, infty] .",torch.cdist.yaml,2,not useful,
4937,312,6,"['default', 'value']",Default value for n is k.,torch.lobpcg.yaml,2,not useful,
4938,312,6,"['default', 'value']",Default value equals 30 minutes.,torch.distributed.init_process_group.yaml,2,can't handle,
4515,250,7,"['value', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
4940,312,6,"['default', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2,not useful,
4941,312,6,"['default', 'value']",Default value equals 30 minutes.,torch.distributed.new_group.yaml,2,can't handle,
4942,313,6,"['default', '-SOME_VALUE']",Default: -1,torch.nn.functional.glu.yaml,2,not useful,
4943,313,6,"['default', '-SOME_VALUE']",Default: -1.,torch.nn.functional.gumbel_softmax.yaml,2,not useful,
4944,313,6,"['default', '-SOME_VALUE']",Default: -2.,torch.diag_embed.yaml,2,not useful,
4945,313,6,"['default', '-SOME_VALUE']",Default: -1.,torch.diag_embed.yaml,2,not useful,
4946,313,6,"['default', '-SOME_VALUE']",Default: -100,torch.nn.functional.nll_loss.yaml,2,not useful,
4947,313,6,"['default', '-SOME_VALUE']",Default: -100,torch.nn.functional.cross_entropy.yaml,2,not useful,
4948,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-12,torch.nn.functional.normalize.yaml,2,not useful,
4949,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-8,torch.nn.functional.cosine_similarity.yaml,2,not useful,
4950,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-15,torch.pinverse.yaml,2,not useful,
4951,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-08,torch.allclose.yaml,2,not useful,
4952,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-05,torch.allclose.yaml,2,not useful,
4953,314,6,"['default', '1e-SOME_VALUE']",Default: 1e-8,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
2800,85,16,"['parameters', 'prune']","parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,not useful,
4516,250,7,"['value', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
5778,465,5,"['quantized', 'input']",quantized input,torch.nn.quantized.functional.relu.yaml,2,not useful,
4805,292,7,enum,Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,7,enum,
5284,370,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,3,matrix->numeric,
5285,370,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,3,matrix->numeric,
4960,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,10,not useful,
4961,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,10,not useful,
4962,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,10,not useful,
4963,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,10,not useful,
4964,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,10,not useful,
4965,316,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,10,not useful,
4966,317,6,"['input', 'tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,3,not useful,
4967,317,6,"['input', 'tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,3,not useful,
5286,370,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.addbmm.yaml,3,matrix->numeric,
4969,317,6,"['input', 'tensor', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,3,not useful,
5287,370,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.addbmm.yaml,3,matrix->numeric,
5288,370,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.bmm.yaml,3,matrix->numeric,
5289,370,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.bmm.yaml,3,matrix->numeric,
4918,309,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,3,not useful,defaults to the <dtype> of
4919,309,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,3,not useful,defaults to the <dtype> of
4921,309,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,3,not useful,defaults to the <dtype> of
4922,309,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,3,not useful,defaults to the <dtype> of
4923,309,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,3,not useful,defaults to the <dtype> of
4936,312,6,"['default', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2,not useful,
4956,315,6,"['dimension', 'tensor']",dimension along which to split the tensor,torch.chunk.yaml,2,"int+range[-1,inf)",
4954,315,6,"['dimension', 'tensor']",dimension along which to split the tensor.,torch.split.yaml,2,"int+range[-1,inf)",
4955,315,6,"['dimension', 'tensor']",A dimension along which to chunk the tensor.,torch.cuda.comm.scatter.yaml,2,"dtype(int)+range(-1,inf)",
4958,315,6,"['dimension', 'tensor']",the dimension over which the tensors are concatenated,torch.cat.yaml,2,"dtype(int)+range(-1,inf)+ndim(0)",
4959,315,6,"['dimension', 'tensor']",a dimension along which the tensors will be concatenated.,torch.cuda.comm.gather.yaml,2,"dtype(int)+range(-1,inf)",
5253,364,6,"['element', 'tensor']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,2,not useful,
5098,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,9,dtype(bool),
5099,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,9,dtype(bool),
5100,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,9,dtype(bool),
5101,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,9,dtype(bool),
5102,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,9,dtype(bool),
5103,339,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,9,dtype(bool),
5296,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","filters of shape (in _channels , out _channels/groups , kH , kW)",torch.nn.functional.conv_transpose2d.yaml,6,shape,
5297,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.functional.conv2d.yaml,6,shape,
5298,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","filters of shape (in _channels , out _channels/groups , kT , kH , kW)",torch.nn.functional.conv_transpose3d.yaml,6,shape,
5299,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","filters of shape (out _channels , in _channels/groups , kT , kH , kW)",torch.nn.functional.conv3d.yaml,6,shape,
5300,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.quantized.functional.conv2d.yaml,6,shape,
4996,322,6,"['input', 'output', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
5301,372,6,"['filters', 'shape', '_channels', '_channels/groups', 'kh', 'kw']","quantized filters of shape (out _channels , in _channels/groups , kD , kH , kW)",torch.nn.quantized.functional.conv3d.yaml,6,shape,
5338,379,6,"['float', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,3,dtype+range(0-1),
4999,322,6,"['input', 'output', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,3,not useful,
5339,379,6,"['float', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,3,dtype+range(0-1),
5340,379,6,"['float', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,3,dtype+range(0-1),
5342,379,6,"['float', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,3,dtype+range(0-1),
5343,379,6,"['float', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,3,dtype+range(0-1),
5332,378,6,"['float', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2,dtype+range(0-1),
5333,378,6,"['float', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2,dtype+range(0-1),
5334,378,6,"['float', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2,dtype+range(0-1),
5336,378,6,"['float', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,dtype+range(0-1),
5008,324,6,"['input', 'returned']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2,not useful,
5009,324,6,"['input', 'returned']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2,not useful,
5010,324,6,"['input', 'returned']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2,not useful,
5337,378,6,"['float', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2,dtype+range(0-1),
5012,324,6,"['input', 'returned']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2,not useful,
5002,323,6,"['input', 'matrix']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
5003,323,6,"['input', 'matrix']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
5004,323,6,"['input', 'matrix']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
5005,323,6,"['input', 'matrix']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
5006,323,6,"['input', 'matrix']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
4997,322,6,"['input', 'output', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
4998,322,6,"['input', 'output', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
5000,322,6,"['input', 'output', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3,not useful,
5001,322,6,"['input', 'output', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,not useful,
5011,324,6,"['input', 'returned']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2,not useful,
5013,324,6,"['input', 'returned']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2,not useful,
4990,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv_transpose2d.yaml,7,shape,
4991,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.functional.conv2d.yaml,7,shape,
4992,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv_transpose3d.yaml,7,shape,
4993,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW)",torch.nn.functional.conv3d.yaml,7,shape,
4994,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,7,shape,
4995,321,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,7,shape,
4984,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m)",torch.lobpcg.yaml,6,shape,
4985,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,6,shape,
4986,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,6,shape,
4987,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,6,shape,
4988,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,6,shape,
4989,320,6,"['input', 'tensor', 'size', '*', 'm', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,6,shape,
4978,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,10,shape,
4979,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,10,shape,
4980,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,10,shape,
4981,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n) where * is zero or more batch dimensions",torch.inverse.yaml,10,shape,
4982,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,10,shape,
4983,319,6,"['input', 'tensor', 'size', '*', 'n', 'n', '*', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,10,shape,
4968,317,6,"['input', 'tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,3,not useful,
4970,317,6,"['input', 'tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,3,not useful,
4971,317,6,"['input', 'tensor', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,3,not useful,
4974,318,6,"['input', 'tensor', 'times']",input tensor of shape B times P times M .,torch.cdist.yaml,3,shape,
4975,318,6,"['input', 'tensor', 'times']",input tensor of shape B times R times M .,torch.cdist.yaml,3,shape,
4972,318,6,"['input', 'tensor', 'times']","input tensor (minibatch , in _channels , iT times iH , iW)",torch.nn.functional.avg_pool3d.yaml,3,shape,
4976,318,6,"['input', 'tensor', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
4977,318,6,"['input', 'tensor', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
4973,318,6,"['input', 'tensor', 'times']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,3,shape,
5230,361,6,"['inputs', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2,not useful,
5231,361,6,"['inputs', 'tuple']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2,not useful,
5232,361,6,"['inputs', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2,not useful,
5233,361,6,"['inputs', 'tuple']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2,not useful,
5234,361,6,"['inputs', 'tuple']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2,not useful,
5056,332,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3,not useful,
5057,332,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3,not useful,
5058,332,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3,not useful,
5059,332,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3,not useful,
5350,381,6,"['int', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,3,not useful,
5061,332,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3,not useful,
5062,333,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,3,not useful,
5063,333,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.functional.avg_pool3d.yaml,3,not useful,
5064,333,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,3,not useful,
5065,333,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.quantized.functional.avg_pool2d.yaml,3,not useful,
5066,333,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,3,not useful,
5067,333,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.functional.avg_pool2d.yaml,3,not useful,
5068,334,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5069,334,6,"['size', 'dimension']",the size of the original signal (without batch dimension).,torch.irfft.yaml,2,not useful,
5070,334,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5071,334,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5351,381,6,"['int', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,3,not useful,
5352,381,6,"['int', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,3,not useful,
5074,335,6,"['size', 'window']",the size of the window.,torch.nn.functional.avg_pool1d.yaml,2,not useful,
5354,381,6,"['int', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,3,not useful,
5076,335,6,"['size', 'window']",the size of window frame and STFT filter.,torch.stft.yaml,2,not useful,
5355,381,6,"['int', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,3,not useful,
5344,380,6,"['int', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2,not useful,
5345,380,6,"['int', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2,not useful,
5080,336,6,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2,not useful,
5081,336,6,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2,not useful,
5082,336,6,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2,not useful,
5083,336,6,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2,not useful,
5346,380,6,"['int', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2,not useful,
5085,336,6,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2,not useful,
5348,380,6,"['int', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,not useful,
5349,380,6,"['int', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2,not useful,
5373,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,5,structure+dtype,a ... or ..
5368,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,5,structure(sequence),
5369,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,5,structure(sequence),
5370,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,5,structure(sequence),
5371,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,5,structure(sequence),
5372,384,6,"['integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,5,structure(sequence),
5291,371,6,"['matrix', 'size']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,2,shape,
5292,371,6,"['matrix', 'size']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
5293,371,6,"['matrix', 'size']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
5294,371,6,"['matrix', 'size']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
5295,371,6,"['matrix', 'size']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
5321,376,6,"['model', 'in-place']",input model to be modified in-place,torch.quantization.prepare_qat.yaml,2,dtype(model),^input model
5325,376,6,"['model', 'in-place']",input model to be modified in-place,torch.quantization.prepare.yaml,2,dtype(model),^input model
5326,377,6,"['model', 'model']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2,dtype,a <dtype>
5329,377,6,"['model', 'model']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2,dtype,a <dtype>
5330,377,6,"['model', 'model']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2,dtype,
5104,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,9,not useful,
5105,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9,not useful,
5106,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9,not useful,
5107,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9,not useful,
5108,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9,not useful,
5109,340,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9,not useful,
5110,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9,not useful,
5111,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9,not useful,
5112,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9,not useful,
5113,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9,not useful,
5114,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9,not useful,
5115,341,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9,not useful,
5116,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,8,not useful,
5117,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
5118,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8,not useful,
5119,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8,not useful,
5120,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
5121,342,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
5122,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7,not useful,
5123,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7,not useful,
5124,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7,not useful,
5125,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7,not useful,
5126,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,not useful,
5127,343,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7,not useful,
5128,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8,not useful,
5129,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
5130,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8,not useful,
5131,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8,not useful,
5132,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
5133,344,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
5134,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9,not useful,
5135,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9,not useful,
5136,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9,not useful,
5137,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9,not useful,
5138,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9,not useful,
5139,345,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9,not useful,
5140,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8,not useful,
5141,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
5142,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8,not useful,
5143,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8,not useful,
5144,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
5145,346,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
5146,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9,not useful,
5147,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9,not useful,
5148,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9,not useful,
5149,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9,not useful,
5150,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9,not useful,
5151,347,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9,not useful,
5152,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7,not useful,
5153,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7,not useful,
5154,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7,not useful,
5155,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7,not useful,
5156,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7,not useful,
5157,348,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7,not useful,
5158,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8,not useful,
5159,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
5160,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8,not useful,
5161,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8,not useful,
5162,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
5163,349,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
5164,350,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5165,350,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5166,350,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5167,350,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5168,350,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5169,350,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5170,351,6,"['multiplier', 'alpha']",multiplier for batch1 mathbin{@} batch2 ( alpha ),torch.baddbmm.yaml,2,not useful,
5171,351,6,"['multiplier', 'alpha']",multiplier for vec1 otimes vec2 ( alpha ),torch.addr.yaml,2,not useful,
5172,351,6,"['multiplier', 'alpha']",multiplier for batch1 @ batch2 ( alpha ),torch.addbmm.yaml,2,not useful,
5173,351,6,"['multiplier', 'alpha']",multiplier for mat1 @ mat2 ( alpha ),torch.addmm.yaml,2,not useful,
5174,351,6,"['multiplier', 'alpha']",multiplier for mat1 @ mat2 ( alpha ),torch.sparse.addmm.yaml,2,not useful,
5175,351,6,"['multiplier', 'alpha']",multiplier for mat @ vec ( alpha ),torch.addmv.yaml,2,not useful,
5176,352,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.baddbmm.yaml,2,not useful,
5177,352,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addr.yaml,2,not useful,
5178,352,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addbmm.yaml,2,not useful,
5179,352,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addmm.yaml,2,not useful,
5180,352,6,"['multiplier', 'beta']",multiplier for `mat` ( beta ),torch.sparse.addmm.yaml,2,not useful,
5181,352,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addmv.yaml,2,not useful,
5182,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,6,not useful,
5183,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,6,not useful,
5184,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,6,not useful,
5185,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,6,not useful,
5186,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,6,not useful,
5187,353,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,6,not useful,
5278,369,6,"['n', 'times']",the square matrix of shape (n times n) for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2,shape,
5280,369,6,"['n', 'times']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2,shape,
4245,216,8,"['default', 'specified']",Default is the number of X columns (when specified) or 1.,torch.lobpcg.yaml,2,not useful,
5281,369,6,"['n', 'times']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
5282,369,6,"['n', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
5283,369,6,"['n', 'times']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
5194,355,6,"['set', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5279,369,6,"['n', 'times']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
5196,355,6,"['set', 'SOME_VALUE']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5197,355,6,"['set', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5198,355,6,"['set', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5189,354,6,"['number', 'columns']",the number of columns with default being `n`,torch.eye.yaml,2,"dtype(int)+range(0,inf)",
5192,354,6,"['number', 'columns']",number of columns in the 2-D matrix.,torch.triu_indices.yaml,2,"dtype(int)+range(0,inf)",
5193,354,6,"['number', 'columns']",number of columns in the 2-D matrix.,torch.tril_indices.yaml,2,"dtype(int)+range(0,inf)",
5260,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,4,shape,
5261,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,4,shape,
5262,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,4,shape,
5263,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,4,shape,
5264,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,4,shape,
5265,366,6,"['optional', 'bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,4,shape,
4885,303,6,"['output', 'tuple', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,3,tuple+ndim(1)+shape([2]),
4882,303,6,"['output', 'tuple', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,3,tuple+ndim(1)+shape([2]),
4884,303,6,"['output', 'tuple', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,3,tuple+ndim(1)+shape([2]),
4883,303,6,"['output', 'tuple', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,3,tuple+ndim(1)+shape([2]),
5212,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hvp.yaml,8,not useful,
5213,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vjp.yaml,8,not useful,
5214,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jvp.yaml,8,not useful,
5215,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vhp.yaml,8,not useful,
5216,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jacobian.yaml,8,not useful,
5217,358,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hessian.yaml,8,not useful,
5218,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.kl_div.yaml,6,not useful,
5219,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy.yaml,6,not useful,
5220,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.nll_loss.yaml,6,not useful,
5221,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.cross_entropy.yaml,6,not useful,
5222,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6,not useful,
5223,359,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.poisson_nll_loss.yaml,6,not useful,
5224,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.hvp.yaml,3,not useful,
5225,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.vjp.yaml,3,not useful,
5226,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.jvp.yaml,3,not useful,
5227,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.vhp.yaml,3,not useful,
5228,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.jacobian.yaml,3,not useful,
5229,360,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.hessian.yaml,3,not useful,
4886,303,6,"['output', 'tuple', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,3,tuple+ndim(1)+shape([2]),
4887,303,6,"['output', 'tuple', 'tensor']",the output tuple of tensors,torch.svd.yaml,3,tuple+ndim(1)+tensor,
5270,367,6,"['provided', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2,not useful,
5271,367,6,"['provided', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2,not useful,
5356,382,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2,not useful,
4517,250,7,"['value', 'SOME_VALUE']",Small value to avoid evaluation of log(0) when `log_input`=``False``.,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5357,382,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2,not useful,
5358,382,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2,not useful,
5359,382,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2,not useful,
5360,382,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2,not useful,
5236,362,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3,not useful,
5237,362,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3,not useful,
5242,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,9,not useful,
5243,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,9,not useful,
5244,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,9,not useful,
5245,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,9,not useful,
5246,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,9,not useful,
5247,363,6,"['returns', 'statistic', 'current', 'device', 'given', 'current_device', 'device', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,9,not useful,
5248,364,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5249,364,6,"['element', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5250,364,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5251,364,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
3843,170,9,"['true', 'computed']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2,not useful,
5238,362,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3,not useful,
5254,365,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5255,365,6,"['argument', 'optional']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5256,365,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5257,365,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5239,362,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3,not useful,
5241,362,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3,not useful,
5311,374,6,"['rng', 'state']",The device to return the RNG state of.,torch.cuda.get_rng_state.yaml,2,not useful,the <dtype>
5312,374,6,"['rng', 'state']",The device to set the RNG state.,torch.cuda.set_rng_state.yaml,2,not useful,the <dtype>
5206,357,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,4,not useful,
5207,357,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4,not useful,
5208,357,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5209,357,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5266,367,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5267,367,6,"['provided', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5268,367,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5269,367,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5210,357,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4,not useful,
5211,357,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4,not useful,
5272,368,6,"['provided', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5273,368,6,"['provided', 'SOME_VALUE']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5274,368,6,"['provided', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5275,368,6,"['provided', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5276,368,6,"['provided', 'SOME_VALUE']","Default: if not provided, 0.",torch.triu_indices.yaml,2,not useful,
5277,368,6,"['provided', 'SOME_VALUE']","Default: if not provided, 0.",torch.tril_indices.yaml,2,not useful,
5200,356,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,4,not useful,
5201,356,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4,not useful,
5202,356,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5203,356,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5204,356,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4,not useful,
5205,356,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4,not useful,
5195,355,6,"['set', 'SOME_VALUE']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2,not useful,
5086,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
5087,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
5088,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
5089,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
5090,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,5,"dtype(int)+ndim(0,1)+structure",
3333,124,12,"['SOME_VALUE', 'SOME_VALUE']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,2,not useful,
5091,337,6,"['single', 'number', 'tuple', 'dh', 'dw']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,5,"dtype(int)+ndim(0,1)+structure",
5072,334,6,"['size', 'dimension']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
5060,332,6,"['size', 'input', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,3,dependency(shape),
5075,335,6,"['size', 'window']",the size of returned window,torch.hann_window.yaml,2,dtype(int)+range(non-negative),
5077,335,6,"['size', 'window']",the size of returned window,torch.hamming_window.yaml,2,dtype(int)+range(non-negative),
5078,335,6,"['size', 'window']",the size of returned window,torch.blackman_window.yaml,2,dtype(int)+range(non-negative),
5079,335,6,"['size', 'window']",the size of returned window,torch.bartlett_window.yaml,2,dtype(int)+range(non-negative),
5306,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,9,enum,
5303,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,9,enum,
5305,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,9,enum,
5302,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,9,enum,
5304,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,9,enum,
5307,373,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,9,enum,
5041,329,6,"['tensor', 'dimension']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
5015,325,6,"['tensor', 'indices']",the output tensor containing indices,torch.nonzero.yaml,2,tensor_t,
5017,325,6,"['tensor', 'indices']",Tensor containing indices into the embedding matrix,torch.nn.functional.embedding.yaml,2,tensor,^(a|the)\s*tensor
5018,325,6,"['tensor', 'indices']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2,ndim+dtype,
5308,374,6,"['rng', 'state']",CPU RNG state is always forked.,torch.random.fork_rng.yaml,2,not useful,
5309,374,6,"['rng', 'state']",CPU RNG state is always forked.,torch.random.fork_rng2.yaml,2,not useful,
5014,325,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
5016,325,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
5019,325,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
4036,192,9,"['first', 'tensor']",the first multiplicand tensor,torch.mul.yaml,2,not useful,
5314,375,6,"['diagonal', 'consider']",the diagonal to consider,torch.diag.yaml,2,not useful,
5315,375,6,"['diagonal', 'consider']",which diagonal to consider.,torch.diag_embed.yaml,2,not useful,
5316,375,6,"['diagonal', 'consider']",the diagonal to consider,torch.tril.yaml,2,not useful,
5317,375,6,"['diagonal', 'consider']",the diagonal to consider,torch.triu.yaml,2,not useful,
5318,375,6,"['diagonal', 'consider']",which diagonal to consider.,torch.diagonal.yaml,2,not useful,
5319,375,6,"['diagonal', 'consider']",the diagonal to consider.,torch.diagflat.yaml,2,not useful,
5320,376,6,"['model', 'in-place']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,2,not useful,
5032,328,6,"['tensor', 'points']",the tensor with the ending points,torch.lerp.yaml,2,tensor,^(a|the)\s*tensor
5322,376,6,"['model', 'in-place']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,2,not useful,
5323,376,6,"['model', 'in-place']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,2,not useful,
5324,376,6,"['model', 'in-place']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,2,not useful,
5033,328,6,"['tensor', 'points']",the tensor with the starting points,torch.lerp.yaml,2,tensor,^(a|the)\s*tensor
5034,328,6,"['tensor', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
5327,377,6,"['model', 'model']","the inputs to the model, e.g., such that `model(*args)` is a valid invocation of the model.",torch.onnx.export.yaml,2,not useful,
5035,328,6,"['tensor', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
5036,328,6,"['tensor', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4237,215,8,"['output', 'summed']",The indices not apprearing in the output are summed over after multiplying the operands entries.,torch.einsum.yaml,2,not useful,
5037,328,6,"['tensor', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
5026,327,6,"['tensor', 'size', '*', 'm', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,5,shape,
5027,327,6,"['tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,5,shape,
5028,327,6,"['tensor', 'size', '*', 'm', 'n']","the tensor to factor of size (*, m, n)",torch.lu.yaml,5,shape,
5335,378,6,"['float', 'parameters']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2,can't handle ,
5029,327,6,"['tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,5,shape,
5030,327,6,"['tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,5,shape,
5031,327,6,"['tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,5,shape,
5044,330,6,"['tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3,not useful,
5045,330,6,"['tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3,not useful,
5341,379,6,"['float', 'parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,3,can't handle ,
5046,330,6,"['tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3,not useful,
5047,330,6,"['tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3,not useful,
5049,330,6,"['tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3,not useful,
5020,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.le.yaml,3,tensor,^(a|the)\s*tensor
5021,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.gt.yaml,3,tensor,^(a|the)\s*tensor
5347,380,6,"['int', 'parameters']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2,can't handle ,
5022,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.ne.yaml,3,tensor,^(a|the)\s*tensor
5023,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.eq.yaml,3,tensor,^(a|the)\s*tensor
5024,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.ge.yaml,3,tensor,^(a|the)\s*tensor
5025,326,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.lt.yaml,3,tensor,^(a|the)\s*tensor
4888,304,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,3,dtype(bool),if `true/false`
5353,381,6,"['int', 'parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,3,can't handle ,
4889,304,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,3,dtype(bool),if `true/false`
4890,304,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,3,dtype(bool),if `true/false`
4891,304,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,3,dtype(bool),if `true/false`
4892,304,6,"['true', 'computed', 'differentiable']","If `True`, the Jacobian will be computed in a differentiable manner.",torch.autograd.functional.jacobian.yaml,3,dtype(bool),if `true/false`
4893,304,6,"['true', 'computed', 'differentiable']","If `True`, the Hessian will be computed in a differentiable manner.",torch.autograd.functional.hessian.yaml,3,dtype(bool),if `true/false`
4900,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,8,dtype(bool),if `true/false`
4901,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,8,dtype(bool),if `true/false`
4902,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,8,dtype(bool),if `true/false`
5362,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,8,not useful,
5363,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,8,not useful,
5364,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,8,not useful,
5365,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,8,not useful,
5366,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8,not useful,
5367,383,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,8,not useful,
4903,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,8,dtype(bool),if `true/false`
4904,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,8,dtype(bool),if `true/false`
4905,306,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,8,dtype(bool),if `true/false`
4906,307,6,"['true', 'tensor']","If `True`, gradient w.r.t. `weight` will be a sparse tensor.",torch.nn.functional.embedding.yaml,2,dtype(bool),if `true/false`
4908,307,6,"['true', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
4909,307,6,"['true', 'tensor']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2,dtype(bool),if `true/false`
5374,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.kl_div.yaml,3,not useful,
5375,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy.yaml,3,not useful,
5376,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.nll_loss.yaml,3,not useful,
5377,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.cross_entropy.yaml,3,not useful,
5378,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3,not useful,
5379,385,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.poisson_nll_loss.yaml,3,not useful,
4910,307,6,"['true', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
4911,307,6,"['true', 'tensor']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,bool,
4894,305,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2,dtype(bool),if `true/false`
4895,305,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2,dtype(bool),if `true/false`
4896,305,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2,dtype(bool),if `true/false`
4897,305,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2,dtype(bool),if `true/false`
5095,338,6,"['tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,4,structure,tuple of 
5096,338,6,"['tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,4,structure,tuple of 
5092,338,6,"['tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,4,structure,tuple of 
5093,338,6,"['tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,4,structure,tuple of 
5390,388,5,"['*', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5391,388,5,"['*', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5392,388,5,"['*', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5393,388,5,"['*', '-']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5097,338,6,"['tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,4,structure,tuple of 
5094,338,6,"['tuple', 'two', 'output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,4,structure,tuple of 
5050,331,6,"['whether', 'default']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,2,dtype(bool),
5051,331,6,"['whether', 'default']",whether or not to display a progress bar to stderr Default: True,torch.hub.download_url_to_file.yaml,2,dtype(bool),
5052,331,6,"['whether', 'default']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2,dtype,
5053,331,6,"['whether', 'default']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2,dtype(bool),
5054,331,6,"['whether', 'default']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2,dtype(bool),
5055,331,6,"['whether', 'default']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2,dtype(bool),
5814,472,5,"['-', '-']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5394,388,5,"['*', '-']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5395,389,5,"['*', 'm', 'k']","multiple right-hand sides of size (*, m, k) where * is zero of more batch dimensions (b )",torch.triangular_solve.yaml,3,shape,
5396,389,5,"['*', 'm', 'k']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,3,shape,
5397,389,5,"['*', 'm', 'k']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,3,shape,
5398,389,5,"['*', 'm', 'k']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,3,shape,
5865,483,5,"['+', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2,ndim,
5409,391,5,"['true', 'output', 'computed']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,3,not useful,
5867,483,5,"['+', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2,ndim,
5868,483,5,"['+', 'SOME_VALUE']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2,ndim,
5795,469,5,"['=', '=']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,not useful,
5802,470,5,"['=', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,not useful,
5414,392,5,"['true', 'use']","If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2,not useful,
5803,470,5,"['=', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,not useful,
5800,470,5,"['=', 'n']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,not useful,
5386,387,5,"['b', '*']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype(bool),if `true/false`
5387,387,5,"['b', '*']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
5388,387,5,"['b', '*']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
5389,387,5,"['b', '*']","if `True`, the output will be in `B x T x *` format.",torch.nn.utils.rnn.pad_packed_sequence.yaml,2,dtype(bool),if `true/false`
5870,484,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.irfft.yaml,5,dtype-> bool,
5871,484,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.fft.yaml,5,dtype-> bool,
5872,484,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.ifft.yaml,5,dtype-> bool,
5873,484,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,5,dtype-> bool,
5874,484,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.rfft.yaml,5,dtype-> bool,
5608,431,5,"['data', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,not useful,
5605,431,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,not useful,the desired <dtype> of
5606,431,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,not useful,the desired <dtype> of
5607,431,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,not useful,the desired <dtype> of
5609,431,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,not useful,the desired <dtype> of
5600,430,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,3,dtype,the desired <dtype> of
5602,430,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,3,dtype,the desired <dtype> of
5603,430,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,3,dtype,the desired <dtype> of
5604,430,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,3,dtype,the desired <dtype> of
5445,399,5,"['default', '=', 'SOME_VALUE']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,3,not useful,
5446,399,5,"['default', '=', 'SOME_VALUE']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,3,not useful,
5447,399,5,"['default', '=', 'SOME_VALUE']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,3,not useful,
5448,399,5,"['default', '=', 'SOME_VALUE']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,3,not useful,
5425,395,5,"['default', 'none', 'defaults', 'device', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,5,not useful,defaults to the <dtype> of
5440,398,5,"['default', 'torch.preserve_format']",Default: `torch.preserve_format`.,torch.ones_like.yaml,2,not useful,
5441,398,5,"['default', 'torch.preserve_format']",Default: `torch.preserve_format`.,torch.empty_like.yaml,2,not useful,
5442,398,5,"['default', 'torch.preserve_format']",Default: `torch.preserve_format`.,torch.zeros_like.yaml,2,not useful,
5443,398,5,"['default', 'torch.preserve_format']",Default: `torch.preserve_format`.,torch.randn_like.yaml,2,not useful,
5444,398,5,"['default', 'torch.preserve_format']",Default: `torch.preserve_format`.,torch.rand_like.yaml,2,not useful,
5426,395,5,"['default', 'none', 'defaults', 'device', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,5,not useful,defaults to the <dtype> of
5427,395,5,"['default', 'none', 'defaults', 'device', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,5,not useful,defaults to the <dtype> of
5428,395,5,"['default', 'none', 'defaults', 'device', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,5,not useful,defaults to the <dtype> of
5429,395,5,"['default', 'none', 'defaults', 'device', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,5,not useful,defaults to the <dtype> of
5449,399,5,"['default', '=', 'SOME_VALUE']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,3,not useful,
5430,396,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,5,not useful,defaults to the <dtype> of
5431,396,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,5,not useful,defaults to the <dtype> of
5432,396,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,5,not useful,defaults to the <dtype> of
5433,396,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,5,not useful,defaults to the <dtype> of
5454,400,5,"['SOME_VALUE', 'n']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2,not useful,
5434,396,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,5,not useful,defaults to the <dtype> of
5435,397,5,"['default', 'none', 'defaults', 'layout', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,5,not useful,defaults to the <dtype> of
5436,397,5,"['default', 'none', 'defaults', 'layout', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,5,not useful,defaults to the <dtype> of
5437,397,5,"['default', 'none', 'defaults', 'layout', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,5,not useful,defaults to the <dtype> of
5438,397,5,"['default', 'none', 'defaults', 'layout', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,5,not useful,defaults to the <dtype> of
5460,402,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5439,397,5,"['default', 'none', 'defaults', 'layout', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,5,not useful,defaults to the <dtype> of
5462,402,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5463,402,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5595,429,5,"['desired', 'seed']",The desired seed.,torch.cuda.manual_seed.yaml,2,not useful,
5596,429,5,"['desired', 'seed']",The desired seed.,torch.manual_seed.yaml,2,not useful,
5466,403,5,"['dimension', 'take']",first dimension with respect to which to take diagonal.,torch.diag_embed.yaml,2,not useful,
5467,403,5,"['dimension', 'take']",second dimension with respect to which to take diagonal.,torch.diag_embed.yaml,2,not useful,
5468,403,5,"['dimension', 'take']",first dimension with respect to which to take diagonal.,torch.diagonal.yaml,2,not useful,
5469,403,5,"['dimension', 'take']",second dimension with respect to which to take diagonal.,torch.diagonal.yaml,2,not useful,
5597,429,5,"['desired', 'seed']",The desired seed.,torch.random.manual_seed2.yaml,2,not useful,
5598,429,5,"['desired', 'seed']",The desired seed.,torch.random.manual_seed.yaml,2,not useful,
5599,429,5,"['desired', 'seed']",The desired seed.,torch.cuda.manual_seed_all.yaml,2,not useful,
5456,401,5,"['dimension', 'default']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2,"int+range[-1,inf)",
5455,401,5,"['dimension', 'default']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz.yaml,2,"dtype(int)+range(-1,inf)+ndim",
5457,401,5,"['dimension', 'default']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz2.yaml,2,"dtype(int)+range(-1,inf)+ndim",
5458,401,5,"['dimension', 'default']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2,"dtype(int)+range(-1,inf)+ndim",
5459,401,5,"['dimension', 'default']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,"dtype(int)+range(0,inf)",
5461,402,5,"['dimension', 'input']",dimension on which to split the input.,torch.nn.functional.glu.yaml,2,"int+range[-1,inf)",
5464,402,5,"['dimension', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2,"dtype(int)+range(-1,inf)+ndim",
5465,403,5,"['dimension', 'take']",the dimension to take the cross-product in.,torch.cross.yaml,2,"dtype(int)+range(-1,inf)",
5555,421,5,"['false', 'input']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2,dtype(bool),
5556,421,5,"['false', 'input']","If `False`, the input will get sorted unconditionally.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype(bool),
5557,421,5,"['false', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,dtype(bool),
5558,421,5,"['false', 'input']","if `True` the loss is computed as exp(input) - target * input , if `False` then loss is input - target * log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2,dtype(bool),
5559,421,5,"['false', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,dtype(bool),
5825,475,5,"['file-like', 'object', 'implement', 'string', 'containing', 'file', 'name']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,7,string,
5826,475,5,"['file-like', 'object', 'implement', 'string', 'containing', 'file', 'name']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,7,string,
5827,475,5,"['file-like', 'object', 'implement', 'string', 'containing', 'file', 'name']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,7,string,
5828,475,5,"['file-like', 'object', 'implement', 'string', 'containing', 'file', 'name']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,7,string,
5829,475,5,"['file-like', 'object', 'implement', 'string', 'containing', 'file', 'name']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,7,string,
5560,422,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
5561,422,5,"['first', 'input', 'tensor']",the first input tensor,torch.add.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
5562,422,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_and.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
5563,422,5,"['first', 'input', 'tensor']",the first input tensor,torch.atan2.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
5564,422,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_or.yaml,3,tensor_t (tensor),the (fist/second) input/output tensor
5845,479,5,"['float', 'SOME_VALUE.SOME_VALUE', 'SOME_VALUE.SOME_VALUE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,7,dtype + range,
5846,479,5,"['float', 'SOME_VALUE.SOME_VALUE', 'SOME_VALUE.SOME_VALUE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,7,dtype + range,
5847,479,5,"['float', 'SOME_VALUE.SOME_VALUE', 'SOME_VALUE.SOME_VALUE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,7,dtype + range,
2354,60,21,"['types', 'types']",The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes.,torch.jit.trace.yaml,2,not useful,
5848,479,5,"['float', 'SOME_VALUE.SOME_VALUE', 'SOME_VALUE.SOME_VALUE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,7,dtype + range,
5190,354,6,"['number', 'columns']",Default is the number of X columns (when specified) or 1.,torch.lobpcg.yaml,2,not useful,
5849,479,5,"['float', 'SOME_VALUE.SOME_VALUE', 'SOME_VALUE.SOME_VALUE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,7,dtype + range,
5698,449,5,"['function', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2,dtype,a <dtype>
5821,474,5,"['side', 'output']",The right hand side follows after -> and gives the indices for the output.,torch.einsum.yaml,2,not useful,
5699,449,5,"['function', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2,dtype,a <dtype>
5695,449,5,"['function', 'function']",A list of function names for which to generate function bindings.,torch.utils.cpp_extension.load_inline.yaml,2,"list(callable), incorrect",
5486,407,5,"['input', 'b']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
5487,407,5,"['input', 'b']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
5485,407,5,"['input', 'b']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2,dtype(bool),if `true/false`
5488,407,5,"['input', 'b']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,2,shape,
5489,407,5,"['input', 'b']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,2,shape,
5480,406,5,"['input', 'batch', 'matrices']","the input tensor A of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3,shape,
5481,406,5,"['input', 'batch', 'matrices']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,shape,
5482,406,5,"['input', 'batch', 'matrices']","the input tensor of size (*, n, n) where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3,shape,
5483,406,5,"['input', 'batch', 'matrices']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
5484,406,5,"['input', 'batch', 'matrices']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
5490,408,5,"['input', 'matrix', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m) where * is zero or more batch dimensions",torch.triangular_solve.yaml,9,shape,
5491,408,5,"['input', 'matrix', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix b of size (*, m, k) , where * is zero or more batch dimensions",torch.cholesky_solve.yaml,9,shape,
5492,408,5,"['input', 'matrix', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,9,shape,
5493,408,5,"['input', 'matrix', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where * is zero or more batch dimensions.",torch.solve.yaml,9,shape,
5494,408,5,"['input', 'matrix', 'size', '*', 'm', '*', 'zero', 'batch', 'dimensions']","input matrix B of size (*, m, k) , where * is zero or more batch dimensions.",torch.solve.yaml,9,shape,
5475,405,5,"['input', 'tensor', 'size', '*', 'm', 'n']","The input tensor of size (*, m, n) where * is zero or more batch dimensions",torch.pinverse.yaml,6,shape,
5476,405,5,"['input', 'tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,6,shape,
5477,405,5,"['input', 'tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,6,shape,
5478,405,5,"['input', 'tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,6,shape,
5479,405,5,"['input', 'tensor', 'size', '*', 'm', 'n']","the input tensor of size (*, m, n)",torch.pca_lowrank.yaml,6,shape,
5470,404,5,"['input', 'tensor', 'values']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,3,tensor_t,
5471,404,5,"['input', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3,dtype(bool),
5472,404,5,"['input', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,dtype(bool),
5473,404,5,"['input', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3,dtype(bool),
5474,404,5,"['input', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,dtype(bool),
5860,482,5,"['int', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,6,dtype+ range,
5861,482,5,"['int', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,6,dtype+ range,
5862,482,5,"['int', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,6,dtype+ range,
5863,482,5,"['int', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,6,dtype+ range,
5864,482,5,"['int', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,6,dtype+ range,
5875,485,5,"['iterable', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2,structure,iterable of ..
5876,485,5,"['iterable', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2,structure,iterable of ..
5877,485,5,"['iterable', 'tensor']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,2,structure,iterable of ..
5540,418,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,6,not useful,
5541,418,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,6,not useful,
5542,418,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,6,not useful,
5543,418,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,6,not useful,
5544,418,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,6,not useful,
5545,419,5,"['single', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5878,485,5,"['iterable', 'tensor']",an iterable of tensors to add.,torch.cuda.comm.reduce_add.yaml,2,structure,iterable of ..
5547,419,5,"['single', 'SOME_VALUE']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5548,419,5,"['single', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5549,419,5,"['single', 'SOME_VALUE']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5879,485,5,"['iterable', 'tensor']",iterable of tensors to gather.,torch.cuda.comm.gather.yaml,2,structure,iterable of ..
5770,464,5,"['lower', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2,not useful,
5771,464,5,"['lower', 'triangular']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,not useful,
5772,464,5,"['lower', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,2,not useful,
5773,464,5,"['lower', 'triangular']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2,not useful,
5774,464,5,"['lower', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2,not useful,
5761,462,5,"['n', '>=', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,not useful,
5763,462,5,"['n', '>=', 'SOME_VALUE']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,not useful,
5762,462,5,"['n', '>=', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,not useful,
5764,462,5,"['n', '>=', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,not useful,
5760,462,5,"['n', '>=', 'SOME_VALUE']","an n-dimensional torch.Tensor, where n >= 2",torch.nn.init.orthogonal_.yaml,3,ndim,>=2
5748,459,5,"['n', 'c', 'c']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*3,
5749,459,5,"['n', 'c', 'c']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*3,
5745,459,5,"['n', 'c', 'c']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,3,shape,
5747,459,5,"['n', 'c', 'c']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
5565,423,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,13,not useful,
5566,423,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,13,not useful,
5567,423,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,13,not useful,
5568,423,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,13,not useful,
5569,423,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,13,not useful,
5746,459,5,"['n', 'c', 'c']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3,shape,
5743,458,5,"['n', 'c', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*3,
5744,458,5,"['n', 'c', 'n']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*3,
5740,458,5,"['n', 'c', 'n']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,3,shape,
5574,424,5,"['none', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2,not useful,
5742,458,5,"['n', 'c', 'n']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
3299,121,12,"['output', 'size']",the target output image size.,torch.nn.functional.affine_grid.yaml,2,not useful,
3976,185,9,"['target', 'output']",the target output image size.,torch.nn.functional.affine_grid.yaml,2,not useful,
4313,224,8,"['target', 'output', 'size']",the target output image size.,torch.nn.functional.affine_grid.yaml,3,not useful,
5741,458,5,"['n', 'c', 'n']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3,shape,
5580,426,5,"['multiplier', '@', 'alpha']",multiplier for batch1 mathbin{@} batch2 ( alpha ),torch.baddbmm.yaml,3,not useful,
5581,426,5,"['multiplier', '@', 'alpha']",multiplier for batch1 @ batch2 ( alpha ),torch.addbmm.yaml,3,not useful,
5582,426,5,"['multiplier', '@', 'alpha']",multiplier for mat1 @ mat2 ( alpha ),torch.addmm.yaml,3,not useful,
5583,426,5,"['multiplier', '@', 'alpha']",multiplier for mat1 @ mat2 ( alpha ),torch.sparse.addmm.yaml,3,not useful,
5584,426,5,"['multiplier', '@', 'alpha']",multiplier for mat @ vec ( alpha ),torch.addmv.yaml,3,not useful,
5585,427,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.baddbmm.yaml,3,not useful,
5586,427,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addr.yaml,3,not useful,
5587,427,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addbmm.yaml,3,not useful,
5588,427,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addmm.yaml,3,not useful,
5589,427,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addmv.yaml,3,not useful,
5590,428,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,5,cannot handle,
5591,428,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,5,cannot handle,
5592,428,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,5,cannot handle,
5593,428,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,5,cannot handle,
5594,428,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,5,cannot handle,
5738,457,5,"['n', 'c']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*3,
5739,457,5,"['n', 'c']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*3,
5735,457,5,"['n', 'c']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
5737,457,5,"['n', 'c']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2,shape,
5736,457,5,"['n', 'c']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,shape,
5755,461,5,"['n', 'm']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
5756,461,5,"['n', 'm']","the input tensor of size (*, m, n) where k <= n <= m.",torch.lobpcg.yaml,2,shape,
5757,461,5,"['n', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,2,shape,
5759,461,5,"['n', 'm']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,2,shape,
5726,455,5,"['n', 'n', 'k']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*3,
5728,455,5,"['n', 'n', 'k']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*3,
5727,455,5,"['n', 'n', 'k']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*2,
5729,455,5,"['n', 'n', 'k']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*2,
5730,456,5,"['n', 'SOME_VALUE', 'SOME_VALUE']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,3,shape,
5731,456,5,"['n', 'SOME_VALUE', 'SOME_VALUE']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
5732,456,5,"['n', 'SOME_VALUE', 'SOME_VALUE']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,shape,
5733,456,5,"['n', 'SOME_VALUE', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3,shape*2,
5734,456,5,"['n', 'SOME_VALUE', 'SOME_VALUE']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3,shape*2,
5750,460,5,"['n', 'times', 'n']",the square matrix of shape (n times n) for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,3,shape,
5751,460,5,"['n', 'times', 'n']","(N times C times H times W for 2D or N times C times D times H times W for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3,shape,
5615,433,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,7,not useful,
5616,433,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,7,not useful,
5617,433,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,7,not useful,
5618,433,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,7,not useful,
5619,433,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,7,not useful,
5620,434,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,2,not useful,
5621,434,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
5622,434,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,2,not useful,
5623,434,5,"['specified', 'used']","When specified, it is used as initial approximation of eigenvectors.",torch.lobpcg.yaml,2,not useful,
5624,434,5,"['specified', 'used']","When specified, it will be used as preconditioner.",torch.lobpcg.yaml,2,not useful,
5752,460,5,"['n', 'times', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3,shape,
5753,460,5,"['n', 'times', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of matrices of dimension m times n .",torch.qr.yaml,3,shape,
5754,460,5,"['n', 'times', 'n']","the input tensor of size (*, m, n) where * is zero or more batch dimensions consisting of m times n matrices.",torch.svd.yaml,3,shape,
5575,425,5,"['none', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,not useful,
5579,425,5,"['none', 'specified']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2,not useful,
5570,424,5,"['none', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2,not useful,
5626,435,5,"['number', '=']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2,not useful,
5627,435,5,"['number', '=']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2,not useful,
5628,435,5,"['number', '=']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2,not useful,
5629,435,5,"['number', '=']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2,not useful,
5625,435,5,"['number', '=']","(T, N, C) where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2,not useful,
5636,437,5,"['values', 'indices']",values selected at indices where `condition` is `True`,torch.where.yaml,2,not useful,
5637,437,5,"['values', 'indices']",values selected at indices where `condition` is `False`,torch.where.yaml,2,not useful,
5630,436,5,"['number', 'classes']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*3,
5633,436,5,"['number', 'classes']","(N, C) where C = number of classes or (N, C, H, W) in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K) where K >= 1 in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*3,
5631,436,5,"['number', 'classes']",Total number of classes.,torch.nn.functional.one_hot.yaml,2,"dtype(int)+range(0,inf)",
5632,436,5,"['number', 'classes']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,not useful,
5380,386,5,"['output', 'tensor', 'must', 'booltensor']",the output tensor that must be a BoolTensor,torch.le.yaml,4,bool+tensor,
5381,386,5,"['output', 'tensor', 'must', 'booltensor']",the output tensor that must be a BoolTensor,torch.gt.yaml,4,bool+tensor,
5382,386,5,"['output', 'tensor', 'must', 'booltensor']",the output tensor that must be a BoolTensor,torch.ne.yaml,4,bool+tensor,
5383,386,5,"['output', 'tensor', 'must', 'booltensor']",the output tensor that must be a BoolTensor,torch.ge.yaml,4,bool+tensor,
5384,386,5,"['output', 'tensor', 'must', 'booltensor']",the output tensor that must be a BoolTensor,torch.lt.yaml,4,bool+tensor,
5793,468,5,"['padding', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5794,468,5,"['padding', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
5787,467,5,"['pixels', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5789,467,5,"['pixels', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
5680,446,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,8,dtype(callable),
5681,446,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,8,dtype(callable),
5682,446,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,8,dtype(callable),
5683,446,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,8,dtype(callable),
5684,446,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,8,dtype(callable),
5685,447,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,8,dtype(callable),
5686,447,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,8,dtype(callable),
5687,447,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,8,dtype(callable),
5688,447,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,8,dtype(callable),
5689,447,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,8,dtype(callable),
5690,448,5,"['python', 'function', 'takes', 'tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,6,dtype(callable),
5691,448,5,"['python', 'function', 'takes', 'tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,6,dtype(callable),
5692,448,5,"['python', 'function', 'takes', 'tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,6,dtype(callable),
5693,448,5,"['python', 'function', 'takes', 'tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,6,dtype(callable),
5694,448,5,"['python', 'function', 'takes', 'tensor', 'tuple', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,6,dtype(callable),
5855,481,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,3,dtype(int)+ndim(0)+range(non-negative),
5856,481,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,3,dtype(int)+ndim(0)+range(non-negative),
5857,481,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,3,dtype(int)+ndim(0)+range(non-negative),
5858,481,5,"['quantity', 'parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,3,dtype(int)+ndim(0)+range(non-negative),
5859,481,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,3,dtype(int)+ndim(0)+range(non-negative),
5850,480,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,2,dtype(int)+ndim(0)+range(non-negative),
5851,480,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,2,dtype(int)+ndim(0)+range(non-negative),
5852,480,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,2,dtype(int)+ndim(0)+range(non-negative),
5853,480,5,"['quantity', 'parameters']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2,dtype(int)+ndim(0)+range(non-negative),
5675,445,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy.yaml,12,not useful,
5676,445,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.nll_loss.yaml,12,not useful,
5677,445,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.cross_entropy.yaml,12,not useful,
5678,445,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,12,not useful,
5679,445,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.poisson_nll_loss.yaml,12,not useful,
5854,480,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,2,dtype(int)+ndim(0)+range(non-negative),
5780,466,5,"['quantized', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,shape,
5781,466,5,"['quantized', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
5782,466,5,"['quantized', '_channels']","quantized filters of shape (out _channels , in _channels/groups , kH , kW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
5783,466,5,"['quantized', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
5784,466,5,"['quantized', '_channels']","quantized filters of shape (out _channels , in _channels/groups , kD , kH , kW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
5775,465,5,"['quantized', 'input']","quantized input tensor (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.avg_pool2d.yaml,2,shape,
5776,465,5,"['quantized', 'input']",dtype,torch.nn.quantized.functional.linear.yaml,2,dtype,
5777,465,5,"['quantized', 'input']","quantized input tensor of shape (minibatch , in _channels , iH , iW)",torch.nn.quantized.functional.conv2d.yaml,2,shape,
5779,465,5,"['quantized', 'input']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW)",torch.nn.quantized.functional.conv3d.yaml,2,shape,
5672,444,5,"['result', 'tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,5,structure,tuple of 
5673,444,5,"['result', 'tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,5,structure,tuple of 
5670,444,5,"['result', 'tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,5,structure,tuple of 
5671,444,5,"['result', 'tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,5,structure,tuple of 
5674,444,5,"['result', 'tuple', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,5,structure,tuple of 
5710,452,5,"['sequence', 'integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,6,structure(sequence),
5711,452,5,"['sequence', 'integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,6,structure(sequence),
5712,452,5,"['sequence', 'integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,6,structure(sequence),
5713,452,5,"['sequence', 'integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,6,structure(sequence),
5038,329,6,"['tensor', 'dimension']",There should be one index letter per tensor dimension.,torch.einsum.yaml,2,not useful,
5700,450,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2,not useful,
5701,450,5,"['func', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2,not useful,
5702,450,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2,not useful,
5703,450,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2,not useful,
5816,473,5,"['one', 'dimension']",There should be one index letter per tensor dimension.,torch.einsum.yaml,2,not useful,
5705,451,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.hvp.yaml,2,Can't handle,
5706,451,5,"['must', 'size']",Must be the same size as the output of `func`.,torch.autograd.functional.vjp.yaml,2,Can't handle,
5707,451,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.jvp.yaml,2,Can't handle,
5708,451,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.vhp.yaml,2,Can't handle,
3463,135,12,"['set', 'false']",This flag should be set to `False` when this redirect causes issues.,torch.utils.cpp_extension.load_inline.yaml,2,not useful,
5714,452,5,"['sequence', 'integers', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,6,structure(sequence),
5645,439,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4,not useful,
5646,439,5,"['set', 'corner', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5647,439,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4,not useful,
5648,439,5,"['set', 'corner', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4,not useful,
5649,439,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4,not useful,
5640,438,5,"['set', 'input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
5641,438,5,"['set', 'input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3,not useful,
5642,438,5,"['set', 'input', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,3,not useful,
5643,438,5,"['set', 'input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3,not useful,
5720,454,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,2,not useful,
5721,454,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,2,not useful,
5722,454,5,"['used', 'used']","This option parallels the `align_corners` option in `interpolate()`, and so whichever option is used here should also be used there to resize the input image before grid sampling.",torch.nn.functional.grid_sample.yaml,2,not useful,
5723,454,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,2,not useful,
5644,438,5,"['set', 'input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3,not useful,
5655,441,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout.yaml,2,not useful,
5656,441,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout2d.yaml,2,not useful,
5657,441,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout3d.yaml,2,not useful,
5658,441,5,"['set', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5659,441,5,"['set', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
5650,440,5,"['set', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2,not useful,
5651,440,5,"['set', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2,not useful,
5652,440,5,"['set', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5653,440,5,"['set', 'value']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2,not useful,
5654,440,5,"['set', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2,not useful,
5660,442,5,"['shape', 'n']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
5661,442,5,"['shape', 'n']","input of shape (N, C, H_in, W_in) (4-D case) or (N, C, D_in, H_in, W_in) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
5662,442,5,"['shape', 'n']",the square matrix of shape (n times n) for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2,shape,
5664,442,5,"['shape', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
5663,442,5,"['shape', 'n']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
5667,443,5,"['shape', 'times']",input tensor of shape B times P times M .,torch.cdist.yaml,2,shape,
5668,443,5,"['shape', 'times']",input tensor of shape B times R times M .,torch.cdist.yaml,2,shape,
5665,443,5,"['shape', 'times']",the square matrix of shape (n times n) for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2,shape,
5666,443,5,"['shape', 'times']",input tensor of shape N times M .,torch.nn.functional.pdist.yaml,2,shape,
5669,443,5,"['shape', 'times']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
5546,419,5,"['single', 'SOME_VALUE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2,"dtype(int)+ndim(0,1)+structure",
5715,453,5,"['SOME_VALUE-d', 'tensor']",a sequence of 2 or more 2-D tensors whose product is to be determined.,torch.chain_matmul.yaml,2,ndim+dtype,
5716,453,5,"['SOME_VALUE-d', 'tensor']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2,ndim+dtype,
5717,453,5,"['SOME_VALUE-d', 'tensor']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2,ndim,
5718,453,5,"['SOME_VALUE-d', 'tensor']",1-d int tensor,torch.bincount.yaml,2,ndim,
5719,453,5,"['SOME_VALUE-d', 'tensor']",the input 2-D tensor,torch.matrix_rank.yaml,2,ndim,
5450,400,5,"['SOME_VALUE', 'n']","flow-field of shape (N, H_out, W_out, 2) (4-D case) or (N, D_out, H_out, W_out, 3) (5-D case)",torch.nn.functional.grid_sample.yaml,2,shape,
5451,400,5,"['SOME_VALUE', 'n']",input batch of affine matrices with shape (N times 2 times 3 ) for 2D or (N times 3 times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2,shape,
5452,400,5,"['SOME_VALUE', 'n']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2,shape*2,
5453,400,5,"['SOME_VALUE', 'n']","(N) where each value is 0 <= targets[i] <= C-1 , or (N, d_1, d_2, ..., d_K) where K >= 1 for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2,shape*2,
5535,417,5,"['target', 'output', 'size', 'single', 'integer', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,6,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
5536,417,5,"['target', 'output', 'size', 'single', 'integer', 'tuple']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,6,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
5537,417,5,"['target', 'output', 'size', 'single', 'integer', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,6,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
5538,417,5,"['target', 'output', 'size', 'single', 'integer', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,6,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
5539,417,5,"['target', 'output', 'size', 'single', 'integer', 'tuple']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,6,"dtype(int) + structure(tuple) + ndim(0,1) + shape([3])",
5505,411,5,"['tensor', 'compute']",the tensor to compute OR with,torch.logical_or.yaml,2,tensor,^(a|the)\s*tensor
5506,411,5,"['tensor', 'compute']",the tensor to compute AND with,torch.logical_and.yaml,2,tensor,^(a|the)\s*tensor
5507,411,5,"['tensor', 'compute']",the tensor to compute the multivariate log-gamma function,torch.mvlgamma.yaml,2,tensor,^(a|the)\s*tensor
5508,411,5,"['tensor', 'compute']",the tensor to compute XOR with,torch.logical_xor.yaml,2,tensor,^(a|the)\s*tensor
5509,411,5,"['tensor', 'compute']",the tensor to compute the digamma function on,torch.digamma.yaml,2,tensor,^(a|the)\s*tensor
5500,410,5,"['tensor', 'data']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2,list,
5502,410,5,"['tensor', 'data']",Tensor to fill with received data.,torch.distributed.recv.yaml,2,tensor,^(a|the)\s*tensor
5503,410,5,"['tensor', 'data']",Tensor to fill with received data.,torch.distributed.irecv.yaml,2,tensor,^(a|the)\s*tensor
5498,409,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2,tuple+ndim(1)+shape([2]),
5496,409,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2,tuple+ndim(1)+shape([2]),
5497,409,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2,tuple+ndim(1)+shape([2]),
5415,393,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation",torch.nn.functional.avg_pool3d.yaml,2,dtype(bool),
5416,393,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype(bool),
5417,393,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool1d.yaml,2,dtype(bool),
5418,393,5,"['true', 'include']","If True, includes CUDA-specific include paths.",torch.utils.cpp_extension.include_paths.yaml,2,bool,
5419,393,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool2d.yaml,2,dtype(bool),
5402,390,5,"['true', 'otherwise']","When True (nonzero), yield x, otherwise yield y",torch.where.yaml,2,dtype(bool),
5404,390,5,"['true', 'otherwise']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2,bool,
5697,449,5,"['function', 'function']","To do this, each function `foo` is called via an intermediary `_safe_foo` function.",torch.utils.cpp_extension.load_inline.yaml,2,not useful,
5405,391,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,3,dtype(bool),if `true/false`
5406,391,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,3,dtype(bool),if `true/false`
5407,391,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,3,dtype(bool),if `true/false`
5408,391,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,3,dtype(bool),if `true/false`
5420,394,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.hann_window.yaml,2,bool,
5421,394,5,"['true', 'returns']","if set to `True`, returns an info IntTensor.",torch.lu.yaml,2,dtype(bool),
5785,467,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input as squares rather than points.",torch.nn.functional.grid_sample.yaml,2,not useful,
5786,467,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2,not useful,
5422,394,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.hamming_window.yaml,2,bool,
5788,467,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2,not useful,
5423,394,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.blackman_window.yaml,2,bool,
5790,468,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5791,468,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5792,468,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5424,394,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.bartlett_window.yaml,2,bool,
5410,392,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2,dtype(bool),
5411,392,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype(bool),
3846,171,9,"['SOME_VALUE', 'SOME_VALUE', 'SOME_VALUE']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,3,not useful,
5797,469,5,"['=', '=']",Ignored if `dim` = `None` and `out` = `None`.,torch.norm.yaml,2,not useful,
5798,469,5,"['=', '=']",Ignored if `dim` = `None` and `out` = `None`.,torch.norm.yaml,2,not useful,
5412,392,5,"['true', 'use']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2,dtype(bool),
5413,392,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2,dtype(bool),
5553,420,5,"['tuple', 'tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,3,tuple+ndim(1)+shape([2]),
5554,420,5,"['tuple', 'tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,3,tuple+ndim(1)+shape([2]),
5550,420,5,"['tuple', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3,dtype(callable),
5804,470,5,"['=', 'n']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2,not useful,
5805,471,5,"['-', 'SOME_VALUE']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
4428,239,8,"['=', 'SOME_VALUE']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,2,not useful,
5807,471,5,"['-', 'SOME_VALUE']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5808,471,5,"['-', 'SOME_VALUE']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5809,471,5,"['-', 'SOME_VALUE']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5810,472,5,"['-', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5796,469,5,"['=', '=']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,2,not useful,
5812,472,5,"['-', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5813,472,5,"['-', '-']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5551,420,5,"['tuple', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3,dtype(callable),
5815,473,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
3815,167,10,"['inputs', 'function']",tuple containing inputs to the `function`,torch.utils.checkpoint.checkpoint.yaml,2,not useful,
5817,473,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5818,473,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5806,471,5,"['-', 'SOME_VALUE']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,2,not useful,
5820,474,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2,not useful,
5552,420,5,"['tuple', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3,dtype(callable),
5613,432,5,"['type', 'specified']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2,dtype,^dtype
5823,474,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2,not useful,
5824,474,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2,not useful,
5610,432,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2,dtype,the desired <dtype> of
5611,432,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2,dtype,the desired <dtype> of
5612,432,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2,dtype,the desired <dtype> of
5614,432,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2,dtype,the desired <dtype> of
5765,463,5,"['upper', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2,dtype(bool),
5830,476,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diag_embed.yaml,2,not useful,
5831,476,5,"['main', 'diagonal']",diagonal offset from the main diagonal.,torch.triu_indices.yaml,2,not useful,
5832,476,5,"['main', 'diagonal']",diagonal offset from the main diagonal.,torch.tril_indices.yaml,2,not useful,
5833,476,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diagonal.yaml,2,not useful,
5834,476,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diagflat.yaml,2,not useful,
5766,463,5,"['upper', 'triangular']","input matrix u of size (*, m, m) , where * is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2,shape,
2185,52,22,"['process', 'group']",URL specifying how to initialize the process group.,torch.distributed.init_process_group.yaml,2,not useful,
5767,463,5,"['upper', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,2,dtype(bool),
5768,463,5,"['upper', 'triangular']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2,ndim,
5769,463,5,"['upper', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2,dtype(bool),
5840,478,5,"['original', 'module']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,2,not useful,
5841,478,5,"['original', 'module']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,2,not useful,
5533,416,5,"['use', 'instead']",use operator_export_type] export the internal IR directly instead of converting it to ONNX ops.,torch.onnx.export.yaml,2,not useful,
5843,478,5,"['original', 'module']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,2,not useful,
5844,478,5,"['original', 'module']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,2,not useful,
5530,416,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2,dtype(bool),
5531,416,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2,dtype(bool),
5532,416,5,"['use', 'instead']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2,dtype(bool),
5534,416,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2,dtype(bool),
5724,454,5,"['used', 'used']",module used for unpickling metadata and objects (has to match the `pickle_module` used to serialize file),torch.load.yaml,2,dtype(module),
5635,437,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2,structure,tuple of 
5638,437,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2,structure,tuple of 
5639,437,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2,structure,tuple of 
5520,414,5,"['whether', 'elements']",controls whether to return largest or smallest elements,torch.topk.yaml,2,dtype(bool),
5521,414,5,"['whether', 'elements']",controls whether to return the elements in sorted order,torch.topk.yaml,2,dtype(bool),
5522,414,5,"['whether', 'elements']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2,dtype(bool),
5523,414,5,"['whether', 'elements']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2,dtype(bool),
5524,414,5,"['whether', 'elements']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2,dtype(bool),
5515,413,5,"['whether', 'input']","controls whether `input` was halfed to avoid redundancy, e.g., by `rfft()`.",torch.irfft.yaml,2,dtype(bool),
5516,413,5,"['whether', 'input']",whether to pad `input` on both sides so that the t -th frame is centered at time t times hop _length .,torch.stft.yaml,2,dtype(bool),
5517,413,5,"['whether', 'input']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2,dtype(bool),
5518,413,5,"['whether', 'input']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2,dtype(bool),
5519,413,5,"['whether', 'input']",indicates whether `input` is symmetric.,torch.matrix_rank.yaml,2,dtype(bool),
5510,412,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool3d.yaml,3,dtype(bool),
5511,412,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool1d.yaml,3,dtype(bool),
5512,412,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool2d.yaml,3,dtype(bool),
5811,472,5,"['-', '-']",use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate euclidean distance (p = 2) 'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.,torch.cdist.yaml,2,not useful,
5513,412,5,"['whether', 'return', 'indices']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,3,dtype(bool),
5514,412,5,"['whether', 'return', 'indices']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,3,dtype(bool),
5869,483,5,"['+', 'SOME_VALUE']",Default: `False` target * log(target) - target + 0.5 * log(2 * pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2,not useful,
5525,415,5,"['whether', 'unique']",Whether to also return the counts for each unique element.,torch.unique.yaml,2,dtype(bool),
5526,415,5,"['whether', 'unique']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2,dtype(bool),
5527,415,5,"['whether', 'unique']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2,dtype(bool),
5528,415,5,"['whether', 'unique']",Whether to also return the counts for each unique element.,torch.unique_consecutive.yaml,2,dtype(bool),
5529,415,5,"['whether', 'unique']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2,dtype(bool),
5836,477,5,"['window', 'function']",the optional window function.,torch.stft.yaml,2,dtype,the optional <> <>
5835,477,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.hann_window.yaml,2,dtype,
5837,477,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.hamming_window.yaml,2,dtype,
5838,477,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.blackman_window.yaml,2,dtype,
5839,477,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.bartlett_window.yaml,2,dtype,
5880,486,5,"['flattened', 'input']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2,not useful,
5881,486,5,"['flattened', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2,not useful,
5882,486,5,"['flattened', 'input']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2,not useful,
5883,486,5,"['flattened', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2,not useful,
5884,486,5,"['flattened', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2,not useful,