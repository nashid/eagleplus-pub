API,Arg,Descp,Normalized_descp,dtype,structure,shape,ndim,range,enum
torch.linspace,steps,number of points to sample between `start` and `end`. Default: `100`.,number of points to sample between PARAM and PARAM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.linspace,steps,number of points to sample between `start` and `end`. Default: `100`.,Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.linspace,steps,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.linspace,steps,DF: 100,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.adaptive_avg_pool1d,output_size,the target output size (single integer),the target output size BSTR,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,QSTR
torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,QSTR
torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,QSTR
torch.nn.TripletMarginLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,QSTR
torch.nn.TripletMarginLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,QSTR
torch.nn.TripletMarginLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,QSTR
torch.clamp,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.clamp,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.addcdiv,input,the tensor to be added,the D_STRUCTURE to be added,,D_STRUCTURE,,,,
torch.addcdiv,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_and,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_and,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.logical_and,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.jit.save,_extra_files,Map from filename to contents which will be stored as part of 'f'.,Map from filename to contents which will be stored as part of QSTR,D_TYPE,,,,
torch.jit.save,_extra_files,DF: ExtraFilesMap{},DEFAULT DF_STR,D_TYPE,,,,
torch.sum,dim,the dimension or dimensions to reduce.,the dimension or dimensions to reduce,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.sum,dim,DD: int or tuple of python:ints,D_TYPE or D_STRUCTURE of python D_TYPE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.numel,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.numel,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.is_tensor,obj,Object to test,Object to test,,,,,
torch.is_tensor,obj,DD: Object,ONE_WORD Object,,,,,
torch.nn.utils.spectral_norm,name,name of weight parameter,name of weight parameter,D_TYPE,,,,
torch.nn.utils.spectral_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,
torch.nn.utils.spectral_norm,name,DF: weight,DEFAULT DF_STR,D_TYPE,,,,
torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",select LOBPCG method,D_TYPE,,,,
torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",See the description of the function above,D_TYPE,,,,
torch.lobpcg,method,"select LOBPCG method. See the description of the function above. Default is ""ortho"".",Default is QSTR,D_TYPE,,,,
torch.lobpcg,method,"DD: str, optional",D_TYPE optional,D_TYPE,,,,
torch.lobpcg,method,DF: None,DEFAULT None,D_TYPE,,,,
torch.round,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.round,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.round,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.CTCLoss,zero_infinity,Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,Whether to zero infinite losses and the associated gradients,D_TYPE,,CONSTANT_VAL,,
torch.nn.CTCLoss,zero_infinity,Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,Default CONSTANT_BOOL Infinite losses mainly occur when the inputs are too D_TYPE to be aligned to the targets,D_TYPE,,CONSTANT_VAL,,
torch.nn.CTCLoss,zero_infinity,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.CTCLoss,zero_infinity,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",quantity of parameters to prune,D_TYPE,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE should be between CONSTANT_FLOAT and CONSTANT_FLOAT and represent the fraction of parameters to prune,D_TYPE,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_unstructured,amount,"quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.",If D_TYPE it represents the absolute number of parameters to prune,D_TYPE,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.nn.utils.prune.random_unstructured,amount,DD: int or float,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,"int:[0,inf);torch.float32:[0,1]",
torch.rand,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,
torch.rand,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,
torch.rand,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,
torch.rand,layout,DF: torch.strided,torch strided,,,,,
torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",Specifies a PARAM value that is ignored and does not contribute to the PARAM gradient,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",When PARAM is CONSTANT_BOOL the loss is averaged over non ignored targets,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.nll_loss,ignore_index,"Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100",Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.nll_loss,ignore_index,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.nll_loss,ignore_index,DF: -100,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.addr,beta,multiplier for `input` ( beta ),multiplier for PARAM BSTR,D_TYPE,,CONSTANT_VAL,,
torch.addr,beta,"DD: Number, optional",Number optional,D_TYPE,,CONSTANT_VAL,,
torch.addr,beta,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,input,"input tensor of shape (minibatch , in _channels , iW) ",input D_STRUCTURE of shape BSTR,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.multinomial,generator,a pseudorandom number generator for sampling,a pseudorandom number generator for sampling,,,,,
torch.multinomial,generator,"DD: `torch.Generator`, optional",torch Generator optional,,,,,
torch.multinomial,generator,DF: None,DEFAULT None,,,,,
torch.distributed.send,dst,Destination rank.,Destination rank,D_TYPE,,CONSTANT_VAL,,
torch.distributed.send,dst,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.nn.RReLU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,D_TYPE,,CONSTANT_VAL,,
torch.nn.RReLU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.RReLU,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.masked_select,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.masked_select,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.InstanceNorm1d,momentum,the value used for the running_mean and running_var computation. Default: 0.1,the value used for the running_mean and running_var computation,D_TYPE,,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,momentum,the value used for the running_mean and running_var computation. Default: 0.1,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.InstanceNorm1d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.cuda.set_rng_state,new_state,The desired state,The desired state,D_TYPE,,,,
torch.cuda.set_rng_state,new_state,DD: torch.ByteTensor,ONE_WORD D_TYPE,D_TYPE,,,,
torch.symeig,out,"the output tuple of (Tensor, Tensor)",the output D_STRUCTURE of BSTR,,D_STRUCTURE,BSTR,,,
torch.symeig,out,"DD: tuple, optional",D_STRUCTURE optional,,D_STRUCTURE,BSTR,,,
torch.symeig,out,DF: None,DEFAULT None,,D_STRUCTURE,BSTR,,,
torch.floor_divide,other,the denominator,the denominator,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.floor_divide,other,DD: Tensor or Scalar,D_STRUCTURE or Scalar,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.addmm,mat1,the first matrix to be multiplied,the first matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,
torch.addmm,mat1,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,
torch.eye,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,D_TYPE,,CONSTANT_VAL,,
torch.eye,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.eye,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.eye,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,input,"input tensor of shape (minibatch , in _channels , iH , iW) ",input D_STRUCTURE of shape BSTR,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.MaxUnpool2d,padding,Padding that was added to the input,Padding that was added to the input,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.MaxUnpool2d,padding,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.MaxUnpool2d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the jacobian for said PARAM which is the expected mathematical value,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.jacobian,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.triu,diagonal,the diagonal to consider,the diagonal to consider,D_TYPE,,CONSTANT_VAL,,
torch.triu,diagonal,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.triu,diagonal,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,
torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,
torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,
torch.nn.MultiLabelMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,
torch.nn.MultiLabelMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,,,,,
torch.nn.MultiLabelMarginLoss,reduce,DF: None,DEFAULT None,,,,,
torch.sigmoid,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.sigmoid,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.ConvTranspose2d,groups,Number of blocked connections from input channels to output channels. Default: 1,Number of blocked connections from input channels to output channels,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose2d,groups,Number of blocked connections from input channels to output channels. Default: 1,Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose2d,groups,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.ConvTranspose2d,groups,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.triu_indices,row,number of rows in the 2-D matrix.,number of rows in the CONSTANT_NUM D matrix,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.triu_indices,row,DD: `int`,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.distributed.get_world_size,group,The process group to work on,The process group to work on,,,,,
torch.distributed.get_world_size,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,
torch.distributed.get_world_size,group,DF: <objectobject>,DEFAULT REXPR,,,,,
torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,QSTR
torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,QSTR
torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,QSTR
torch.nn.CrossEntropyLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,QSTR
torch.nn.CrossEntropyLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,QSTR
torch.nn.CrossEntropyLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,QSTR
torch.nn.utils.prune.remove,name,parameter name within `module` on which pruning will act.,parameter name within PARAM on which pruning will act,D_TYPE,,CONSTANT_VAL,,
torch.nn.utils.prune.remove,name,DD: str,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.pca_lowrank,niter,"the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.",the number of subspace iterations to conduct niter must be a nonnegative D_TYPE and defaults to CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.pca_lowrank,niter,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.pca_lowrank,niter,DF: 2,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,Specifies the reduction to apply to the output QSTR QSTR QSTR QSTR,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR no reduction will be applied,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the sum of the output will be divided by batchsize,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the output will be summed,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,QSTR the output will be divided by the number of elements in the output,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`,Default QSTR,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,QSTR
torch.nn.KLDivLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,QSTR
torch.empty_like,device,"the desired device of returned tensor. Default: if `None`, defaults to the device of `input`.",the desired device of returned D_STRUCTURE,,,,,
torch.empty_like,device,"the desired device of returned tensor. Default: if `None`, defaults to the device of `input`.",Default if QSTR defaults to the device of PARAM,,,,,
torch.empty_like,device,"DD: `torch.device`, optional",torch device optional,,,,,
torch.empty_like,device,DF: None,DEFAULT None,,,,,
torch.distributions.kl.kl_divergence,p,A `Distribution` object.,A QSTR object,,,,,
torch.distributions.kl.kl_divergence,p,DD: Distribution,ONE_WORD Distribution,,,,,
torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,
torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,
torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,
torch.nn.BCELoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,
torch.nn.BCELoss,reduce,"DD: bool, optional",D_TYPE optional,,,,,
torch.nn.BCELoss,reduce,DF: None,DEFAULT None,,,,,
torch.utils.cpp_extension.load,extra_cflags,optional list of compiler flags to forward to the build.,optional D_STRUCTURE of compiler flags to forward to the build,,,,,
torch.utils.cpp_extension.load,extra_cflags,DF: None,DEFAULT None,,,,,
torch.narrow,start,the starting dimension,the starting dimension,D_TYPE,,,,
torch.narrow,start,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,
torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,the value used for the running_mean and running_var computation,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,Can be set to QSTR for cumulative moving average i e,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,simple average,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,momentum,the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,momentum,DF: 0.1,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.logical_and,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_and,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.MaxUnpool1d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,Stride of the max pooling window,D_TYPE,D_STRUCTURE,,,,
torch.nn.MaxUnpool1d,stride,Stride of the max pooling window. It is set to `kernel_size` by default.,It is set to PARAM by default,D_TYPE,D_STRUCTURE,,,,
torch.nn.MaxUnpool1d,stride,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,
torch.nn.MaxUnpool1d,stride,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,
torch.hub.download_url_to_file,url,URL of the object to download,URL of the object to download,D_TYPE,,,,
torch.hub.download_url_to_file,url,DD: string,ONE_WORD D_TYPE,D_TYPE,,,,
torch.distributed.all_gather,async_op,Whether this op should be an async op,Whether this op should be an async op,D_TYPE,,CONSTANT_VAL,,
torch.distributed.all_gather,async_op,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.distributed.all_gather,async_op,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,value used as an exponent to compute sizes of the clusters. Default: 4.0,value used as an exponent to compute sizes of the clusters,D_TYPE,,CONSTANT_VAL,,
torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,value used as an exponent to compute sizes of the clusters. Default: 4.0,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,"DD: float, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.AdaptiveLogSoftmaxWithLoss,div_value,DF: 4.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.mode,out,"the result tuple of two output tensors (values, indices)",the result D_STRUCTURE of two output D_STRUCTURE BSTR,,D_STRUCTURE,,,,
torch.mode,out,"DD: tuple, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.mode,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.mm,input,the first matrix to be multiplied,the first matrix to be multiplied,D_TYPE,D_STRUCTURE,,,,
torch.mm,input,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,
torch.ne,other,the tensor or value to compare,the D_STRUCTURE or value to compare,D_TYPE,D_STRUCTURE,,,,
torch.ne,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,D_STRUCTURE,,,,
torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",the size of the padding,D_TYPE,D_STRUCTURE,,,,
torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If is D_TYPE uses the same padding in all boundaries,D_TYPE,D_STRUCTURE,,,,
torch.nn.ConstantPad3d,padding,"the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )",If a CONSTANT_NUM D_STRUCTURE uses BSTR,D_TYPE,D_STRUCTURE,,,,
torch.nn.ConstantPad3d,padding,"DD: int, tuple",D_TYPE D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,
torch.lu,get_infos,"if set to `True`, returns an info IntTensor. Default: `False`",if set to CONSTANT_BOOL returns an info IntTensor,D_TYPE,,CONSTANT_VAL,,
torch.lu,get_infos,"if set to `True`, returns an info IntTensor. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.lu,get_infos,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.lu,get_infos,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.embedding,padding_idx,"If given, pads the output with the embedding vector at `padding_idx` (initialized to zeros) whenever it encounters the index.",If given pads the output with the embedding vector at QSTR BSTR whenever it encounters the index,D_TYPE,,,,
torch.nn.functional.embedding,padding_idx,"DD: int, optional",D_TYPE optional,D_TYPE,,,,
torch.nn.functional.embedding,padding_idx,DF: None,DEFAULT None,D_TYPE,,,,
torch.gt,out,the output tensor that must be a BoolTensor,the output D_STRUCTURE that must be a D_TYPE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.gt,out,"DD: Tensor, optional",D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.gt,out,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,batch_first,"if `True`, the input is expected in `B x T x *` format.",if CONSTANT_BOOL the PARAM is expected in B x T x format,D_TYPE,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,batch_first,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.utils.rnn.pack_padded_sequence,batch_first,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.Conv1d,kernel_size,Size of the convolving kernel,Size of the convolving kernel,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.nn.Conv1d,kernel_size,DD: int or tuple,D_TYPE or D_STRUCTURE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,"[0,inf)",
torch.cuda.manual_seed,seed,The desired seed.,The desired seed,D_TYPE,,,,
torch.cuda.manual_seed,seed,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,
torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Deprecated BSTR,,,,,
torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",By default the losses are averaged or summed over observations for each minibatch depending on PARAM,,,,,
torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",When QSTR is CONSTANT_BOOL returns a loss per batch element instead and ignores PARAM,,,,,
torch.nn.TripletMarginLoss,reduce,"Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`",Default CONSTANT_BOOL,,,,,
torch.nn.TripletMarginLoss,reduce,"DD: bool, optional",D_TYPE optional,,,,,
torch.nn.TripletMarginLoss,reduce,DF: None,DEFAULT None,,,,,
torch.nn.LSTM,input_size,The number of expected features in the input x,The number of expected features in the input x,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.LSTM,input_size,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.rand_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",the desired data type of returned D_STRUCTURE,D_TYPE,,CONSTANT_VAL,,
torch.rand_like,dtype,"the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.",Default if QSTR defaults to the dtype of PARAM,D_TYPE,,CONSTANT_VAL,,
torch.rand_like,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.rand_like,dtype,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,,
torch.hub.load_state_dict_from_url,progress,whether or not to display a progress bar to stderr. Default: True,whether or not to display a progress bar to stderr,D_TYPE,,CONSTANT_VAL,,
torch.hub.load_state_dict_from_url,progress,whether or not to display a progress bar to stderr. Default: True,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.hub.load_state_dict_from_url,progress,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.hub.load_state_dict_from_url,progress,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.as_strided,storage_offset,the offset in the underlying storage of the output tensor,the offset in the underlying storage of the output D_STRUCTURE,D_TYPE,,CONSTANT_VAL,,
torch.as_strided,storage_offset,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.as_strided,storage_offset,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.AvgPool1d,kernel_size,the size of the window,the size of the window,D_TYPE,,,"[0,inf)",
torch.nn.BatchNorm3d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module has learnable affine parameters,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm3d,affine,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.utils.cpp_extension.load,sources,A list of relative or absolute paths to C++ source files.,A D_STRUCTURE of relative or absolute paths to C source files,,D_STRUCTURE,,,,
torch.hamming_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL returns a window to be used as periodic function,D_TYPE,,CONSTANT_VAL,,
torch.hamming_window,periodic,"If True, returns a window to be used as periodic function. If False, return a symmetric window.",If CONSTANT_BOOL return a symmetric window,D_TYPE,,CONSTANT_VAL,,
torch.hamming_window,periodic,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.hamming_window,periodic,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.histc,min,lower end of the range (inclusive),lower end of the range BSTR,D_TYPE,,CONSTANT_VAL,,
torch.histc,min,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.histc,min,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.abs,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.abs,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.utils.weight_norm,name,name of weight parameter,name of weight parameter,D_TYPE,,,,
torch.nn.utils.weight_norm,name,"DD: str, optional",D_TYPE optional,D_TYPE,,,,
torch.nn.utils.weight_norm,name,DF: weight,DEFAULT DF_STR,D_TYPE,,,,
torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL both the output and result will be computed in a differentiable way,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vjp,create_graph,"If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vjp,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vjp,create_graph,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",the desired device of returned D_STRUCTURE,,,,,
torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",Default if QSTR uses the current device for the default D_STRUCTURE type see torch set_default_tensor_type,,,,,
torch.ones,device,"the desired device of returned tensor. Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`). `device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.",QSTR will be the CPU for CPU D_STRUCTURE types and the current CUDA device for CUDA D_STRUCTURE types,,,,,
torch.ones,device,"DD: `torch.device`, optional",torch device optional,,,,,
torch.ones,device,DF: None,DEFAULT None,,,,,
torch.remainder,input,the dividend,the dividend,,D_STRUCTURE,,,,
torch.remainder,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.square,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.square,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.sum,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,CONSTANT_VAL,,
torch.sum,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.sum,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.MaxPool2d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later",if CONSTANT_BOOL will return the max indices along with the outputs,D_TYPE,,CONSTANT_VAL,,
torch.nn.MaxPool2d,return_indices,"if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later",Useful for torch nn MaxUnpool2d later,D_TYPE,,CONSTANT_VAL,,
torch.nn.MaxPool2d,return_indices,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.logsumexp,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.logsumexp,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.logsumexp,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.einsum,*operands,The operands to compute the Einstein sum of.,The operands to compute the Einstein sum of,,D_STRUCTURE,,,,
torch.einsum,*operands,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.norm,input,the input tensor,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.norm,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.addcdiv,value,multiplier for tensor1 / tensor2 ,multiplier for PARAM PARAM,D_TYPE,,CONSTANT_VAL,,
torch.addcdiv,value,"DD: Number, optional",Number optional,D_TYPE,,CONSTANT_VAL,,
torch.addcdiv,value,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.normal,size,a sequence of integers defining the shape of the output tensor.,a D_STRUCTURE of D_TYPE defining the shape of the output D_STRUCTURE,D_TYPE,D_STRUCTURE,,,"[0,inf)",
torch.normal,size,DD: int...,ONE_WORD D_TYPE,D_TYPE,D_STRUCTURE,,,"[0,inf)",
torch.tensordot,a,Left tensor to contract,Left D_STRUCTURE to contract,,D_STRUCTURE,,,,
torch.tensordot,a,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.quantization.propagate_qconfig_,qconfig_dict,"dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",D_STRUCTURE that maps from name or type of submodule to quantization configuration qconfig applies to all submodules of a given PARAM unless qconfig for the submodules are specified BSTR,,D_STRUCTURE,,,,
torch.quantization.propagate_qconfig_,qconfig_dict,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.utils.prune.global_unstructured,pruning_method,"a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has `PRUNING_TYPE='unstructured'`.",a valid pruning function from this module or a custom one implemented by the user that satisfies the implementation guidelines and has PRUNING_TYPE QSTR,,,,,
torch.nn.utils.prune.global_unstructured,pruning_method,DD: function,ONE_WORD function,,,,,
torch.nn.init.uniform_,a,the lower bound of the uniform distribution,the lower bound of the uniform distribution,D_TYPE,,CONSTANT_VAL,,
torch.nn.init.uniform_,a,DF: 0.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.neg,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.neg,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.div,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.div,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.ifft,input,the input tensor of at least `signal_ndim` `+ 1` dimensions,the input D_STRUCTURE of at least PARAM CONSTANT_NUM dimensions,,D_STRUCTURE,,CONSTANT_VAL,,
torch.ifft,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",the spacing between kernel elements,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,dilation,"the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1",Default CONSTANT_NUM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.conv_transpose2d,dilation,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.argsort,descending,controls the sorting order (ascending or descending),controls the sorting order BSTR,D_TYPE,,CONSTANT_VAL,,
torch.argsort,descending,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.argsort,descending,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.MaxPool2d,stride,the stride of the window. Default value is `kernel_size`,the stride of the window,,,,,
torch.nn.MaxPool2d,stride,the stride of the window. Default value is `kernel_size`,Default value is PARAM,,,,,
torch.nn.MaxPool2d,stride,DF: None,DEFAULT None,,,,,
torch.sparse_coo_tensor,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,If autograd should record operations on the returned D_STRUCTURE,D_TYPE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,requires_grad,If autograd should record operations on the returned tensor. Default: `False`.,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,requires_grad,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,requires_grad,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.log1p,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.log1p,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.log1p,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the vhp for said PARAM which is the expected mathematical value,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vhp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vhp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.vhp,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.conv_transpose1d,weight,"filters of shape (in _channels ,  out _channels/groups , kW) ",filters of shape BSTR,,BSTR,CONSTANT_VAL,,
torch.tensordot,b,Right tensor to contract,Right D_STRUCTURE to contract,,D_STRUCTURE,,,,
torch.tensordot,b,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.rand_like,input,the size of `input` will determine size of the output tensor.,the size of QSTR will determine size of the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.rand_like,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.kthvalue,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,CONSTANT_VAL,,
torch.kthvalue,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.kthvalue,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.can_cast,to,The target `torch.dtype`.,The target D_TYPE,,,,,
torch.can_cast,to,DD: dpython:type,dpython type,,,,,
torch.repeat_interleave,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.repeat_interleave,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",Geometrically we consider the pixels of the PARAM and output as squares rather than points,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the center points of their corner pixels preserving the values at the corner pixels,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",If set to CONSTANT_BOOL the PARAM and output D_STRUCTURE are aligned by the corner points of their corner pixels and the interpolation uses edge value padding for out of boundary values making this operation independent of PARAM PARAM when PARAM is kept the same,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",This only has an effect when PARAM is QSTR,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,"Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.interpolate,align_corners,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,,
torch.min,keepdim,whether the output tensor has `dim` retained or not.,whether the output D_STRUCTURE has PARAM retained or not,D_TYPE,,CONSTANT_VAL,,
torch.min,keepdim,DD: bool,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.min,keepdim,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.adaptive_max_pool3d,return_indices,whether to return pooling indices. Default: `False`,whether to return pooling indices,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.adaptive_max_pool3d,return_indices,whether to return pooling indices. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.adaptive_max_pool3d,return_indices,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,,
torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Specifies the reduction to apply to the output QSTR QSTR QSTR,D_TYPE,,,,QSTR
torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",QSTR no reduction will be applied QSTR the sum of the output will be divided by the number of elements in the output QSTR the output will be summed,D_TYPE,,,,QSTR
torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Note PARAM and PARAM are in the process of being deprecated and in the meantime specifying either of those two args will override QSTR,D_TYPE,,,,QSTR
torch.nn.NLLLoss,reduction,"Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`",Default QSTR,D_TYPE,,,,QSTR
torch.nn.NLLLoss,reduction,"DD: string, optional",D_TYPE optional,D_TYPE,,,,QSTR
torch.nn.NLLLoss,reduction,DF: mean,DEFAULT DF_STR,D_TYPE,,,,QSTR
torch.utils.cpp_extension.load,is_python_module,"If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library.",If CONSTANT_BOOL BSTR imports the produced shared library as a Python module,D_TYPE,,CONSTANT_VAL,,
torch.utils.cpp_extension.load,is_python_module,"If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library.",If CONSTANT_BOOL loads it into the process as a plain dynamic library,D_TYPE,,CONSTANT_VAL,,
torch.utils.cpp_extension.load,is_python_module,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.distributed.broadcast,src,Source rank.,Source rank,D_TYPE,,CONSTANT_VAL,,
torch.distributed.broadcast,src,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.linspace,layout,the desired layout of returned Tensor. Default: `torch.strided`.,the desired layout of returned D_STRUCTURE,,,,,
torch.linspace,layout,the desired layout of returned Tensor. Default: `torch.strided`.,Default torch strided,,,,,
torch.linspace,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,
torch.linspace,layout,DF: torch.strided,torch strided,,,,,
torch.unsqueeze,dim,the index at which to insert the singleton dimension,the index at which to insert the singleton dimension,D_TYPE,,,,
torch.unsqueeze,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,,,
torch.clamp,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.clamp,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.clamp,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.quantized.functional.conv3d,scale,quantization scale for the output. Default: 1.0,quantization scale for the output,D_TYPE,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,scale,quantization scale for the output. Default: 1.0,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.quantized.functional.conv3d,scale,DF: 1.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,alpha,multiplicative factor. Default: 0.0001,multiplicative factor,D_TYPE,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,alpha,multiplicative factor. Default: 0.0001,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.LocalResponseNorm,alpha,DF: 0.0001,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape. Default: `False`",when CONSTANT_BOOL will use ceil instead of floor to compute the output shape,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,ceil_mode,"when True, will use ceil instead of floor to compute the output shape. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,ceil_mode,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.RNN,input_size,The number of expected features in the input x,The number of expected features in the input x,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.RNN,input_size,DF: None,DEFAULT None,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.nn.functional.dropout3d,inplace,"If set to `True`, will do this operation in-place. Default: `False`",If set to CONSTANT_BOOL will do this operation in place,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.dropout3d,inplace,"If set to `True`, will do this operation in-place. Default: `False`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.dropout3d,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,full,"whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .",whether to compute full loss i e to add the Stirling approximation term,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,full,"whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .",Default CONSTANT_BOOL PARAM log BSTR,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.poisson_nll_loss,full,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.empty_like,layout,"the desired layout of returned tensor. Default: if `None`, defaults to the layout of `input`.",the desired layout of returned D_STRUCTURE,,,,,
torch.empty_like,layout,"the desired layout of returned tensor. Default: if `None`, defaults to the layout of `input`.",Default if QSTR defaults to the layout of PARAM,,,,,
torch.empty_like,layout,"DD: `torch.layout`, optional",torch layout optional,,,,,
torch.empty_like,layout,DF: None,DEFAULT None,,,,,
torch.narrow,input,the tensor to narrow,the D_STRUCTURE to narrow,,D_STRUCTURE,,,,
torch.narrow,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.functional.kl_div,target,Tensor of the same shape as input,D_STRUCTURE of the same shape as PARAM,,D_STRUCTURE,BSTR,,,
torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the hessian for said PARAM which is the expected mathematical value,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.utils.dlpack.from_dlpack,dlpack,a PyCapsule object with the dltensor,a PyCapsule object with the dltensor,,,,,
torch.nn.functional.gumbel_softmax,dim,A dimension along which softmax will be computed. Default: -1.,A dimension along which softmax will be computed,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.gumbel_softmax,dim,A dimension along which softmax will be computed. Default: -1.,Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.gumbel_softmax,dim,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.gumbel_softmax,dim,DF: -1,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.quantize_per_channel,axis,dimension on which apply per-channel quantization,dimension on which apply per channel quantization,D_TYPE,,CONSTANT_VAL,,
torch.quantize_per_channel,axis,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,,
torch.eye,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",the desired data type of returned D_STRUCTURE,D_TYPE,,,,
torch.eye,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",Default if QSTR uses a global default see torch set_default_tensor_type,D_TYPE,,,,
torch.eye,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,
torch.eye,dtype,DF: None,DEFAULT None,D_TYPE,,,,
torch.eq,other,the tensor or value to compare,the D_STRUCTURE or value to compare,D_TYPE,D_STRUCTURE,,,,
torch.eq,other,DD: Tensor or float,D_STRUCTURE or D_TYPE,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,The p of the p norm to compute for the PARAM option,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.embedding,norm_type,The p of the p-norm to compute for the `max_norm` option. Default `2`.,Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.embedding,norm_type,"DD: float, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.functional.embedding,norm_type,DF: 2.0,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm1d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",a D_TYPE value that when set to CONSTANT_BOOL this module has learnable affine parameters,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm1d,affine,"a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.BatchNorm1d,affine,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.logical_not,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_not,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.logical_not,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.Softshrink,lambd,the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5,the lambda BSTR value for the Softshrink formulation,D_TYPE,,CONSTANT_VAL,,
torch.nn.Softshrink,lambd,the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.Softshrink,lambd,DF: 0.5,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.chunk,chunks,number of chunks to return,number of chunks to return,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.chunk,chunks,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,"[0,inf)",
torch.cuda.stream,stream,selected stream. This manager is a no-op if it's `None`.,selected stream,,,,,
torch.cuda.stream,stream,selected stream. This manager is a no-op if it's `None`.,This manager is a no op if it QSTR,,,,,
torch.cuda.stream,stream,DD: Stream,ONE_WORD Stream,,,,,
torch.nn.Unfold,stride,the stride of the sliding blocks in the input spatial dimensions. Default: 1,the stride of the sliding blocks in the input spatial dimensions,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Unfold,stride,the stride of the sliding blocks in the input spatial dimensions. Default: 1,Default CONSTANT_NUM,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Unfold,stride,"DD: int or tuple, optional",D_TYPE or D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.Unfold,stride,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.binary_cross_entropy_with_logits,weight,a manual rescaling weight if provided it's repeated to match input tensor shape,a manual rescaling weight if provided it repeated to match PARAM D_STRUCTURE shape,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,weight,"DD: Tensor, optional",D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,a PARAM of positive examples,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,a weight of positive examples. Must be a vector with length equal to the number of classes.,Must be a vector with length equal to the number of classes,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,"DD: Tensor, optional",D_STRUCTURE optional,D_TYPE,D_STRUCTURE,,,,
torch.nn.functional.binary_cross_entropy_with_logits,pos_weight,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,,,,
torch.utils.checkpoint.checkpoint_sequential,functions,A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,A torch nn Sequential or the D_STRUCTURE of modules or functions BSTR to run sequentially,,,,,
torch.nn.init.ones_,tensor,an n-dimensional torch.Tensor,an n dimensional D_STRUCTURE,,D_STRUCTURE,,CONSTANT_VAL,,
torch.quantization.swap_module,mapping,a dictionary that maps from nn module to nnq module,a D_STRUCTURE that maps from nn module to nnq module,,D_STRUCTURE,,,,
torch.nonzero,input,the input tensor.,the input D_STRUCTURE,,D_STRUCTURE,,,,
torch.nonzero,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_and,other,the tensor to compute AND with,the D_STRUCTURE to compute AND with,,D_STRUCTURE,,,,
torch.logical_and,other,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.cdist,x1,input tensor of shape B  times P  times M .,input D_STRUCTURE of shape B times P times M,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.cdist,x1,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.distributed.reduce,group,The process group to work on,The process group to work on,,,,,
torch.distributed.reduce,group,"DD: ProcessGroup, optional",ProcessGroup optional,,,,,
torch.distributed.reduce,group,DF: <objectobject>,DEFAULT REXPR,,,,,
torch.sin,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.sin,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.sin,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.CosineSimilarity,dim,Dimension where cosine similarity is computed. Default: 1,Dimension where cosine similarity is computed,D_TYPE,,CONSTANT_VAL,,
torch.nn.CosineSimilarity,dim,Dimension where cosine similarity is computed. Default: 1,Default CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.nn.CosineSimilarity,dim,"DD: int, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.CosineSimilarity,dim,DF: 1,DEFAULT CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,,
torch.le,input,the tensor to compare,the D_STRUCTURE to compare,,D_STRUCTURE,,,,
torch.le,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.rfft,signal_ndim,"the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3",the number of dimensions in each signal,D_TYPE,,CONSTANT_VAL,"[0,inf)",QSTR
torch.rfft,signal_ndim,"the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3",QSTR can only be CONSTANT_NUM,D_TYPE,,CONSTANT_VAL,"[0,inf)",QSTR
torch.rfft,signal_ndim,DD: int,ONE_WORD D_TYPE,D_TYPE,,CONSTANT_VAL,"[0,inf)",QSTR
torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",implicit zero paddings on both sides of the PARAM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,padding,"implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0",Default CONSTANT_NUM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.avg_pool1d,padding,DF: 0,DEFAULT CONSTANT_NUM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL an error will be raised when we detect that there exists an input such that all the outputs are independent of it,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",If CONSTANT_BOOL we return a D_STRUCTURE of zeros as the hvp for said PARAM which is the expected mathematical value,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,"If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hvp,strict,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.distributed.all_gather,tensor,Tensor to be broadcast from current process.,D_STRUCTURE to be broadcast from current process,,D_STRUCTURE,,,,
torch.distributed.all_gather,tensor,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.autograd.gradgradcheck,grad_outputs,The gradients with respect to the function's outputs.,The gradients with respect to the function outputs,,D_STRUCTURE,,,,
torch.autograd.gradgradcheck,grad_outputs,"DD: tuple of Tensor or Tensor, optional",D_STRUCTURE of D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.autograd.gradgradcheck,grad_outputs,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.cat,tensors,"any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.",any python D_STRUCTURE of D_STRUCTURE of the same type,,D_STRUCTURE,,,,
torch.cat,tensors,"any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.",Non empty D_STRUCTURE provided must have the same shape except in the cat dimension,,D_STRUCTURE,,,,
torch.cat,tensors,DD: sequence of Tensors,D_STRUCTURE of D_STRUCTURE,,D_STRUCTURE,,,,
torch.autograd.gradcheck,rtol,relative tolerance,relative tolerance,D_TYPE,,CONSTANT_VAL,,
torch.autograd.gradcheck,rtol,"DD: float, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.gradcheck,rtol,DF: 0.001,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.logspace,start,the starting value for the set of points,the starting value for the set of points,D_TYPE,,,,
torch.logspace,start,DD: float,ONE_WORD D_TYPE,D_TYPE,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Deprecated BSTR,,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",By default the losses are averaged over each loss element in the batch,,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Note that for some losses there are multiple elements per sample,,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",If the field QSTR is set to CONSTANT_BOOL the losses are instead summed for each minibatch,,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Ignored when PARAM is CONSTANT_BOOL,,,,,
torch.nn.L1Loss,size_average,"Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`",Default CONSTANT_BOOL,,,,,
torch.nn.L1Loss,size_average,"DD: bool, optional",D_TYPE optional,,,,,
torch.nn.L1Loss,size_average,DF: None,DEFAULT None,,,,,
torch.nn.Transformer,activation,"the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).",the activation function of encoder decoder intermediate layer relu or gelu default relu,D_TYPE,,,,
torch.nn.Transformer,activation,DF: relu,DEFAULT DF_STR,D_TYPE,,,,
torch.nn.utils.clip_grad_norm_,max_norm,max norm of the gradients,max norm of the gradients,D_TYPE,,,,
torch.nn.utils.clip_grad_norm_,max_norm,DD: float or int,ONE_WORD D_TYPE,D_TYPE,,,,
torch.jit.save,m,A `ScriptModule` to save.,A QSTR to save,,,,,
torch.logical_or,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.logical_or,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.logical_or,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.addcmul,input,the tensor to be added,the D_STRUCTURE to be added,,D_STRUCTURE,,,,
torch.addcmul,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,,,,
torch.sparse.addmm,mat,a dense matrix to be added,a dense matrix to be added,D_TYPE,D_STRUCTURE,,,,
torch.sparse.addmm,mat,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,,,
torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",the desired data type of returned D_STRUCTURE,D_TYPE,,,,
torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",Default if QSTR uses a global default see torch set_default_tensor_type,D_TYPE,,,,
torch.bartlett_window,dtype,"the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.",Only D_TYPE point types are supported,D_TYPE,,,,
torch.bartlett_window,dtype,"DD: `torch.dtype`, optional",D_TYPE optional,D_TYPE,,,,
torch.bartlett_window,dtype,DF: None,DEFAULT None,D_TYPE,,,,
torch.pinverse,input,"The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",The input D_STRUCTURE of size BSTR where is zero or more batch dimensions,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.pinverse,input,DD: Tensor,ONE_WORD D_STRUCTURE,,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,bias,"If `True`, adds a learnable bias to the output. Default: `True`",If CONSTANT_BOOL adds a learnable bias to the output,D_TYPE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,bias,"If `True`, adds a learnable bias to the output. Default: `True`",Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,bias,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.nn.ConvTranspose3d,bias,DF: True,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.log,out,the output tensor.,the output D_STRUCTURE,,D_STRUCTURE,,,,
torch.log,out,"DD: Tensor, optional",D_STRUCTURE optional,,D_STRUCTURE,,,,
torch.log,out,DF: None,DEFAULT None,,D_STRUCTURE,,,,
torch.nn.quantized.functional.conv2d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,non quantized bias D_STRUCTURE of shape BSTR,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.conv2d,bias,non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.,The D_STRUCTURE type must be D_TYPE,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.onnx.export,verbose,"if specified, we will print out a debug description of the trace being exported.",if specified we will print out a debug description of the trace being exported,D_TYPE,,CONSTANT_VAL,,
torch.onnx.export,verbose,"DD: bool, default False",D_TYPE default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.onnx.export,verbose,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.InstanceNorm3d,eps,a value added to the denominator for numerical stability. Default: 1e-5,a value added to the denominator for numerical stability,D_TYPE,,CONSTANT_VAL,,
torch.nn.InstanceNorm3d,eps,a value added to the denominator for numerical stability. Default: 1e-5,Default CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.nn.InstanceNorm3d,eps,DF: 1e-05,DEFAULT CONSTANT_FLOAT,D_TYPE,,CONSTANT_VAL,,
torch.autograd.grad,outputs,outputs of the differentiated function.,outputs of the differentiated function,,D_STRUCTURE,,,,
torch.autograd.grad,outputs,DD: sequence of Tensor,D_STRUCTURE of D_STRUCTURE,,D_STRUCTURE,,,,
torch.nn.CELU,inplace,can optionally do the operation in-place. Default: `False`,can optionally do the operation in place,D_TYPE,,CONSTANT_VAL,,
torch.nn.CELU,inplace,can optionally do the operation in-place. Default: `False`,Default CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.CELU,inplace,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,values,"Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",Initial values for the D_STRUCTURE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,values,"Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",Can be a D_STRUCTURE NumPy D_STRUCTURE scalar and other types,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.sparse_coo_tensor,values,DD: array_like,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,,CONSTANT_VAL,,
torch.nn.functional.affine_grid,theta,input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,input batch of affine matrices with shape BSTR for 3D,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.functional.affine_grid,theta,DD: Tensor,ONE_WORD D_STRUCTURE,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",If CONSTANT_BOOL the Hessian will be computed in a differentiable manner,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Note that when PARAM is CONSTANT_BOOL the result can not require gradients or be disconnected from the PARAM,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,"If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.",Defaults to CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,"DD: bool, optional",D_TYPE optional,D_TYPE,,CONSTANT_VAL,,
torch.autograd.functional.hessian,create_graph,DF: False,DEFAULT CONSTANT_BOOL,D_TYPE,,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",stride of the pooling operation,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Can be a single number or a D_STRUCTURE BSTR,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,stride,"stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`",Default PARAM,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.nn.quantized.functional.avg_pool2d,stride,DF: None,DEFAULT None,D_TYPE,D_STRUCTURE,BSTR,CONSTANT_VAL,,
torch.autograd.gradcheck,inputs,inputs to the function,inputs to the function,,D_STRUCTURE,,,,
torch.autograd.gradcheck,inputs,DD: tuple of Tensor or Tensor,D_STRUCTURE of D_STRUCTURE,,D_STRUCTURE,,,,
