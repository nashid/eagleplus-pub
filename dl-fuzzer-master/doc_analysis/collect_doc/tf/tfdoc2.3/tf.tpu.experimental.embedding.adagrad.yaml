aliases:
- tf.compat.v1.tpu.experimental.embedding.Adagrad
constraints:
  clip_weight_max:
    default: None
    descp: the maximum value to clip by; None means +infinity.
  clip_weight_min:
    default: None
    descp: the minimum value to clip by; None means -infinity.
  initial_accumulator_value:
    default: '0.1'
    descp: initial accumulator for Adagrad.
  learning_rate:
    default: '0.001'
    descp: The learning rate. It should be a floating point value or a callable taking
      no arguments for a dynamic learning rate.
  multiply_weight_decay_factor_by_learning_rate:
    default: None
    descp: if true, weight_decay_factor is multiplied by the current learning rate.
  slot_variable_creation_fn:
    default: None
    descp: Defaults to None. If you wish do directly control the creation of the slot
      variables, set this to a callable taking two parameters, a variable and a list
      of slot names to create for it. This function should return a dict with the
      slot names as keys and the created variables as values. When set to None (the
      default), uses the built-in variable creation.
  use_gradient_accumulation:
    default: 'True'
    descp: setting this to False makes embedding gradients calculation less accurate
      but faster.
  weight_decay_factor:
    default: None
    descp: amount of weight decay to apply; None means that the weights are not decayed.
inputs:
  optional:
  - learning_rate
  - initial_accumulator_value
  - use_gradient_accumulation
  - clip_weight_min
  - clip_weight_max
  - weight_decay_factor
  - multiply_weight_decay_factor_by_learning_rate
  - slot_variable_creation_fn
  required: []
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/tpu/experimental/embedding/Adagrad
package: tensorflow
target: Adagrad
title: tf.tpu.experimental.embedding.Adagrad
version: 2.3.0
