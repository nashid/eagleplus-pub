aliases:
- tf.compat.v1.custom_gradient
constraints:
  f:
    default: None
    descp: 'function `f(*x)` that returns a tuple `(y, grad_fn)` where: `x` is a sequence
      of `Tensor` inputs to the function.`y` is a `Tensor` or sequence of `Tensor`
      outputs of applying TensorFlow operations in `f` to `x`.`grad_fn` is a function
      with the signature `g(*grad_ys)` which returns a list of `Tensor`s - the derivatives
      of `Tensor`s in `y` with respect to the `Tensor`s in `x`.  `grad_ys` is a `Tensor`
      or sequence of`Tensor`s the same size as `y` holding the initial value gradients
      for each `Tensor` in `y`. In a pure mathematical sense, a vector-argument vector-valued
      function `f`''s derivatives should be its Jacobian matrix`J`. Here we are expressing
      the Jacobian `J` as a function `grad_fn`which defines how `J` will transform
      a vector `grad_ys` when left-multiplied with it (`grad_ys * J`). This functional
      representation of a matrix is convenient to use for chain-rule calculation (in
      e.g. the back-propagation algorithm).If `f` uses `Variable`s (that are not part
      of the inputs), i.e. through `get_variable`, then `grad_fn` should have signature
      `g(*grad_ys, variables=None)`, where `variables` is a list of the `Variable`s,
      and return a 2-tuple `(grad_xs, grad_vars)`, where`grad_xs` is the same as above,
      and `grad_vars` is a `list<Tensor>`with the derivatives of `Tensor`s in `y`
      with respect to the variables (that is, grad_vars has one Tensor per variable
      in variables). '
    normalized_default: DEFAULT None
    normalized_descp:
    - function f BSTR that returns a D_STRUCTURE BSTR where QSTR is a D_STRUCTURE
      of D_STRUCTURE inputs to the function QSTR is a D_STRUCTURE of D_STRUCTURE outputs
      of applying TensorFlow operations in QSTR to QSTR QSTR is a function with the
      signature QSTR y QSTR x
    - QSTR is a D_STRUCTURE ofD_STRUCTUREs the same size as QSTR holding the initial
      value gradients for each D_STRUCTURE in QSTR
    - In a pure mathematical sense a vector argument vector valued function QSTR derivatives
      should be its Jacobian matrix QSTR
    - Here we are expressing the Jacobian QSTR as a function QSTR which defines how
      QSTR will transform a vector QSTR when left multiplied with it grad_ys J
    - This functional representation of a matrix is convenient to use for chain rule
      calculation in e g
    - the back propagation algorithm If QSTR uses QSTR BSTR i e
    - through QSTR then QSTR should have signature g grad_ys variables None where
      QSTR is a D_STRUCTURE of the QSTR s and return a CONSTANT_NUM D_STRUCTURE BSTR
      where QSTR is the same as above and QSTR is a D_STRUCTURE REXPR QSTR y with
      respect to the variables BSTR
inputs:
  optional:
  - f
  required: []
link: https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/custom_gradient
outputs: A function `h(x)` which returns the same value as `f(x)[0]` and whose gradient
  (as calculated by `tf.gradients`) is determined by `f(x)[1]`.
package: tensorflow
target: custom_gradient
title: tf.custom_gradient
version: 2.1.0
