constraints:
  accumulators:
    descp: A Tensor of type float32. Value of accumulators used in the Adagrad optimization
      algorithm.
    dtype:
    - tf.float32
    tensor_t:
    - tf.tensor
  config:
    default: ''
    descp: An optional string. Defaults to "".
    dtype:
    - tf.string
    ndim:
    - '0'
  gradient_accumulators:
    descp: A Tensor of type float32. Value of gradient_accumulators used in the Adagrad
      optimization algorithm.
    dtype:
    - tf.float32
    tensor_t:
    - tf.tensor
  name:
    default: None
    descp: A name for the operation (optional).
    dtype:
    - tf.string
    ndim:
    - '0'
  num_shards:
    descp: An int.
    dtype:
    - int
    ndim:
    - '0'
    range:
    - '[0,inf)'
  parameters:
    descp: A Tensor of type float32. Value of parameters used in the Adagrad optimization
      algorithm.
    dtype:
    - tf.float32
    tensor_t:
    - tf.tensor
  shard_id:
    descp: An int.
    dtype:
    - int
    ndim:
    - '0'
  table_id:
    default: '-1'
    descp: An optional int. Defaults to -1.
    dtype:
    - int
    ndim:
    - '0'
  table_name:
    default: ''
    descp: An optional string. Defaults to "".
    dtype:
    - tf.string
    ndim:
    - '0'
inputs:
  optional:
  - table_id
  - table_name
  - config
  - name
  required:
  - parameters
  - accumulators
  - gradient_accumulators
  - num_shards
  - shard_id
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/LoadTPUEmbeddingAdagradParametersGradAccumDebug
outputs:
- The created Operation.
package: tensorflow
target: LoadTPUEmbeddingAdagradParametersGradAccumDebug
title: tf.raw_ops.LoadTPUEmbeddingAdagradParametersGradAccumDebug
version: 2.3.0
