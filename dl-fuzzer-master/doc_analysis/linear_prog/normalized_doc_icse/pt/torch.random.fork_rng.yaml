constraints:
  _caller:
    default: fork_rng
    descp: ''
    normalized_default: DEFAULT DF_STR
    normalized_descp: []
  _devices_kw:
    default: devices
    descp: ''
    normalized_default: DEFAULT DF_STR
    normalized_descp: []
  devices:
    default: None
    descp: CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By
      default, `fork_rng()` operates on all devices, but will emit a warning if your
      machine has a lot of devices, since this function will run very slowly in that
      case. If you explicitly specify devices, this warning will be suppressed
    doc_dtype: iterable of CUDA IDs
    normalized_default: DEFAULT None
    normalized_descp:
    - CUDA devices for which to fork the RNG
    - CPU RNG state is always forked
    - By default fork_rng operates on all devices but will emit a warning if your
      machine has a lot of devices since this function will run very slowly in that
      case
    - If you explicitly specify devices this warning will be suppressed
    normalized_docdtype: D_STRUCTURE of CUDA IDs
  enabled:
    default: 'True'
    descp: if `False`, the RNG is not forked.  This is a convenience argument for
      easily disabling the context manager without having to delete it and unindent
      your Python code under it.
    doc_dtype: bool
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if CONSTANT_BOOL the RNG is not forked
    - This is a convenience argument for easily disabling the context manager without
      having to delete it and unindent your Python code under it
    normalized_docdtype: ONE_WORD D_TYPE
inputs:
  optional:
  - devices
  - enabled
  - _caller
  - _devices_kw
  required: []
link: https://pytorch.org/docs/stable/random.html#torch.random.fork_rng
package: torch
target: fork_rng
title: torch.random.fork_rng
version: 1.5.0
