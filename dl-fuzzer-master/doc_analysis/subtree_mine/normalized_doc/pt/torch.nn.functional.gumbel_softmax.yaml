constraints:
  dim:
    default: '-1'
    descp: 'A dimension along which softmax will be computed. Default: -1.'
    doc_dtype: int
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - A dimension along which softmax will be computed
    - Default CONSTANT_NUM
    normalized_docdtype: ONE_WORD D_TYPE
  eps:
    default: 1e-10
    descp: ''
    normalized_default: DEFAULT CONSTANT_FLOAT
    normalized_descp: []
  hard:
    default: 'False'
    descp: if `True`, the returned samples will be discretized as one-hot vectors,
      but will be differentiated as if it is the soft sample in autograd
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - if CONSTANT_BOOL, the returned samples will be discretized as one hot D_STRUCTURE,
      but will be differentiated as if it is the soft sample in autograd
  logits:
    descp: '[ u2026, num_features] unnormalized log probabilities'
    normalized_descp:
    - BSTR unnormalized log probabilities
  tau:
    default: '1'
    descp: non-negative scalar temperature
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - non negative scalar temperature
inputs:
  optional:
  - tau
  - hard
  - eps
  - dim
  required:
  - logits
link: https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.gumbel_softmax
package: torch
target: gumbel_softmax
title: torch.nn.functional.gumbel_softmax
version: 1.5.0
