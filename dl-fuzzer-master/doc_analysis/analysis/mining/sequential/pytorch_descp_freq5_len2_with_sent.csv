,idx,freq,sequence,sentence,api,len
0,2,200,"['input', 'tensor']",the input tensor.,torch.std2.yaml,2
1,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,2
2,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,2
3,2,200,"['input', 'tensor']",the input tensor.,torch.real.yaml,2
4,2,200,"['input', 'tensor']",the input tensor.,torch.log1p.yaml,2
5,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
6,2,200,"['input', 'tensor']",the input tensor.,torch.prod2.yaml,2
7,2,200,"['input', 'tensor']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,2
8,2,200,"['input', 'tensor']",the input tensor.,torch.pow.yaml,2
9,2,200,"['input', 'tensor']",the input tensor.,torch.exp.yaml,2
10,2,200,"['input', 'tensor']",the input tensor.,torch.cummax.yaml,2
11,2,200,"['input', 'tensor']",the input tensor.,torch.mean2.yaml,2
12,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
13,2,200,"['input', 'tensor']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,2
14,2,200,"['input', 'tensor']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
15,2,200,"['input', 'tensor']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,2
16,2,200,"['input', 'tensor']",the input tensor.,torch.ceil.yaml,2
17,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
18,2,200,"['input', 'tensor']",the input tensor.,torch.sum2.yaml,2
19,2,200,"['input', 'tensor']",the input tensor.,torch.min22.yaml,2
20,2,200,"['input', 'tensor']",the second input tensor,torch.min22.yaml,2
21,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
22,2,200,"['input', 'tensor']",the input tensor.,torch.sum.yaml,2
23,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
24,2,200,"['input', 'tensor']",the input tensor.,torch.var_mean.yaml,2
25,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
26,2,200,"['input', 'tensor']",the input tensor.,torch.as_strided.yaml,2
27,2,200,"['input', 'tensor']",the input tensor.,torch.max22.yaml,2
28,2,200,"['input', 'tensor']",the second input tensor,torch.max22.yaml,2
29,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
30,2,200,"['input', 'tensor']",the input tensor.,torch.div.yaml,2
31,2,200,"['input', 'tensor']",the input tensor.,torch.nonzero.yaml,2
32,2,200,"['input', 'tensor']",the input tensor.,torch.sinh.yaml,2
33,2,200,"['input', 'tensor']",the input tensor.,torch.var.yaml,2
34,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
35,2,200,"['input', 'tensor']",the input tensor.,torch.std_mean.yaml,2
36,2,200,"['input', 'tensor']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
37,2,200,"['input', 'tensor']",input tensor,torch.nn.functional.glu.yaml,2
38,2,200,"['input', 'tensor']",the input tensor.,torch.erf.yaml,2
39,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
40,2,200,"['input', 'tensor']",the input tensor.,torch.neg.yaml,2
41,2,200,"['input', 'tensor']",input tensor of any shape,torch.nn.functional.normalize.yaml,2
42,2,200,"['input', 'tensor']",the first input tensor,torch.add.yaml,2
43,2,200,"['input', 'tensor']",the second input tensor,torch.add.yaml,2
44,2,200,"['input', 'tensor']",the input tensor.,torch.diag.yaml,2
45,2,200,"['input', 'tensor']",the input tensor.,torch.sigmoid.yaml,2
46,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
47,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2
48,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
49,2,200,"['input', 'tensor']",the input tensor.,torch.erfinv.yaml,2
50,2,200,"['input', 'tensor']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
51,2,200,"['input', 'tensor']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
52,2,200,"['input', 'tensor']",the input tensor.,torch.cummin.yaml,2
53,2,200,"['input', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
54,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_and.yaml,2
55,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_and.yaml,2
56,2,200,"['input', 'tensor']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
57,2,200,"['input', 'tensor']",Input tensor.,torch.distributed.gather.yaml,2
58,2,200,"['input', 'tensor']",the input tensor.,torch.topk.yaml,2
59,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
60,2,200,"['input', 'tensor']",the input tensor.,torch.reciprocal.yaml,2
61,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
62,2,200,"['input', 'tensor']",the input tensor.,torch.cos.yaml,2
63,2,200,"['input', 'tensor']",the input tensor.,torch.masked_select.yaml,2
64,2,200,"['input', 'tensor']",the input tensor.,torch.matrix_power.yaml,2
65,2,200,"['input', 'tensor']",the input tensor.,torch.min2.yaml,2
66,2,200,"['input', 'tensor']",the input tensor.,torch.angle.yaml,2
67,2,200,"['input', 'tensor']",the input tensor.,torch.erfc.yaml,2
68,2,200,"['input', 'tensor']",the input tensor.,torch.round.yaml,2
69,2,200,"['input', 'tensor']",the input tensor.,torch.polygamma.yaml,2
70,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2
71,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
72,2,200,"['input', 'tensor']",the input tensor.,torch.cross.yaml,2
73,2,200,"['input', 'tensor']",the second input tensor,torch.cross.yaml,2
74,2,200,"['input', 'tensor']",the input tensor.,torch.argmin2.yaml,2
75,2,200,"['input', 'tensor']",the input tensor.,torch.acos.yaml,2
76,2,200,"['input', 'tensor']","input tensor (minibatch , in _channels , iH , iW) ",torch.nn.functional.avg_pool2d.yaml,2
77,2,200,"['input', 'tensor']",the input tensor.,torch.numel.yaml,2
78,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
79,2,200,"['input', 'tensor']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
80,2,200,"['input', 'tensor']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
81,2,200,"['input', 'tensor']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
82,2,200,"['input', 'tensor']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
83,2,200,"['input', 'tensor']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
84,2,200,"['input', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2
85,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
86,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2
87,2,200,"['input', 'tensor']",the input tensor.,torch.bitwise_not.yaml,2
88,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2
89,2,200,"['input', 'tensor']",the input tensor.,torch.var_mean2.yaml,2
90,2,200,"['input', 'tensor']",the input tensor.,torch.trunc.yaml,2
91,2,200,"['input', 'tensor']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
92,2,200,"['input', 'tensor']",the input tensor.,torch.conj.yaml,2
93,2,200,"['input', 'tensor']",the input tensor.,torch.repeat_interleave.yaml,2
94,2,200,"['input', 'tensor']",the input tensor.,torch.median2.yaml,2
95,2,200,"['input', 'tensor']",the input tensor.,torch.histc.yaml,2
96,2,200,"['input', 'tensor']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
97,2,200,"['input', 'tensor']",the input tensor.,torch.clamp.yaml,2
98,2,200,"['input', 'tensor']",the input tensor.,torch.flatten.yaml,2
99,2,200,"['input', 'tensor']",the input tensor.,torch.argsort.yaml,2
100,2,200,"['input', 'tensor']",the input tensor.,torch.logical_or.yaml,2
101,2,200,"['input', 'tensor']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
102,2,200,"['input', 'tensor']",the input tensor.,torch.logsumexp.yaml,2
103,2,200,"['input', 'tensor']",the input tensor.,torch.mean.yaml,2
104,2,200,"['input', 'tensor']",the input tensor.,torch.sort.yaml,2
105,2,200,"['input', 'tensor']",the input tensor.,torch.logical_and.yaml,2
106,2,200,"['input', 'tensor']",the input tensor.,torch.sin.yaml,2
107,2,200,"['input', 'tensor']",the input tensor.,torch.tan.yaml,2
108,2,200,"['input', 'tensor']",the input tensor.,torch.argmin.yaml,2
109,2,200,"['input', 'tensor']",the input tensor.,torch.max2.yaml,2
110,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2
111,2,200,"['input', 'tensor']",the input tensor.,torch.cumprod.yaml,2
112,2,200,"['input', 'tensor']",the input tensor.,torch.diag_embed.yaml,2
113,2,200,"['input', 'tensor']",the input tensor.,torch.imag.yaml,2
114,2,200,"['input', 'tensor']",the input tensor.,torch.atan.yaml,2
115,2,200,"['input', 'tensor']",the input tensor.,torch.cosh.yaml,2
116,2,200,"['input', 'tensor']",the input tensor.,torch.asin.yaml,2
117,2,200,"['input', 'tensor']",the input tensor.,torch.argmax2.yaml,2
118,2,200,"['input', 'tensor']",the input tensor,torch.stft.yaml,2
119,2,200,"['input', 'tensor']",the input tensor.,torch.square.yaml,2
120,2,200,"['input', 'tensor']",the input tensor.,torch.median.yaml,2
121,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
122,2,200,"['input', 'tensor']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
123,2,200,"['input', 'tensor']",the input tensor.,torch.log2.yaml,2
124,2,200,"['input', 'tensor']",the input tensor.,torch.logical_xor.yaml,2
125,2,200,"['input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
126,2,200,"['input', 'tensor']",the input tensor.,torch.prod.yaml,2
127,2,200,"['input', 'tensor']",the input tensor.,torch.argmax.yaml,2
128,2,200,"['input', 'tensor']",the input tensor.,torch.floor.yaml,2
129,2,200,"['input', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
130,2,200,"['input', 'tensor']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2
131,2,200,"['input', 'tensor']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2
132,2,200,"['input', 'tensor']",the input tensor,torch.norm.yaml,2
133,2,200,"['input', 'tensor']",the input tensor,torch.unique.yaml,2
134,2,200,"['input', 'tensor']",the input tensor.,torch.index_select.yaml,2
135,2,200,"['input', 'tensor']",the input tensor.,torch.kthvalue.yaml,2
136,2,200,"['input', 'tensor']",the first input tensor,torch.atan2.yaml,2
137,2,200,"['input', 'tensor']",the second input tensor,torch.atan2.yaml,2
138,2,200,"['input', 'tensor']",the input tensor.,torch.flip.yaml,2
139,2,200,"['input', 'tensor']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
140,2,200,"['input', 'tensor']",the input tensor.,torch.var2.yaml,2
141,2,200,"['input', 'tensor']",the input tensor.,torch.logical_not.yaml,2
142,2,200,"['input', 'tensor']",the input tensor containing probabilities,torch.multinomial.yaml,2
143,2,200,"['input', 'tensor']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2
144,2,200,"['input', 'tensor']",the input tensor.,torch.min.yaml,2
145,2,200,"['input', 'tensor']",the input tensor.,torch.expm1.yaml,2
146,2,200,"['input', 'tensor']",the input tensor.,torch.mode.yaml,2
147,2,200,"['input', 'tensor']",the input tensor.,torch.log.yaml,2
148,2,200,"['input', 'tensor']",the input tensor.,torch.abs.yaml,2
149,2,200,"['input', 'tensor']",the input tensor.,torch.tril.yaml,2
150,2,200,"['input', 'tensor']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2
151,2,200,"['input', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,2
152,2,200,"['input', 'tensor']",the input tensor.,torch.triu.yaml,2
153,2,200,"['input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
154,2,200,"['input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
155,2,200,"['input', 'tensor']",the input tensor,torch.nn.quantized.functional.interpolate.yaml,2
156,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2
157,2,200,"['input', 'tensor']",the input tensor.,torch.unsqueeze.yaml,2
158,2,200,"['input', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
159,2,200,"['input', 'tensor']",an input tensor or number,torch.result_type.yaml,2
160,2,200,"['input', 'tensor']",an input tensor or number,torch.result_type.yaml,2
161,2,200,"['input', 'tensor']",the input tensor.,torch.log10.yaml,2
162,2,200,"['input', 'tensor']",the first input tensor,torch.bitwise_or.yaml,2
163,2,200,"['input', 'tensor']",the second input tensor,torch.bitwise_or.yaml,2
164,2,200,"['input', 'tensor']",the input tensor.,torch.std_mean2.yaml,2
165,2,200,"['input', 'tensor']",the input tensor.,torch.sqrt.yaml,2
166,2,200,"['input', 'tensor']",the input tensor.,torch.lgamma.yaml,2
167,2,200,"['input', 'tensor']",the input tensor.,torch.sign.yaml,2
168,2,200,"['input', 'tensor']",the input tensor,torch.unique_consecutive.yaml,2
169,2,200,"['input', 'tensor']",the input tensor.,torch.rot90.yaml,2
170,2,200,"['input', 'tensor']",the input tensor.,torch.dist.yaml,2
171,2,200,"['input', 'tensor']",the Right-hand-side input tensor,torch.dist.yaml,2
172,2,200,"['input', 'tensor']",the input tensor.,torch.squeeze.yaml,2
173,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2
174,2,200,"['input', 'tensor']",the input tensor.,torch.cumsum.yaml,2
175,2,200,"['input', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2
176,2,200,"['input', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2
177,2,200,"['input', 'tensor']",the input tensor.,torch.tanh.yaml,2
178,2,200,"['input', 'tensor']",the input tensor.,torch.t.yaml,2
179,2,200,"['input', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
180,2,200,"['input', 'tensor']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
181,2,200,"['input', 'tensor']",the input tensor.,torch.transpose.yaml,2
182,2,200,"['input', 'tensor']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
183,2,200,"['input', 'tensor']",the input 2-D tensor,torch.matrix_rank.yaml,2
184,2,200,"['input', 'tensor']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2
185,2,200,"['input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
186,2,200,"['input', 'tensor']",the input tensor.,torch.roll.yaml,2
187,2,200,"['input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
188,2,200,"['input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
189,2,200,"['input', 'tensor']",the input tensor,torch.nn.functional.interpolate.yaml,2
190,2,200,"['input', 'tensor']",the input tensor.,torch.std.yaml,2
191,2,200,"['input', 'tensor']",the input tensor.,torch.diagonal.yaml,2
192,2,200,"['input', 'tensor']",the input tensor.,torch.diagflat.yaml,2
193,2,200,"['input', 'tensor']",the input tensor.,torch.renorm.yaml,2
194,2,200,"['input', 'tensor']",the input tensor.,torch.rsqrt.yaml,2
195,2,200,"['input', 'tensor']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2
196,2,200,"['input', 'tensor']",the input tensor.,torch.max.yaml,2
197,2,200,"['input', 'tensor']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
198,2,200,"['input', 'tensor']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
199,2,200,"['input', 'tensor']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
200,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,2
201,3,163,"['output', 'tensor']",the output tensor.,torch.std2.yaml,2
202,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_xor.yaml,2
203,3,163,"['output', 'tensor']",the output tensor.,torch.real.yaml,2
204,3,163,"['output', 'tensor']",the output tensor.,torch.addcdiv.yaml,2
205,3,163,"['output', 'tensor']",the output tensor.,torch.log1p.yaml,2
206,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,2
207,3,163,"['output', 'tensor']",the output tensor.,torch.bernoulli.yaml,2
208,3,163,"['output', 'tensor']",the shape of the output tensor,torch.empty_strided.yaml,2
209,3,163,"['output', 'tensor']",the strides of the output tensor,torch.empty_strided.yaml,2
210,3,163,"['output', 'tensor']",the output tensor.,torch.pow.yaml,2
211,3,163,"['output', 'tensor']",the output tensor.,torch.exp.yaml,2
212,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
213,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,2
214,3,163,"['output', 'tensor']",the output tensor.,torch.mean2.yaml,2
215,3,163,"['output', 'tensor']",the output tensor.,torch.arange.yaml,2
216,3,163,"['output', 'tensor']",the output tensor.,torch.ceil.yaml,2
217,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,2
218,3,163,"['output', 'tensor']",the output tensor.,torch.min22.yaml,2
219,3,163,"['output', 'tensor']",the output tensor.,torch.lerp.yaml,2
220,3,163,"['output', 'tensor']",the shape of the output tensor,torch.as_strided.yaml,2
221,3,163,"['output', 'tensor']",the offset in the underlying storage of the output tensor,torch.as_strided.yaml,2
222,3,163,"['output', 'tensor']",the stride of the output tensor,torch.as_strided.yaml,2
223,3,163,"['output', 'tensor']",the output tensor.,torch.max22.yaml,2
224,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
225,3,163,"['output', 'tensor']",the output tensor containing indices,torch.nonzero.yaml,2
226,3,163,"['output', 'tensor']",the output tensor.,torch.sinh.yaml,2
227,3,163,"['output', 'tensor']",the output tensors,torch.eig.yaml,2
228,3,163,"['output', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
229,3,163,"['output', 'tensor']",the output tensor.,torch.erf.yaml,2
230,3,163,"['output', 'tensor']",the output tensor.,torch.neg.yaml,2
231,3,163,"['output', 'tensor']",the output tensor.,torch.baddbmm.yaml,2
232,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.le.yaml,2
233,3,163,"['output', 'tensor']",the output tensor.,torch.nn.functional.normalize.yaml,2
234,3,163,"['output', 'tensor']",the output tensor.,torch.diag.yaml,2
235,3,163,"['output', 'tensor']",the output tensor.,torch.mv.yaml,2
236,3,163,"['output', 'tensor']",the output tensor.,torch.addr.yaml,2
237,3,163,"['output', 'tensor']",the output tensor.,torch.sigmoid.yaml,2
238,3,163,"['output', 'tensor']",the output tensor.,torch.fmod.yaml,2
239,3,163,"['output', 'tensor']",the output tensor.,torch.pow2.yaml,2
240,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
241,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.gt.yaml,2
242,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.ne.yaml,2
243,3,163,"['output', 'tensor']",the output tensor.,torch.erfinv.yaml,2
244,3,163,"['output', 'tensor']",the output tensor.,torch.addbmm.yaml,2
245,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
246,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_and.yaml,2
247,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
248,3,163,"['output', 'tensor']",the output tensor.,torch.reciprocal.yaml,2
249,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
250,3,163,"['output', 'tensor']",the output tensor.,torch.cos.yaml,2
251,3,163,"['output', 'tensor']",the output tensor.,torch.masked_select.yaml,2
252,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,2
253,3,163,"['output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2
254,3,163,"['output', 'tensor']",the output tensor.,torch.angle.yaml,2
255,3,163,"['output', 'tensor']",the output tensor.,torch.erfc.yaml,2
256,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
257,3,163,"['output', 'tensor']",the output tensor.,torch.rand.yaml,2
258,3,163,"['output', 'tensor']",the output tensor.,torch.round.yaml,2
259,3,163,"['output', 'tensor']",the output tensor.,torch.polygamma.yaml,2
260,3,163,"['output', 'tensor']",the output tensor.,torch.cross.yaml,2
261,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,2
262,3,163,"['output', 'tensor']",the output tensor.,torch.acos.yaml,2
263,3,163,"['output', 'tensor']",the output tensor.,torch.eye.yaml,2
264,3,163,"['output', 'tensor']",the output tensor.,torch.addmm.yaml,2
265,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
266,3,163,"['output', 'tensor']",the output tensor.,torch.addcmul.yaml,2
267,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_not.yaml,2
268,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,2
269,3,163,"['output', 'tensor']",the output tensor.,torch.trunc.yaml,2
270,3,163,"['output', 'tensor']",the output tensor.,torch.conj.yaml,2
271,3,163,"['output', 'tensor']",the output tensor.,torch.mul.yaml,2
272,3,163,"['output', 'tensor']",the output tensor.,torch.remainder.yaml,2
273,3,163,"['output', 'tensor']",the output tensor for c,torch.cholesky_solve.yaml,2
274,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,2
275,3,163,"['output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2
276,3,163,"['output', 'tensor']",the output tensor.,torch.histc.yaml,2
277,3,163,"['output', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2
278,3,163,"['output', 'tensor']",the output tensor.,torch.clamp.yaml,2
279,3,163,"['output', 'tensor']",the output tensor.,torch.eq.yaml,2
280,3,163,"['output', 'tensor']",the output tensor.,torch.logical_or.yaml,2
281,3,163,"['output', 'tensor']",the output tensor.,torch.normal.yaml,2
282,3,163,"['output', 'tensor']",the output tensor.,torch.inverse.yaml,2
283,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,2
284,3,163,"['output', 'tensor']",the output tensor.,torch.logsumexp.yaml,2
285,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
286,3,163,"['output', 'tensor']",the output tensor.,torch.logical_and.yaml,2
287,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
288,3,163,"['output', 'tensor']",the output tensor.,torch.ones.yaml,2
289,3,163,"['output', 'tensor']",the output tensor.,torch.randperm.yaml,2
290,3,163,"['output', 'tensor']",the output tensor.,torch.sin.yaml,2
291,3,163,"['output', 'tensor']",the output tensor.,torch.tan.yaml,2
292,3,163,"['output', 'tensor']",the output tensor.,torch.stack.yaml,2
293,3,163,"['output', 'tensor']",the output tensor.,torch.normal222.yaml,2
294,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
295,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,2
296,3,163,"['output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2
297,3,163,"['output', 'tensor']",the output tensor.,torch.cumprod.yaml,2
298,3,163,"['output', 'tensor']",the output tensor.,torch.imag.yaml,2
299,3,163,"['output', 'tensor']",the output tensor.,torch.matmul.yaml,2
300,3,163,"['output', 'tensor']",the output tensor.,torch.atan.yaml,2
301,3,163,"['output', 'tensor']",the output tensor.,torch.cosh.yaml,2
302,3,163,"['output', 'tensor']",the output tensor.,torch.asin.yaml,2
303,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,2
304,3,163,"['output', 'tensor']",the output tensor,torch.normal22.yaml,2
305,3,163,"['output', 'tensor']",the output tensor.,torch.square.yaml,2
306,3,163,"['output', 'tensor']",the output tensor.,torch.linspace.yaml,2
307,3,163,"['output', 'tensor']",the output tensor.,torch.bmm.yaml,2
308,3,163,"['output', 'tensor']",the output tensor.,torch.normal2.yaml,2
309,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.ge.yaml,2
310,3,163,"['output', 'tensor']",the output tensor.,torch.log2.yaml,2
311,3,163,"['output', 'tensor']",the output tensor.,torch.logical_xor.yaml,2
312,3,163,"['output', 'tensor']",the output tensor.,torch.floor.yaml,2
313,3,163,"['output', 'tensor']",the output tensor.,torch.addmv.yaml,2
314,3,163,"['output', 'tensor']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,2
315,3,163,"['output', 'tensor']",the output tensor.,torch.norm.yaml,2
316,3,163,"['output', 'tensor']",the output tensor that must be a BoolTensor,torch.lt.yaml,2
317,3,163,"['output', 'tensor']",the output tensor.,torch.index_select.yaml,2
318,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,2
319,3,163,"['output', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
320,3,163,"['output', 'tensor']",the output tensor.,torch.atan2.yaml,2
321,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,2
322,3,163,"['output', 'tensor']",the output tensor.,torch.var2.yaml,2
323,3,163,"['output', 'tensor']",the output tensor.,torch.logical_not.yaml,2
324,3,163,"['output', 'tensor']",the output tensor.,torch.multinomial.yaml,2
325,3,163,"['output', 'tensor']",the output tensor for inv,torch.cholesky_inverse.yaml,2
326,3,163,"['output', 'tensor']",the output tensor.,torch.expm1.yaml,2
327,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,2
328,3,163,"['output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
329,3,163,"['output', 'tensor']",the output tensor.,torch.log.yaml,2
330,3,163,"['output', 'tensor']",the output tensor.,torch.abs.yaml,2
331,3,163,"['output', 'tensor']",the output tensor.,torch.tril.yaml,2
332,3,163,"['output', 'tensor']",the output tensor.,torch.logspace.yaml,2
333,3,163,"['output', 'tensor']",the output tensor.,torch.triu.yaml,2
334,3,163,"['output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
335,3,163,"['output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
336,3,163,"['output', 'tensor']",the output tensor.,torch.log10.yaml,2
337,3,163,"['output', 'tensor']",the output tensor.,torch.bitwise_or.yaml,2
338,3,163,"['output', 'tensor']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,2
339,3,163,"['output', 'tensor']",the output tensor.,torch.sqrt.yaml,2
340,3,163,"['output', 'tensor']",the output tensor.,torch.lgamma.yaml,2
341,3,163,"['output', 'tensor']",the output tensor.,torch.sign.yaml,2
342,3,163,"['output', 'tensor']",the output tensor.,torch.cat.yaml,2
343,3,163,"['output', 'tensor']",Output tensor.,torch.distributed.scatter.yaml,2
344,3,163,"['output', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2
345,3,163,"['output', 'tensor']",the output tensor.,torch.squeeze.yaml,2
346,3,163,"['output', 'tensor']",the output tensor.,torch.cumsum.yaml,2
347,3,163,"['output', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2
348,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
349,3,163,"['output', 'tensor']",the output tensor.,torch.zeros.yaml,2
350,3,163,"['output', 'tensor']",the output tensor.,torch.tanh.yaml,2
351,3,163,"['output', 'tensor']",the output tuple of tensors,torch.svd.yaml,2
352,3,163,"['output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
353,3,163,"['output', 'tensor']",the output tensor.,torch.randn.yaml,2
354,3,163,"['output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
355,3,163,"['output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
356,3,163,"['output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
357,3,163,"['output', 'tensor']",the number to fill the output tensor with.,torch.full.yaml,2
358,3,163,"['output', 'tensor']",the output tensor.,torch.full.yaml,2
359,3,163,"['output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
360,3,163,"['output', 'tensor']",the output tensor.,torch.renorm.yaml,2
361,3,163,"['output', 'tensor']",the output tensor.,torch.rsqrt.yaml,2
362,3,163,"['output', 'tensor']",the output tensor.,torch.mm.yaml,2
363,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
364,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,2
365,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,2
366,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,2
367,4,112,"['returned', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.empty_strided.yaml,2
368,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_strided.yaml,2
369,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,2
370,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,2
371,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,2
372,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.arange.yaml,2
373,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
374,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
375,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,2
376,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,2
377,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,2
378,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,2
379,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones_like.yaml,2
380,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,2
381,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,2
382,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,2
383,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,2
384,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_like.yaml,2
385,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,2
386,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,2
387,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,2
388,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hann_window.yaml,2
389,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2
390,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2
391,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,2
392,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2
393,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,2
394,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,2
395,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros_like.yaml,2
396,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,2
397,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,2
398,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,2
399,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand.yaml,2
400,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,2
401,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,2
402,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,2
403,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.eye.yaml,2
404,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,2
405,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,2
406,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,2
407,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,2
408,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn_like.yaml,2
409,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2
410,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,2
411,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,2
412,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,2
413,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones.yaml,2
414,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,2
415,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,2
416,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,2
417,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randperm.yaml,2
418,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,2
419,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,2
420,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,2
421,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,2
422,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,2
423,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,2
424,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,2
425,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,2
426,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.linspace.yaml,2
427,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2
428,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,2
429,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,2
430,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2
431,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hamming_window.yaml,2
432,4,112,"['returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
433,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,2
434,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,2
435,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,2
436,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2
437,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.blackman_window.yaml,2
438,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,2
439,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,2
440,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,2
441,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2
442,4,112,"['returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2
443,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.bartlett_window.yaml,2
444,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,2
445,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,2
446,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,2
447,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.logspace.yaml,2
448,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2
449,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,2
450,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,2
451,4,112,"['returned', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.tensor.yaml,2
452,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.tensor.yaml,2
453,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,2
454,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2
455,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.sparse_coo_tensor.yaml,2
456,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,2
457,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,2
458,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,2
459,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,2
460,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros.yaml,2
461,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2
462,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,2
463,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,2
464,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,2
465,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn.yaml,2
466,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,2
467,4,112,"['returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,2
468,4,112,"['returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,2
469,4,112,"['returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,2
470,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand_like.yaml,2
471,4,112,"['returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,2
472,4,112,"['returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,2
473,4,112,"['returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,2
474,4,112,"['returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.full.yaml,2
475,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,2
476,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
477,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
478,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,2
479,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,2
480,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
481,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
482,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,2
483,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,2
484,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,2
485,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.l1_unstructured.yaml,2
486,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,2
487,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,2
488,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.random_unstructured.yaml,2
489,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,2
490,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
491,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
492,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,2
493,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.ln_structured.yaml,2
494,5,108,"['SOME_DTYPE', 'tensor']",float tensor to quantize,torch.quantize_per_channel.yaml,2
495,5,108,"['SOME_DTYPE', 'tensor']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
496,5,108,"['SOME_DTYPE', 'tensor']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
497,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,2
498,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,2
499,5,108,"['SOME_DTYPE', 'tensor']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
500,5,108,"['SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
501,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,2
502,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
503,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
504,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,2
505,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,2
506,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
507,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
508,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,2
509,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.random_structured.yaml,2
510,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,2
511,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,2
512,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.remove.yaml,2
513,5,108,"['SOME_DTYPE', 'tensor']",It should match `devices` in length and sum to `tensor.size(dim)`.,torch.cuda.comm.scatter.yaml,2
514,5,108,"['SOME_DTYPE', 'tensor']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,2
515,5,108,"['SOME_DTYPE', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2
516,5,108,"['SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
517,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,2
518,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
519,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
520,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,2
521,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,2
522,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
523,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
524,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,2
525,5,108,"['SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
526,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,2
527,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
528,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
529,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,2
530,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
531,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
532,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,2
533,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
534,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
535,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,2
536,5,108,"['SOME_DTYPE', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2
537,5,108,"['SOME_DTYPE', 'tensor']",float tensor to quantize,torch.quantize_per_tensor.yaml,2
538,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,2
539,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
540,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
541,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2
542,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune,torch.nn.utils.prune.custom_from_mask.yaml,2
543,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,2
544,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
545,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
546,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2
547,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,2
548,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
549,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
550,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,2
551,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
552,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
553,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2
554,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,2
555,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
556,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
557,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,2
558,5,108,"['SOME_DTYPE', 'tensor']",1-d int tensor,torch.bincount.yaml,2
559,5,108,"['SOME_DTYPE', 'tensor']",module containing the tensor to prune.,torch.nn.utils.prune.identity.yaml,2
560,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,2
561,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
562,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
563,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,2
564,5,108,"['SOME_DTYPE', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
565,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
566,5,108,"['SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
567,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,2
568,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
569,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
570,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,2
571,5,108,"['SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
572,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,2
573,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
574,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
575,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,2
576,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,2
577,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,2
578,5,108,"['SOME_DTYPE', 'tensor']",the desired device of returned tensor.,torch.full.yaml,2
579,5,108,"['SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
580,5,108,"['SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
581,5,108,"['SOME_DTYPE', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,2
582,5,108,"['SOME_DTYPE', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
583,6,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,3
584,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,3
585,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,3
586,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,3
587,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,3
588,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,3
589,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,3
590,6,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,3
591,6,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,3
592,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,3
593,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,3
594,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,3
595,6,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,3
596,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,3
597,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,3
598,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,3
599,6,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,3
600,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,3
601,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,3
602,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,3
603,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,3
604,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,3
605,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,3
606,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,3
607,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,3
608,6,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,3
609,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,3
610,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,3
611,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,3
612,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,3
613,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,3
614,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,3
615,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,3
616,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,3
617,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,3
618,6,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,3
619,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,3
620,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,3
621,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,3
622,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,3
623,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,3
624,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,3
625,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,3
626,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,3
627,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,3
628,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,3
629,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,3
630,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,3
631,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,3
632,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,3
633,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,3
634,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,3
635,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,3
636,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,3
637,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,3
638,6,88,"['desired', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,3
639,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,3
640,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,3
641,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,3
642,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,3
643,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,3
644,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,3
645,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,3
646,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,3
647,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,3
648,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,3
649,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,3
650,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,3
651,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,3
652,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,3
653,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,3
654,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,3
655,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,3
656,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,3
657,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,3
658,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,3
659,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,3
660,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,3
661,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,3
662,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,3
663,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,3
664,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,3
665,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,3
666,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,3
667,6,88,"['desired', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,3
668,6,88,"['desired', 'returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,3
669,6,88,"['desired', 'returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,3
670,6,88,"['desired', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,3
671,7,86,"['default', 'none']",Default: None.,torch.prod2.yaml,2
672,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
673,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
674,7,86,"['default', 'none']",Default: None,torch.nn.functional.avg_pool3d.yaml,2
675,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
676,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
677,7,86,"['default', 'none']",Default: None,torch.nn.quantized.functional.avg_pool2d.yaml,2
678,7,86,"['default', 'none']",Default: None.,torch.sum2.yaml,2
679,7,86,"['default', 'none']",Default: None.,torch.sum.yaml,2
680,7,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose2d.yaml,2
681,7,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
682,7,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2
683,7,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
684,7,86,"['default', 'none']",Default: `None`,torch.nn.functional.conv2d.yaml,2
685,7,86,"['default', 'none']",Default: `None`,torch.irfft.yaml,2
686,7,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
687,7,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2
688,7,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
689,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
690,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
691,7,86,"['default', 'none']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
692,7,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose3d.yaml,2
693,7,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
694,7,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2
695,7,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
696,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
697,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
698,7,86,"['default', 'none']",Default: None,torch.nn.functional.conv_transpose1d.yaml,2
699,7,86,"['default', 'none']",Default: None,torch.nn.functional.avg_pool2d.yaml,2
700,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
701,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
702,7,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
703,7,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2
704,7,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
705,7,86,"['default', 'none']",Default: None,torch.hub.download_url_to_file.yaml,2
706,7,86,"['default', 'none']",Default: `None`,torch.nn.functional.conv1d.yaml,2
707,7,86,"['default', 'none']",Default: None.,torch.nn.functional.log_softmax.yaml,2
708,7,86,"['default', 'none']",Default: None,torch.nn.functional.conv3d.yaml,2
709,7,86,"['default', 'none']",Default: `None`,torch.lu.yaml,2
710,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
711,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
712,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
713,7,86,"['default', 'none']",Default: None.,torch.cumprod.yaml,2
714,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
715,7,86,"['default', 'none']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
716,7,86,"['default', 'none']",Default: `None` (treated as equal to `floor(n_fft / 4)`),torch.stft.yaml,2
717,7,86,"['default', 'none']",Default: `None`  (treated as equal to `n_fft`),torch.stft.yaml,2
718,7,86,"['default', 'none']",Default: `None` (treated as window of all 1  s),torch.stft.yaml,2
719,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
720,7,86,"['default', 'none']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
721,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
722,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
723,7,86,"['default', 'none']",Default: None.,torch.autograd.grad.yaml,2
724,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
725,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
726,7,86,"['default', 'none']",Default: None.,torch.prod.yaml,2
727,7,86,"['default', 'none']",Default: None.,torch.norm.yaml,2
728,7,86,"['default', 'none']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
729,7,86,"['default', 'none']",default: `None`,torch.unique.yaml,2
730,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
731,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
732,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
733,7,86,"['default', 'none']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2
734,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
735,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
736,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
737,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
738,7,86,"['default', 'none']",Default: None.,torch.nn.functional.softmin.yaml,2
739,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
740,7,86,"['default', 'none']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2
741,7,86,"['default', 'none']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
742,7,86,"['default', 'none']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2
743,7,86,"['default', 'none']",default: `None`,torch.unique_consecutive.yaml,2
744,7,86,"['default', 'none']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
745,7,86,"['default', 'none']",Default: None.,torch.cumsum.yaml,2
746,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
747,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
748,7,86,"['default', 'none']",Default: None.,torch.nn.functional.softmax.yaml,2
749,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
750,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
751,7,86,"['default', 'none']",Default: `None`,torch.matrix_rank.yaml,2
752,7,86,"['default', 'none']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
753,7,86,"['default', 'none']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2
754,7,86,"['default', 'none']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
755,7,86,"['default', 'none']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
756,7,86,"['default', 'none']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
757,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
758,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
759,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
760,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
761,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
762,8,74,"['SOME_DTYPE', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng.yaml,2
763,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,2
764,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2
765,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
766,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
767,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,2
768,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",dictionary that maps float modules to quantized modules to be replaced.,torch.quantization.prepare_qat.yaml,2
769,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
770,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
771,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
772,8,74,"['SOME_DTYPE', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng2.yaml,2
773,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
774,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
775,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
776,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,2
777,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2
778,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",a dictionary that maps from nn module to nnq module,torch.quantization.swap_module.yaml,2
779,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2
780,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
781,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","the inputs to the model, e.g., such that `model(*args)` is a valid invocation of the model.",torch.onnx.export.yaml,2
782,8,74,"['SOME_DTYPE', 'SOME_DTYPE']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
783,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by `model.state_dict().values()`",torch.onnx.export.yaml,2
784,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,2
785,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,2
786,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
787,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
788,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
789,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",device for which to return the device capability.,torch.cuda.get_device_capability.yaml,2
790,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
791,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,2
792,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
793,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
794,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
795,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
796,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
797,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
798,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the corresponding kwargs for callable model.,torch.hub.load.yaml,2
799,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",the corresponding args for callable model.,torch.hub.load.yaml,2
800,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
801,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
802,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",input module with qconfig attributes for all the leaf modules that we want to quantize,torch.quantization.add_observer_.yaml,2
803,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
804,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,2
805,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
806,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",offset in integer value that maps to float zero,torch.quantize_per_tensor.yaml,2
807,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2
808,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
809,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated.",torch.norm.yaml,2
810,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
811,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
812,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
813,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
814,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
815,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
816,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
817,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
818,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
819,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
820,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2
821,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",Model containing the modules to be fused,torch.quantization.fuse_modules.yaml,2
822,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
823,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
824,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
825,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
826,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
827,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",describes what to run in the forward pass of the model or part of the model.,torch.utils.checkpoint.checkpoint.yaml,2
828,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,2
829,8,74,"['SOME_DTYPE', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2
830,8,74,"['SOME_DTYPE', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
831,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool3d.yaml,2
832,9,61,"['default', 'false']",Default: `False`.,torch.empty_strided.yaml,2
833,9,61,"['default', 'false']",Default: `False`.,torch.empty_strided.yaml,2
834,9,61,"['default', 'false']",Default: `False`.,torch.arange.yaml,2
835,9,61,"['default', 'false']",Default: `False`,torch.cholesky.yaml,2
836,9,61,"['default', 'false']",Default: `False`,torch.nn.quantized.functional.avg_pool2d.yaml,2
837,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.grid_sample.yaml,2
838,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.avg_pool1d.yaml,2
839,9,61,"['default', 'false']",Default: `False`.,torch.ones_like.yaml,2
840,9,61,"['default', 'false']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2
841,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool1d.yaml,2
842,9,61,"['default', 'false']",Default: `False`,torch.irfft.yaml,2
843,9,61,"['default', 'false']",Default: `False`.,torch.empty_like.yaml,2
844,9,61,"['default', 'false']",Default: `False`.,torch.hann_window.yaml,2
845,9,61,"['default', 'false']",Default `False`.,torch.nn.functional.embedding.yaml,2
846,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.affine_grid.yaml,2
847,9,61,"['default', 'false']",Default: `False`.,torch.zeros_like.yaml,2
848,9,61,"['default', 'false']",Default: `False`.,torch.rand.yaml,2
849,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.adaptive_max_pool2d.yaml,2
850,9,61,"['default', 'false']",Default: `False`,torch.fft.yaml,2
851,9,61,"['default', 'false']",Default: `False`.,torch.triangular_solve.yaml,2
852,9,61,"['default', 'false']",Default: `False`.,torch.triangular_solve.yaml,2
853,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.avg_pool2d.yaml,2
854,9,61,"['default', 'false']",Default: `False`.,torch.eye.yaml,2
855,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout.yaml,2
856,9,61,"['default', 'false']",Default: `False`.,torch.randn_like.yaml,2
857,9,61,"['default', 'false']",Default: `False`,torch.ifft.yaml,2
858,9,61,"['default', 'false']",Default: `False`.,torch.cholesky_solve.yaml,2
859,9,61,"['default', 'false']",Default: `False`,torch.lu.yaml,2
860,9,61,"['default', 'false']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2
861,9,61,"['default', 'false']",Default: `False`.,torch.ones.yaml,2
862,9,61,"['default', 'false']",Default: `False`.,torch.randperm.yaml,2
863,9,61,"['default', 'false']",Default is False.,torch.hub.load.yaml,2
864,9,61,"['default', 'false']",Default: `False`.,torch.max2.yaml,2
865,9,61,"['default', 'false']",Default is False.,torch.hub.help.yaml,2
866,9,61,"['default', 'false']",Default: False,torch.utils.model_zoo.load_url.yaml,2
867,9,61,"['default', 'false']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2
868,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout2d.yaml,2
869,9,61,"['default', 'false']",Default: `False`.,torch.linspace.yaml,2
870,9,61,"['default', 'false']",Default: `False`.,torch.autograd.grad.yaml,2
871,9,61,"['default', 'false']",Default: `False`.,torch.hamming_window.yaml,2
872,9,61,"['default', 'false']",Default: `False`,torch.norm.yaml,2
873,9,61,"['default', 'false']",Default: `False`.,torch.blackman_window.yaml,2
874,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.dropout3d.yaml,2
875,9,61,"['default', 'false']",Default: `False`.,torch.bartlett_window.yaml,2
876,9,61,"['default', 'false']",Default: `False`.,torch.logspace.yaml,2
877,9,61,"['default', 'false']",Default: `False`,torch.nn.quantized.functional.interpolate.yaml,2
878,9,61,"['default', 'false']",Default: `False`,torch.allclose.yaml,2
879,9,61,"['default', 'false']",Default: `False`.,torch.tensor.yaml,2
880,9,61,"['default', 'false']",Default: `False`.,torch.tensor.yaml,2
881,9,61,"['default', 'false']",Default: `False`.,torch.sparse_coo_tensor.yaml,2
882,9,61,"['default', 'false']",Default: `False`.,torch.zeros.yaml,2
883,9,61,"['default', 'false']",Default: `False`.,torch.randn.yaml,2
884,9,61,"['default', 'false']",Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .,torch.nn.functional.poisson_nll_loss.yaml,2
885,9,61,"['default', 'false']",Default: `False`,torch.matrix_rank.yaml,2
886,9,61,"['default', 'false']",Default is False.,torch.hub.list.yaml,2
887,9,61,"['default', 'false']",Default: `False`.,torch.rand_like.yaml,2
888,9,61,"['default', 'false']",Default: `False`,torch.nn.functional.interpolate.yaml,2
889,9,61,"['default', 'false']",Default: False,torch.hub.load_state_dict_from_url.yaml,2
890,9,61,"['default', 'false']",Default: `False`.,torch.full.yaml,2
891,9,61,"['default', 'false']",Default: `False`,torch.rfft.yaml,2
892,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
893,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
894,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.empty_strided.yaml,2
895,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
896,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
897,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.arange.yaml,2
898,10,57,"['default', 'torch']",Default: `torch.preserve_format`.,torch.ones_like.yaml,2
899,10,57,"['default', 'torch']",Default: `torch.preserve_format`.,torch.empty_like.yaml,2
900,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
901,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
902,10,57,"['default', 'torch']",Default: `torch.preserve_format`.,torch.zeros_like.yaml,2
903,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
904,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
905,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.rand.yaml,2
906,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
907,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
908,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.eye.yaml,2
909,10,57,"['default', 'torch']",Default: `torch.preserve_format`.,torch.randn_like.yaml,2
910,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
911,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
912,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.ones.yaml,2
913,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
914,10,57,"['default', 'torch']",Default: `torch.int64`.,torch.randperm.yaml,2
915,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.randperm.yaml,2
916,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
917,10,57,"['default', 'torch']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
918,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
919,10,57,"['default', 'torch']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
920,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
921,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
922,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.linspace.yaml,2
923,10,57,"['default', 'torch']",Default: `torch.quint8`,torch.nn.quantized.functional.conv2d.yaml,2
924,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
925,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
926,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
927,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
928,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
929,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
930,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
931,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
932,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
933,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.logspace.yaml,2
934,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
935,10,57,"['default', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
936,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
937,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
938,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.zeros.yaml,2
939,10,57,"['default', 'torch']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
940,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
941,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
942,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.randn.yaml,2
943,10,57,"['default', 'torch']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
944,10,57,"['default', 'torch']",Default: `torch.preserve_format`.,torch.rand_like.yaml,2
945,10,57,"['default', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
946,10,57,"['default', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
947,10,57,"['default', 'torch']",Default: `torch.strided`.,torch.full.yaml,2
948,10,57,"['default', 'torch']",Default: `torch.quint8`,torch.nn.quantized.functional.conv3d.yaml,2
949,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
950,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
951,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
952,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
953,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
954,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
955,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
956,11,57,"['current', 'SOME_DTYPE']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
957,11,57,"['current', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
958,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
959,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
960,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
961,11,57,"['current', 'SOME_DTYPE']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
962,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
963,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
964,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
965,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
966,11,57,"['current', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
967,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
968,11,57,"['current', 'SOME_DTYPE']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
969,11,57,"['current', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
970,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
971,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
972,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
973,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
974,11,57,"['current', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
975,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
976,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
977,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
978,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
979,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
980,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
981,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
982,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
983,11,57,"['current', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
984,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
985,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
986,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
987,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
988,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
989,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
990,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
991,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
992,11,57,"['current', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
993,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
994,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
995,11,57,"['current', 'SOME_DTYPE']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
996,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
997,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
998,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
999,11,57,"['current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
1000,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1001,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
1002,11,57,"['current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
1003,11,57,"['current', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2
1004,11,57,"['current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1005,11,57,"['current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
1006,12,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
1007,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.prod2.yaml,2
1008,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.empty_strided.yaml,2
1009,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.arange.yaml,2
1010,12,54,"['data', 'type']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,2
1011,12,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
1012,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.sum2.yaml,2
1013,12,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
1014,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.sum.yaml,2
1015,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.ones_like.yaml,2
1016,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.empty_like.yaml,2
1017,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.hann_window.yaml,2
1018,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2
1019,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2
1020,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2
1021,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.rand.yaml,2
1022,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.eye.yaml,2
1023,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.randn_like.yaml,2
1024,12,54,"['data', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
1025,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2
1026,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.log_softmax.yaml,2
1027,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.ones.yaml,2
1028,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.randperm.yaml,2
1029,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.cumprod.yaml,2
1030,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.cumprod.yaml,2
1031,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.triu_indices.yaml,2
1032,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.tril_indices.yaml,2
1033,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.linspace.yaml,2
1034,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2
1035,12,54,"['data', 'type']",quantization data type to use.,torch.nn.quantized.functional.conv2d.yaml,2
1036,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.hamming_window.yaml,2
1037,12,54,"['data', 'type']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
1038,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.prod.yaml,2
1039,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.norm.yaml,2
1040,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.blackman_window.yaml,2
1041,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.as_tensor.yaml,2
1042,12,54,"['data', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2
1043,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2
1044,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.logspace.yaml,2
1045,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2
1046,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.softmin.yaml,2
1047,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.tensor.yaml,2
1048,12,54,"['data', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2
1049,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2
1050,12,54,"['data', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2
1051,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.cumsum.yaml,2
1052,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.cumsum.yaml,2
1053,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.zeros.yaml,2
1054,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2
1055,12,54,"['data', 'type']",This is useful for preventing data type overflows.,torch.nn.functional.softmax.yaml,2
1056,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.randn.yaml,2
1057,12,54,"['data', 'type']",the desired data type of returned Tensor.,torch.rand_like.yaml,2
1058,12,54,"['data', 'type']",the desired data type of returned tensor.,torch.full.yaml,2
1059,12,54,"['data', 'type']",quantization data type to use.,torch.nn.quantized.functional.conv3d.yaml,2
1060,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1061,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1062,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
1063,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1064,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1065,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
1066,13,51,"['none', 'default']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2
1067,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
1068,13,51,"['none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
1069,13,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
1070,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
1071,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1072,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1073,13,51,"['none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
1074,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1075,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1076,13,51,"['none', 'default']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
1077,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1078,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1079,13,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
1080,13,51,"['none', 'default']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2
1081,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
1082,13,51,"['none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
1083,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1084,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1085,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1086,13,51,"['none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
1087,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1088,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1089,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1090,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1091,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1092,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1093,13,51,"['none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
1094,13,51,"['none', 'default']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2
1095,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1096,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1097,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1098,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1099,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1100,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1101,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1102,13,51,"['none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
1103,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1104,13,51,"['none', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1105,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1106,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1107,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1108,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1109,13,51,"['none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1110,13,51,"['none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1111,14,47,"['single', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
1112,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2
1113,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2
1114,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,2
1115,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2
1116,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,2
1117,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,2
1118,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1119,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1120,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1121,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1122,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,2
1123,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,2
1124,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,2
1125,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,2
1126,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,2
1127,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,2
1128,14,47,"['single', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
1129,14,47,"['single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
1130,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,2
1131,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1132,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1133,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1134,14,47,"['single', 'SOME_STRUCTURE']",size of a single chunk or list of sizes for each chunk,torch.split.yaml,2
1135,14,47,"['single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
1136,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1137,14,47,"['single', 'SOME_STRUCTURE']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,2
1138,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1139,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1140,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2
1141,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,2
1142,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,2
1143,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2
1144,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2
1145,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2
1146,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,2
1147,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,2
1148,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,2
1149,14,47,"['single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
1150,14,47,"['single', 'SOME_STRUCTURE']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2
1151,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,2
1152,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,2
1153,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,2
1154,14,47,"['single', 'SOME_STRUCTURE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
1155,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,2
1156,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,2
1157,14,47,"['single', 'SOME_STRUCTURE']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,2
1158,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1159,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1160,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
1161,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
1162,15,46,"['default', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng.yaml,2
1163,15,46,"['default', 'SOME_DTYPE']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2
1164,15,46,"['default', 'SOME_DTYPE']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2
1165,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
1166,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
1167,15,46,"['default', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng2.yaml,2
1168,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1169,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
1170,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
1171,15,46,"['default', 'SOME_DTYPE']",by default we export the model to the opset version of the onnx submodule.,torch.onnx.export.yaml,2
1172,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1173,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1174,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
1175,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
1176,15,46,"['default', 'SOME_DTYPE']","(any one of default, short, full)",torch.set_printoptions.yaml,2
1177,15,46,"['default', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
1178,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1179,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1180,15,46,"['default', 'SOME_DTYPE']",Default: `torch.int64`.,torch.randperm.yaml,2
1181,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1182,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
1183,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1184,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
1185,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1186,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1187,15,46,"['default', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
1188,15,46,"['default', 'SOME_DTYPE']","If `True` (default), imports the produced shared library as a Python module.",torch.utils.cpp_extension.load.yaml,2
1189,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1190,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1191,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1192,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1193,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1194,15,46,"['default', 'SOME_DTYPE']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1195,15,46,"['default', 'SOME_DTYPE']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2
1196,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1197,15,46,"['default', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
1198,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1199,15,46,"['default', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
1200,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
1201,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
1202,15,46,"['default', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2
1203,15,46,"['default', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1204,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.empty_strided.yaml,2
1205,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.empty_strided.yaml,2
1206,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.arange.yaml,2
1207,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.arange.yaml,2
1208,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.ones_like.yaml,2
1209,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned tensor.,torch.ones_like.yaml,2
1210,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.empty_like.yaml,2
1211,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned tensor.,torch.empty_like.yaml,2
1212,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.hann_window.yaml,2
1213,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned window tensor.,torch.hann_window.yaml,2
1214,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.zeros_like.yaml,2
1215,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned tensor.,torch.zeros_like.yaml,2
1216,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.rand.yaml,2
1217,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.rand.yaml,2
1218,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.eye.yaml,2
1219,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.eye.yaml,2
1220,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.randn_like.yaml,2
1221,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned tensor.,torch.randn_like.yaml,2
1222,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.ones.yaml,2
1223,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.ones.yaml,2
1224,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.randperm.yaml,2
1225,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.randperm.yaml,2
1226,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.triu_indices.yaml,2
1227,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.tril_indices.yaml,2
1228,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.linspace.yaml,2
1229,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.linspace.yaml,2
1230,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.hamming_window.yaml,2
1231,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2
1232,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.blackman_window.yaml,2
1233,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2
1234,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.as_tensor.yaml,2
1235,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.bartlett_window.yaml,2
1236,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2
1237,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.logspace.yaml,2
1238,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.logspace.yaml,2
1239,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.tensor.yaml,2
1240,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,2
1241,16,46,"['SOME_DTYPE', 'returned']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2
1242,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.zeros.yaml,2
1243,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.zeros.yaml,2
1244,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.randn.yaml,2
1245,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.randn.yaml,2
1246,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.rand_like.yaml,2
1247,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned tensor.,torch.rand_like.yaml,2
1248,16,46,"['SOME_DTYPE', 'returned']",the desired device of returned tensor.,torch.full.yaml,2
1249,16,46,"['SOME_DTYPE', 'returned']",the desired layout of returned Tensor.,torch.full.yaml,2
1250,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_strided.yaml,4
1251,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.empty_strided.yaml,4
1252,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.arange.yaml,4
1253,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.arange.yaml,4
1254,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones_like.yaml,4
1255,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned tensor.,torch.ones_like.yaml,4
1256,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.empty_like.yaml,4
1257,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned tensor.,torch.empty_like.yaml,4
1258,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.hann_window.yaml,4
1259,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hann_window.yaml,4
1260,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros_like.yaml,4
1261,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned tensor.,torch.zeros_like.yaml,4
1262,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand.yaml,4
1263,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.rand.yaml,4
1264,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.eye.yaml,4
1265,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.eye.yaml,4
1266,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn_like.yaml,4
1267,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned tensor.,torch.randn_like.yaml,4
1268,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.ones.yaml,4
1269,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.ones.yaml,4
1270,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.randperm.yaml,4
1271,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randperm.yaml,4
1272,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.triu_indices.yaml,4
1273,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.tril_indices.yaml,4
1274,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.linspace.yaml,4
1275,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.linspace.yaml,4
1276,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.hamming_window.yaml,4
1277,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.hamming_window.yaml,4
1278,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.blackman_window.yaml,4
1279,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.blackman_window.yaml,4
1280,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.as_tensor.yaml,4
1281,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.bartlett_window.yaml,4
1282,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,4
1283,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.logspace.yaml,4
1284,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.logspace.yaml,4
1285,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.tensor.yaml,4
1286,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.sparse_coo_tensor.yaml,4
1287,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.zeros.yaml,4
1288,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.zeros.yaml,4
1289,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.randn.yaml,4
1290,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.randn.yaml,4
1291,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.rand_like.yaml,4
1292,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned tensor.,torch.rand_like.yaml,4
1293,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired device of returned tensor.,torch.full.yaml,4
1294,17,45,"['desired', 'SOME_DTYPE', 'returned', 'tensor']",the desired layout of returned Tensor.,torch.full.yaml,4
1295,18,43,"['tensor', 'tensor']",multiplier for tensor1 / tensor2 ,torch.addcdiv.yaml,2
1296,18,43,"['tensor', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
1297,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
1298,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
1299,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
1300,18,43,"['tensor', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
1301,18,43,"['tensor', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
1302,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
1303,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
1304,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
1305,18,43,"['tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2
1306,18,43,"['tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2
1307,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
1308,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
1309,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
1310,18,43,"['tensor', 'tensor']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
1311,18,43,"['tensor', 'tensor']"," If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2
1312,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
1313,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
1314,18,43,"['tensor', 'tensor']",multiplier for tensor1 .* tensor2 ,torch.addcmul.yaml,2
1315,18,43,"['tensor', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2
1316,18,43,"['tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2
1317,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
1318,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
1319,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
1320,18,43,"['tensor', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2
1321,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
1322,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
1323,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
1324,18,43,"['tensor', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
1325,18,43,"['tensor', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
1326,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
1327,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
1328,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
1329,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
1330,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
1331,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
1332,18,43,"['tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
1333,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
1334,18,43,"['tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2
1335,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
1336,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
1337,18,43,"['tensor', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
1338,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2
1339,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2
1340,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,2
1341,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2
1342,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,2
1343,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,2
1344,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1345,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1346,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1347,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,2
1348,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,2
1349,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,2
1350,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,2
1351,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,2
1352,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,2
1353,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,2
1354,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,2
1355,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1356,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1357,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,2
1358,19,43,"['number', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.rand.yaml,2
1359,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1360,19,43,"['number', 'SOME_STRUCTURE']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,2
1361,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1362,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,2
1363,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2
1364,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,2
1365,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,2
1366,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2
1367,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2
1368,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2
1369,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,2
1370,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,2
1371,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,2
1372,19,43,"['number', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.ones.yaml,2
1373,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,2
1374,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,2
1375,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,2
1376,19,43,"['number', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.zeros.yaml,2
1377,19,43,"['number', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.randn.yaml,2
1378,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,2
1379,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,2
1380,19,43,"['number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,2
1381,20,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
1382,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,2
1383,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,2
1384,20,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
1385,20,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
1386,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,2
1387,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,2
1388,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,2
1389,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,2
1390,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,2
1391,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,2
1392,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,2
1393,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,2
1394,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,2
1395,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,2
1396,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,2
1397,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,2
1398,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,2
1399,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,2
1400,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,2
1401,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,2
1402,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,2
1403,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,2
1404,20,42,"['data', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
1405,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,2
1406,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,2
1407,20,42,"['data', 'tensor']",Initial data for the tensor.,torch.as_tensor.yaml,2
1408,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,2
1409,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,2
1410,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,2
1411,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,2
1412,20,42,"['data', 'tensor']",Initial data for the tensor.,torch.tensor.yaml,2
1413,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,2
1414,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,2
1415,20,42,"['data', 'tensor']",Initial data for the tensor.,torch.sparse_coo_tensor.yaml,2
1416,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,2
1417,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,2
1418,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,2
1419,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,2
1420,20,42,"['data', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,2
1421,20,42,"['data', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,2
1422,20,42,"['data', 'tensor']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2
1423,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1424,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
1425,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1426,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
1427,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1428,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
1429,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1430,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
1431,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1432,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
1433,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1434,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
1435,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1436,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
1437,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1438,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
1439,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1440,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
1441,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1442,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
1443,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1444,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
1445,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1446,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
1447,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1448,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
1449,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1450,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
1451,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1452,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
1453,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1454,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
1455,21,41,"['current', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1456,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
1457,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1458,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
1459,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1460,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
1461,21,41,"['current', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1462,21,41,"['current', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
1463,21,41,"['current', 'tensor']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2
1464,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
1465,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
1466,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
1467,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
1468,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
1469,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
1470,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
1471,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
1472,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
1473,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
1474,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
1475,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
1476,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
1477,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
1478,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
1479,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
1480,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
1481,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
1482,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
1483,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
1484,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
1485,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
1486,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
1487,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
1488,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
1489,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
1490,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
1491,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
1492,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
1493,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
1494,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
1495,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
1496,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
1497,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
1498,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
1499,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
1500,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
1501,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
1502,22,40,"['current', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
1503,22,40,"['current', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
1504,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,3
1505,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,3
1506,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,3
1507,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,3
1508,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,3
1509,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,3
1510,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,3
1511,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padH, out_padW)`.",torch.nn.functional.conv_transpose2d.yaml,3
1512,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,3
1513,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,3
1514,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kW,)",torch.nn.functional.avg_pool1d.yaml,3
1515,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,3
1516,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,3
1517,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,3
1518,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,3
1519,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,3
1520,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,3
1521,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(out_padT, out_padH, out_padW)`.",torch.nn.functional.conv_transpose3d.yaml,3
1522,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,3
1523,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,3
1524,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,3
1525,23,39,"['single', 'number', 'SOME_STRUCTURE']",Can be a single number or a tuple `(out_padW)`.,torch.nn.functional.conv_transpose1d.yaml,3
1526,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,3
1527,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,3
1528,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,3
1529,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,3
1530,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,3
1531,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,3
1532,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,3
1533,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,3
1534,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,3
1535,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,3
1536,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,3
1537,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,3
1538,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,3
1539,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,3
1540,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,3
1541,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,3
1542,23,39,"['single', 'number', 'SOME_STRUCTURE']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,3
1543,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1544,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1545,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1546,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1547,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1548,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1549,24,39,"['see', 'torch']",See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.,torch.nn.functional.embedding.yaml,2
1550,24,39,"['see', 'torch']",See documentation of valid entries for argument `p` in `torch.norm()`.,torch.nn.utils.prune.ln_structured.yaml,2
1551,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1552,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1553,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1554,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1555,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1556,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1557,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1558,24,39,"['see', 'torch']",a function or a dict specifying how to remap storage locations (see torch.load),torch.utils.model_zoo.load_url.yaml,2
1559,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1560,24,39,"['see', 'torch']","When a module is passed to `torch.jit.trace`, only the `forward` method is run and traced (see `torch.jit.trace` for details).",torch.jit.trace.yaml,2
1561,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1562,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1563,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1564,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1565,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1566,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1567,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1568,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1569,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1570,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1571,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1572,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1573,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1574,24,39,"['see', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1575,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1576,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1577,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1578,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1579,24,39,"['see', 'torch']",a function or a dict specifying how to remap storage locations (see torch.load),torch.hub.load_state_dict_from_url.yaml,2
1580,24,39,"['see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1581,24,39,"['see', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1582,25,38,"['SOME_STRUCTURE', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
1583,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of 2 or more 2-D tensors whose product is to be determined.,torch.chain_matmul.yaml,2
1584,25,38,"['SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
1585,25,38,"['SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
1586,25,38,"['SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
1587,25,38,"['SOME_STRUCTURE', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2
1588,25,38,"['SOME_STRUCTURE', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2
1589,25,38,"['SOME_STRUCTURE', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
1590,25,38,"['SOME_STRUCTURE', 'tensor']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
1591,25,38,"['SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
1592,25,38,"['SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
1593,25,38,"['SOME_STRUCTURE', 'tensor']"," If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2
1594,25,38,"['SOME_STRUCTURE', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2
1595,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
1596,25,38,"['SOME_STRUCTURE', 'tensor']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,2
1597,25,38,"['SOME_STRUCTURE', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2
1598,25,38,"['SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2
1599,25,38,"['SOME_STRUCTURE', 'tensor']",List of tensors(on different GPUs) to be broadcast from current process.,torch.distributed.all_gather_multigpu.yaml,2
1600,25,38,"['SOME_STRUCTURE', 'tensor']","If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor.",torch.lu.yaml,2
1601,25,38,"['SOME_STRUCTURE', 'tensor']","If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor.",torch.lu.yaml,2
1602,25,38,"['SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
1603,25,38,"['SOME_STRUCTURE', 'tensor']",an iterable of tensors to add.,torch.cuda.comm.reduce_add.yaml,2
1604,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
1605,25,38,"['SOME_STRUCTURE', 'tensor']",sequence of tensors to concatenate,torch.stack.yaml,2
1606,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
1607,25,38,"['SOME_STRUCTURE', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2
1608,25,38,"['SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
1609,25,38,"['SOME_STRUCTURE', 'tensor']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2
1610,25,38,"['SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
1611,25,38,"['SOME_STRUCTURE', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
1612,25,38,"['SOME_STRUCTURE', 'tensor']",any python sequence of tensors of the same type.,torch.cat.yaml,2
1613,25,38,"['SOME_STRUCTURE', 'tensor']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
1614,25,38,"['SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2
1615,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
1616,25,38,"['SOME_STRUCTURE', 'tensor']",the output tuple of tensors,torch.svd.yaml,2
1617,25,38,"['SOME_STRUCTURE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
1618,25,38,"['SOME_STRUCTURE', 'tensor']",iterable of tensors to gather.,torch.cuda.comm.gather.yaml,2
1619,25,38,"['SOME_STRUCTURE', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
1620,26,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,5
1621,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.empty_strided.yaml,5
1622,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.arange.yaml,5
1623,26,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,5
1624,26,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,5
1625,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.ones_like.yaml,5
1626,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.empty_like.yaml,5
1627,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hann_window.yaml,5
1628,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.sparse.sum.yaml,5
1629,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_channel.yaml,5
1630,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.zeros_like.yaml,5
1631,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.rand.yaml,5
1632,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.eye.yaml,5
1633,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.randn_like.yaml,5
1634,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.log_softmax.yaml,5
1635,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.ones.yaml,5
1636,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randperm.yaml,5
1637,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumprod.yaml,5
1638,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.triu_indices.yaml,5
1639,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tril_indices.yaml,5
1640,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.linspace.yaml,5
1641,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.quantize_per_tensor.yaml,5
1642,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.hamming_window.yaml,5
1643,26,38,"['desired', 'data', 'type', 'returned', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,5
1644,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.norm.yaml,5
1645,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.blackman_window.yaml,5
1646,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.as_tensor.yaml,5
1647,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.bartlett_window.yaml,5
1648,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.logspace.yaml,5
1649,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmin.yaml,5
1650,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.tensor.yaml,5
1651,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.sparse_coo_tensor.yaml,5
1652,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.cumsum.yaml,5
1653,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.zeros.yaml,5
1654,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.nn.functional.softmax.yaml,5
1655,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.randn.yaml,5
1656,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned Tensor.,torch.rand_like.yaml,5
1657,26,38,"['desired', 'data', 'type', 'returned', 'tensor']",the desired data type of returned tensor.,torch.full.yaml,5
1658,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1659,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
1660,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1661,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
1662,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
1663,27,37,"['SOME_DTYPE', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
1664,27,37,"['SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
1665,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
1666,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1667,27,37,"['SOME_DTYPE', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
1668,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1669,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1670,27,37,"['SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
1671,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
1672,27,37,"['SOME_DTYPE', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
1673,27,37,"['SOME_DTYPE', 'default']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
1674,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1675,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1676,27,37,"['SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
1677,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1678,27,37,"['SOME_DTYPE', 'default']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2
1679,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1680,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1681,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1682,27,37,"['SOME_DTYPE', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
1683,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1684,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1685,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1686,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1687,27,37,"['SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
1688,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1689,27,37,"['SOME_DTYPE', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1690,27,37,"['SOME_DTYPE', 'default']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2
1691,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1692,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1693,27,37,"['SOME_DTYPE', 'default']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2
1694,27,37,"['SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1695,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1696,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1697,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1698,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1699,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1700,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1701,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1702,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1703,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1704,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1705,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1706,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1707,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1708,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1709,28,37,"['none', 'torch']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
1710,28,37,"['none', 'torch']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
1711,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1712,28,37,"['none', 'torch']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
1713,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1714,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1715,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1716,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1717,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1718,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1719,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1720,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1721,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1722,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1723,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1724,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1725,28,37,"['none', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1726,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1727,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1728,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1729,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1730,28,37,"['none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1731,28,37,"['none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1732,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1733,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1734,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1735,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1736,29,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
1737,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1738,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1739,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1740,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1741,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1742,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1743,29,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
1744,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1745,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1746,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1747,29,37,"['uses', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
1748,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1749,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1750,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1751,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1752,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1753,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1754,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1755,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1756,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1757,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1758,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1759,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1760,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1761,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1762,29,37,"['uses', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1763,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1764,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1765,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1766,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1767,29,37,"['uses', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1768,29,37,"['uses', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1769,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
1770,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
1771,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
1772,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
1773,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
1774,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
1775,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
1776,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
1777,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
1778,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
1779,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
1780,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
1781,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
1782,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
1783,30,36,"['default', 'none', 'torch']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,3
1784,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
1785,30,36,"['default', 'none', 'torch']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,3
1786,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
1787,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
1788,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
1789,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
1790,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
1791,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
1792,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
1793,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
1794,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
1795,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
1796,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
1797,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
1798,30,36,"['default', 'none', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
1799,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
1800,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
1801,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
1802,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
1803,30,36,"['default', 'none', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
1804,30,36,"['default', 'none', 'torch']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
1805,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
1806,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
1807,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
1808,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
1809,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
1810,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
1811,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
1812,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
1813,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
1814,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
1815,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
1816,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
1817,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
1818,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
1819,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
1820,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
1821,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
1822,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
1823,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
1824,31,35,"['default', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3
1825,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
1826,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
1827,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
1828,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
1829,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
1830,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
1831,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
1832,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
1833,31,35,"['default', 'none', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
1834,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
1835,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
1836,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
1837,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
1838,31,35,"['default', 'none', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
1839,31,35,"['default', 'none', 'default']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
1840,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1841,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1842,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1843,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1844,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1845,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1846,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1847,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1848,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1849,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1850,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1851,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1852,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1853,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1854,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1855,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1856,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1857,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1858,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1859,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1860,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1861,32,35,"['default', 'uses']",By default uses the same backend as the global group.,torch.distributed.new_group.yaml,2
1862,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1863,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1864,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1865,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1866,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1867,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1868,32,35,"['default', 'uses']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1869,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1870,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1871,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1872,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1873,32,35,"['default', 'uses']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1874,32,35,"['default', 'uses']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1875,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1876,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1877,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1878,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1879,33,35,"['default', 'see']","If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see `get_default_dtype()`.",torch.arange.yaml,2
1880,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1881,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1882,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1883,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1884,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1885,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1886,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1887,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1888,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1889,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1890,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1891,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1892,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1893,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1894,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1895,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1896,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1897,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1898,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1899,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1900,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1901,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1902,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1903,33,35,"['default', 'see']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1904,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1905,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1906,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1907,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1908,33,35,"['default', 'see']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1909,33,35,"['default', 'see']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1910,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
1911,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
1912,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
1913,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
1914,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
1915,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
1916,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
1917,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
1918,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
1919,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
1920,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
1921,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
1922,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
1923,34,35,"['none', 'SOME_DTYPE']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2
1924,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
1925,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
1926,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
1927,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
1928,34,35,"['none', 'SOME_DTYPE']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
1929,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
1930,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
1931,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
1932,34,35,"['none', 'SOME_DTYPE']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2
1933,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
1934,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
1935,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
1936,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
1937,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
1938,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
1939,34,35,"['none', 'SOME_DTYPE']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
1940,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
1941,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
1942,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
1943,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
1944,34,35,"['none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
1945,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,7
1946,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,7
1947,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,7
1948,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,7
1949,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,7
1950,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,7
1951,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,7
1952,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,7
1953,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,7
1954,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,7
1955,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,7
1956,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,7
1957,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,7
1958,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,7
1959,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,7
1960,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,7
1961,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,7
1962,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,7
1963,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,7
1964,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,7
1965,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,7
1966,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,7
1967,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,7
1968,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,7
1969,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,7
1970,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,7
1971,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,7
1972,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,7
1973,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,7
1974,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,7
1975,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,7
1976,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,7
1977,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,7
1978,35,34,"['default', 'none', 'uses', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,7
1979,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
1980,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
1981,36,33,"['tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,2
1982,36,33,"['tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,2
1983,36,33,"['tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,2
1984,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
1985,36,33,"['tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
1986,36,33,"['tensor', 'SOME_DTYPE']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
1987,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
1988,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
1989,36,33,"['tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
1990,36,33,"['tensor', 'SOME_DTYPE']",an iterator of Tensors that are the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2
1991,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
1992,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
1993,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
1994,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
1995,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
1996,36,33,"['tensor', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
1997,36,33,"['tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,2
1998,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
1999,36,33,"['tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,2
2000,36,33,"['tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
2001,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2002,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2003,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2004,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2005,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2006,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2007,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2008,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2009,36,33,"['tensor', 'SOME_DTYPE']",an iterator of Tensors that are the parameters of a model.,torch.nn.utils.parameters_to_vector.yaml,2
2010,36,33,"['tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2011,36,33,"['tensor', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
2012,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2013,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,3
2014,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2015,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,3
2016,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,3
2017,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,3
2018,37,33,"['current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,3
2019,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,3
2020,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2021,37,33,"['current', 'SOME_DTYPE', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,3
2022,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2023,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2024,37,33,"['current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,3
2025,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,3
2026,37,33,"['current', 'SOME_DTYPE', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,3
2027,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2028,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2029,37,33,"['current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,3
2030,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2031,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2032,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2033,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2034,37,33,"['current', 'SOME_DTYPE', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3
2035,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2036,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2037,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2038,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2039,37,33,"['current', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,3
2040,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2041,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2042,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2043,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2044,37,33,"['current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2045,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2046,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2047,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,3
2048,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,3
2049,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,3
2050,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,3
2051,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2052,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,3
2053,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,3
2054,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2055,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2056,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,3
2057,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,3
2058,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2059,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2060,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2061,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,3
2062,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2063,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,3
2064,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2065,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2066,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2067,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2068,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2069,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2070,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2071,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2072,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2073,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2074,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,3
2075,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,3
2076,38,32,"['default', 'none', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2077,39,32,"['default', 'true']",Default: `True`,torch.nn.quantized.functional.avg_pool2d.yaml,2
2078,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.avg_pool1d.yaml,2
2079,39,32,"['default', 'true']",Default: `True`.,torch.nn.utils.rnn.pack_padded_sequence.yaml,2
2080,39,32,"['default', 'true']",Default: `True`,torch.irfft.yaml,2
2081,39,32,"['default', 'true']",Default: `True`.,torch.triangular_solve.yaml,2
2082,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.kl_div.yaml,2
2083,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.kl_div.yaml,2
2084,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.avg_pool2d.yaml,2
2085,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout.yaml,2
2086,39,32,"['default', 'true']",whether or not to display a progress bar to stderr Default: True,torch.hub.download_url_to_file.yaml,2
2087,39,32,"['default', 'true']",Default is True.,torch.lobpcg.yaml,2
2088,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy.yaml,2
2089,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy.yaml,2
2090,39,32,"['default', 'true']",Default: `True`.,torch.nn.utils.rnn.pack_sequence.yaml,2
2091,39,32,"['default', 'true']",Default: `True`,torch.lu.yaml,2
2092,39,32,"['default', 'true']",Default is True.,torch.hub.load.yaml,2
2093,39,32,"['default', 'true']",Default: True,torch.utils.model_zoo.load_url.yaml,2
2094,39,32,"['default', 'true']",Default: `True`,torch.stft.yaml,2
2095,39,32,"['default', 'true']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2
2096,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout2d.yaml,2
2097,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.nll_loss.yaml,2
2098,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.nll_loss.yaml,2
2099,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.dropout3d.yaml,2
2100,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.cross_entropy.yaml,2
2101,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.cross_entropy.yaml,2
2102,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
2103,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
2104,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2
2105,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2
2106,39,32,"['default', 'true']",Default: `True`,torch.nn.functional.poisson_nll_loss.yaml,2
2107,39,32,"['default', 'true']",Default: True,torch.hub.load_state_dict_from_url.yaml,2
2108,39,32,"['default', 'true']",Default: `True`,torch.rfft.yaml,2
2109,40,32,"['input', 'size']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
2110,40,32,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
2111,40,32,"['input', 'size']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2
2112,40,32,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
2113,40,32,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
2114,40,32,"['input', 'size']",Second input (of size matching x1).,torch.nn.functional.cosine_similarity.yaml,2
2115,40,32,"['input', 'size']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
2116,40,32,"['input', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
2117,40,32,"['input', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
2118,40,32,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
2119,40,32,"['input', 'size']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
2120,40,32,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
2121,40,32,"['input', 'size']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
2122,40,32,"['input', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
2123,40,32,"['input', 'size']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
2124,40,32,"['input', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
2125,40,32,"['input', 'size']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
2126,40,32,"['input', 'size']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
2127,40,32,"['input', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
2128,40,32,"['input', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
2129,40,32,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
2130,40,32,"['input', 'size']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
2131,40,32,"['input', 'size']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
2132,40,32,"['input', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
2133,40,32,"['input', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
2134,40,32,"['input', 'size']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2
2135,40,32,"['input', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
2136,40,32,"['input', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
2137,40,32,"['input', 'size']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
2138,40,32,"['input', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
2139,40,32,"['input', 'size']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2
2140,40,32,"['input', 'size']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
2141,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
2142,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
2143,41,28,"['tensor', 'shape']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2
2144,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
2145,41,28,"['tensor', 'shape']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
2146,41,28,"['tensor', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,2
2147,41,28,"['tensor', 'shape']","the divisor, which may be either a number or a tensor of the same shape as the dividend",torch.fmod.yaml,2
2148,41,28,"['tensor', 'shape']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
2149,41,28,"['tensor', 'shape']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
2150,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
2151,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
2152,41,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.kl_div.yaml,2
2153,41,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.kl_div.yaml,2
2154,41,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.binary_cross_entropy.yaml,2
2155,41,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy.yaml,2
2156,41,28,"['tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2
2157,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
2158,41,28,"['tensor', 'shape']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2
2159,41,28,"['tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
2160,41,28,"['tensor', 'shape']",the divisor that may be either a number or a Tensor of the same shape as the dividend,torch.remainder.yaml,2
2161,41,28,"['tensor', 'shape']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2
2162,41,28,"['tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
2163,41,28,"['tensor', 'shape']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2
2164,41,28,"['tensor', 'shape']",Tensor of arbitrary shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
2165,41,28,"['tensor', 'shape']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
2166,41,28,"['tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
2167,41,28,"['tensor', 'shape']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2
2168,41,28,"['tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
2169,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,2
2170,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2
2171,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",dictionary that maps float modules to quantized modules to be replaced.,torch.quantization.prepare_qat.yaml,2
2172,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
2173,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",an iterable of devices among which to broadcast.,torch.cuda.comm.broadcast_coalesced.yaml,2
2174,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a dictionary that maps from nn module to nnq module,torch.quantization.swap_module.yaml,2
2175,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,2
2176,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
2177,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
2178,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,2
2179,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
2180,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
2181,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
2182,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",an iterable of devices among which to broadcast.,torch.cuda.comm.broadcast.yaml,2
2183,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated.",torch.norm.yaml,2
2184,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
2185,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.as_tensor.yaml,2
2186,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
2187,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.tensor.yaml,2
2188,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
2189,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
2190,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
2191,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",list of list of module names to fuse.,torch.quantization.fuse_modules.yaml,2
2192,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
2193,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
2194,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
2195,42,27,"['SOME_STRUCTURE', 'SOME_DTYPE']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
2196,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2197,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2198,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2199,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2200,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2201,43,26,"['tensor', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
2202,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2203,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2204,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2205,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2206,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2207,43,26,"['tensor', 'type']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
2208,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2209,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2210,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2211,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2212,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2213,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2214,43,26,"['tensor', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2215,43,26,"['tensor', 'type']",any python sequence of tensors of the same type.,torch.cat.yaml,2
2216,43,26,"['tensor', 'type']",any number of tensors of the same type,torch.broadcast_tensors.yaml,2
2217,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2218,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2219,43,26,"['tensor', 'type']",the floating point tensor type or its name,torch.set_default_tensor_type.yaml,2
2220,43,26,"['tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2221,43,26,"['tensor', 'type']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
2222,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2223,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2224,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2225,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2226,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2227,44,25,"['default', 'current', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,3
2228,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2229,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2230,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2231,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2232,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2233,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2234,44,25,"['default', 'current', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3
2235,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2236,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2237,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2238,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2239,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2240,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2241,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2242,44,25,"['default', 'current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
2243,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2244,44,25,"['default', 'current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
2245,44,25,"['default', 'current', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,3
2246,44,25,"['default', 'current', 'SOME_DTYPE']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2247,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
2248,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
2249,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
2250,45,25,"['SOME_DTYPE', 'types']",correspondence between original module types and quantized counterparts,torch.quantization.quantize.yaml,2
2251,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
2252,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
2253,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
2254,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
2255,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
2256,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
2257,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
2258,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
2259,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2260,45,25,"['SOME_DTYPE', 'types']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.as_tensor.yaml,2
2261,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2262,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2263,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2264,45,25,"['SOME_DTYPE', 'types']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.tensor.yaml,2
2265,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2266,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2267,45,25,"['SOME_DTYPE', 'types']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
2268,45,25,"['SOME_DTYPE', 'types']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
2269,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2270,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2271,45,25,"['SOME_DTYPE', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2272,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2273,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2274,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2275,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2276,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2277,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2278,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2279,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2280,46,25,"['type', 'torch']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
2281,46,25,"['type', 'torch']",Quantized input of type torch.quint8,torch.nn.quantized.functional.linear.yaml,2
2282,46,25,"['type', 'torch']",Quantized weight of type torch.qint8,torch.nn.quantized.functional.linear.yaml,2
2283,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2284,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2285,46,25,"['type', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
2286,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2287,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2288,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2289,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2290,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2291,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2292,46,25,"['type', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2293,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2294,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2295,46,25,"['type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2296,46,25,"['type', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
2297,47,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,2
2298,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
2299,47,25,"['shape', '_channels']","filters of shape (in _channels ,  out _channels/groups , kH , kW) ",torch.nn.functional.conv_transpose2d.yaml,2
2300,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
2301,47,25,"['shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2
2302,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
2303,47,25,"['shape', '_channels']","filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.functional.conv2d.yaml,2
2304,47,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,2
2305,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
2306,47,25,"['shape', '_channels']","filters of shape (in _channels ,  out _channels/groups , kT , kH , kW) ",torch.nn.functional.conv_transpose3d.yaml,2
2307,47,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,2
2308,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
2309,47,25,"['shape', '_channels']","filters of shape (in _channels ,  out _channels/groups , kW) ",torch.nn.functional.conv_transpose1d.yaml,2
2310,47,25,"['shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,2
2311,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
2312,47,25,"['shape', '_channels']","filters of shape (out _channels ,  in _channels/groups , kW) ",torch.nn.functional.conv1d.yaml,2
2313,47,25,"['shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2
2314,47,25,"['shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
2315,47,25,"['shape', '_channels']","filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",torch.nn.functional.conv3d.yaml,2
2316,47,25,"['shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2
2317,47,25,"['shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
2318,47,25,"['shape', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,2
2319,47,25,"['shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2
2320,47,25,"['shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
2321,47,25,"['shape', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,2
2322,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2323,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2324,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2325,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2326,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2327,48,24,"['default', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
2328,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2329,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2330,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2331,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2332,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2333,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2334,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2335,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2336,48,24,"['default', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2
2337,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2338,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2339,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2340,48,24,"['default', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2
2341,48,24,"['default', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2342,48,24,"['default', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2
2343,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2344,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2345,48,24,"['default', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2346,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2347,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2348,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2349,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2350,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2351,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,3
2352,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2353,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2354,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2355,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2356,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2357,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2358,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2359,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2360,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2361,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2362,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2363,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2364,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2365,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
2366,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2367,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
2368,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,3
2369,49,24,"['SOME_DTYPE', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2370,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2371,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2372,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2373,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2374,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2375,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2376,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2377,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2378,50,24,"['none', 'type']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
2379,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2380,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2381,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2382,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2383,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2384,50,24,"['none', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,2
2385,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2386,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2387,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2388,50,24,"['none', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,2
2389,50,24,"['none', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2390,50,24,"['none', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,2
2391,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2392,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2393,50,24,"['none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2394,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
2395,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
2396,51,24,"['cuda', 'SOME_DTYPE']",CUDA devices for which to fork the RNG.,torch.random.fork_rng.yaml,2
2397,51,24,"['cuda', 'SOME_DTYPE']",CUDA devices for which to fork the RNG.,torch.random.fork_rng2.yaml,2
2398,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
2399,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
2400,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
2401,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
2402,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
2403,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
2404,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
2405,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
2406,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
2407,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2408,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2409,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2410,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2411,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2412,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2413,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2414,51,24,"['cuda', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
2415,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2416,51,24,"['cuda', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
2417,51,24,"['cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2418,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2419,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2420,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2421,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2422,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2423,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2424,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2425,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2426,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2427,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2428,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2429,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2430,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2431,52,23,"['default', 'none', 'type']","Default: if `None`, infers data type from `data`.",torch.as_tensor.yaml,3
2432,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2433,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2434,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2435,52,23,"['default', 'none', 'type']","Default: if `None`, infers data type from `data`.",torch.tensor.yaml,3
2436,52,23,"['default', 'none', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2437,52,23,"['default', 'none', 'type']","Default: if None, infers data type from `values`.",torch.sparse_coo_tensor.yaml,3
2438,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2439,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2440,52,23,"['default', 'none', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2441,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2442,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2443,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2444,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2445,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2446,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2447,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2448,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2449,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2450,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2451,53,23,"['tensor', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
2452,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2453,53,23,"['tensor', 'torch']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2
2454,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2455,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2456,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2457,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2458,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2459,53,23,"['tensor', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2460,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2461,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2462,53,23,"['tensor', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2463,53,23,"['tensor', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
2464,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2465,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2466,54,23,"['SOME_DTYPE', 'type']","The `nn.Module`, function, or class type to compile.",torch.jit.script.yaml,2
2467,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2468,54,23,"['SOME_DTYPE', 'type']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
2469,54,23,"['SOME_DTYPE', 'type']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,2
2470,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2471,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2472,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2473,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2474,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2475,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2476,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2477,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2478,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2479,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2480,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2481,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2482,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2483,54,23,"['SOME_DTYPE', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2484,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2485,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2486,54,23,"['SOME_DTYPE', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2487,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
2488,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
2489,55,23,"['SOME_DTYPE', 'cuda']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2
2490,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
2491,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
2492,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
2493,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
2494,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
2495,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
2496,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
2497,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
2498,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
2499,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2500,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2501,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2502,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2503,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2504,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2505,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2506,55,23,"['SOME_DTYPE', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
2507,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2508,55,23,"['SOME_DTYPE', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
2509,55,23,"['SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2510,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,2
2511,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,2
2512,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,2
2513,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,2
2514,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,2
2515,56,23,"['none', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2
2516,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,2
2517,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,2
2518,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,2
2519,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,2
2520,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,2
2521,56,23,"['none', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2
2522,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,2
2523,56,23,"['none', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
2524,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,2
2525,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,2
2526,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,2
2527,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,2
2528,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,2
2529,56,23,"['none', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,2
2530,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,2
2531,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,2
2532,56,23,"['none', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,2
2533,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,4
2534,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,4
2535,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,4
2536,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,4
2537,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,4
2538,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,4
2539,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,4
2540,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,4
2541,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,4
2542,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,4
2543,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,4
2544,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,4
2545,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,4
2546,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,4
2547,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,4
2548,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,4
2549,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,4
2550,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,4
2551,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,4
2552,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,4
2553,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,4
2554,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,4
2555,57,23,"['uses', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,4
2556,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2557,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2558,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2559,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2560,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2561,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2562,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2563,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2564,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2565,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2566,58,22,"['tensor', 'type', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,3
2567,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2568,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2569,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2570,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2571,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2572,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2573,58,22,"['tensor', 'type', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2574,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2575,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2576,58,22,"['tensor', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2577,58,22,"['tensor', 'type', 'torch']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,3
2578,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
2579,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
2580,59,22,"['tensor', 'current']",Tensor to be broadcast from current process.,torch.distributed.all_gather.yaml,2
2581,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
2582,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
2583,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
2584,59,22,"['tensor', 'current']",List of tensors(on different GPUs) to be broadcast from current process.,torch.distributed.all_gather_multigpu.yaml,2
2585,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
2586,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
2587,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
2588,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
2589,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
2590,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
2591,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2592,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2593,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2594,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2595,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2596,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2597,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2598,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2599,59,22,"['tensor', 'current']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2600,60,22,"['tensor', 'size']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
2601,60,22,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
2602,60,22,"['tensor', 'size']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
2603,60,22,"['tensor', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
2604,60,22,"['tensor', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
2605,60,22,"['tensor', 'size']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
2606,60,22,"['tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
2607,60,22,"['tensor', 'size']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
2608,60,22,"['tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
2609,60,22,"['tensor', 'size']",It should match `devices` in length and sum to `tensor.size(dim)`.,torch.cuda.comm.scatter.yaml,2
2610,60,22,"['tensor', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
2611,60,22,"['tensor', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
2612,60,22,"['tensor', 'size']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,2
2613,60,22,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
2614,60,22,"['tensor', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
2615,60,22,"['tensor', 'size']","If given, has to be a Tensor of size C",torch.nn.functional.nll_loss.yaml,2
2616,60,22,"['tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
2617,60,22,"['tensor', 'size']","If given, has to be a Tensor of size C",torch.nn.functional.cross_entropy.yaml,2
2618,60,22,"['tensor', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
2619,60,22,"['tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
2620,60,22,"['tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
2621,60,22,"['tensor', 'size']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
2622,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2623,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2624,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2625,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2626,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2627,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2628,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2629,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2630,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2631,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2632,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2633,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2634,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2635,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2636,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2637,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2638,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2639,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2640,61,22,"['SOME_DTYPE', 'current', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
2641,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2642,61,22,"['SOME_DTYPE', 'current', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
2643,61,22,"['SOME_DTYPE', 'current', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2644,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,4
2645,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,4
2646,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,4
2647,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,4
2648,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,4
2649,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,4
2650,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,4
2651,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,4
2652,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,4
2653,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,4
2654,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,4
2655,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,4
2656,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,4
2657,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,4
2658,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,4
2659,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,4
2660,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,4
2661,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,4
2662,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,4
2663,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,4
2664,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,4
2665,62,22,"['SOME_DTYPE', 'current', 'cuda', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,4
2666,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2667,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2668,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2669,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2670,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2671,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2672,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2673,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2674,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2675,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2676,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2677,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2678,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2679,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2680,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2681,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2682,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2683,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2684,63,22,"['SOME_DTYPE', 'cuda', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
2685,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2686,63,22,"['SOME_DTYPE', 'cuda', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
2687,63,22,"['SOME_DTYPE', 'cuda', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2688,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2689,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2690,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2691,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2692,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2693,64,22,"['none', 'SOME_DTYPE', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,3
2694,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2695,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2696,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2697,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2698,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2699,64,22,"['none', 'SOME_DTYPE', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,3
2700,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2701,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2702,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2703,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2704,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2705,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2706,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2707,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2708,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2709,64,22,"['none', 'SOME_DTYPE', 'tensor']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2710,65,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
2711,65,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2
2712,65,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
2713,65,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
2714,65,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2
2715,65,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
2716,65,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
2717,65,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2
2718,65,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
2719,65,22,"['none', 'input']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2
2720,65,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
2721,65,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2
2722,65,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
2723,65,22,"['none', 'input']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2
2724,65,22,"['none', 'input']","If None, derived from the input scale",torch.nn.quantized.functional.linear.yaml,2
2725,65,22,"['none', 'input']","If None, derived from the input zero_point",torch.nn.quantized.functional.linear.yaml,2
2726,65,22,"['none', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
2727,65,22,"['none', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2
2728,65,22,"['none', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2
2729,65,22,"['none', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
2730,65,22,"['none', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2
2731,65,22,"['none', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
2732,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
2733,66,22,"['cpu', 'tensor']",Works only for CPU tensors.,torch.empty_strided.yaml,2
2734,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
2735,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
2736,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
2737,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
2738,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
2739,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
2740,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
2741,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
2742,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
2743,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
2744,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
2745,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
2746,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
2747,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
2748,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
2749,66,22,"['cpu', 'tensor']",Works only for CPU tensors.,torch.tensor.yaml,2
2750,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
2751,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
2752,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
2753,66,22,"['cpu', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
2754,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2755,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2756,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2757,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2758,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2759,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2760,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2761,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2762,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2763,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2764,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2765,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2766,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2767,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2768,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2769,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2770,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2771,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2772,67,22,"['cuda', 'SOME_DTYPE', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
2773,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2774,67,22,"['cuda', 'SOME_DTYPE', 'cuda']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
2775,67,22,"['cuda', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2776,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_strided.yaml,5
2777,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.arange.yaml,5
2778,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones_like.yaml,5
2779,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.empty_like.yaml,5
2780,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hann_window.yaml,5
2781,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros_like.yaml,5
2782,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand.yaml,5
2783,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.eye.yaml,5
2784,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn_like.yaml,5
2785,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.ones.yaml,5
2786,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randperm.yaml,5
2787,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.linspace.yaml,5
2788,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.hamming_window.yaml,5
2789,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.blackman_window.yaml,5
2790,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.bartlett_window.yaml,5
2791,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.logspace.yaml,5
2792,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.tensor.yaml,5
2793,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.sparse_coo_tensor.yaml,5
2794,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.zeros.yaml,5
2795,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.randn.yaml,5
2796,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.rand_like.yaml,5
2797,68,22,"['autograd', 'record', 'operations', 'returned', 'tensor']",If autograd should record operations on the returned tensor.,torch.full.yaml,5
2798,69,22,"['process', 'group']",The process group to work on.,torch.distributed.get_backend.yaml,2
2799,69,22,"['process', 'group']",The default is the general main process group.,torch.distributed.get_backend.yaml,2
2800,69,22,"['process', 'group']","If another specific group is specified, the calling process must be part of `group`.",torch.distributed.get_backend.yaml,2
2801,69,22,"['process', 'group']",The process group to work on,torch.distributed.all_reduce.yaml,2
2802,69,22,"['process', 'group']",The process group to work on,torch.distributed.isend.yaml,2
2803,69,22,"['process', 'group']",The process group to work on,torch.distributed.barrier.yaml,2
2804,69,22,"['process', 'group']",The process group to work on,torch.distributed.all_gather.yaml,2
2805,69,22,"['process', 'group']",The process group to work on,torch.distributed.gather.yaml,2
2806,69,22,"['process', 'group']",URL specifying how to initialize the process group.,torch.distributed.init_process_group.yaml,2
2807,69,22,"['process', 'group']",Timeout for operations executed against the process group.,torch.distributed.init_process_group.yaml,2
2808,69,22,"['process', 'group']",The process group to work on,torch.distributed.recv.yaml,2
2809,69,22,"['process', 'group']",The process group to work on,torch.distributed.broadcast_multigpu.yaml,2
2810,69,22,"['process', 'group']",The process group to work on,torch.distributed.reduce.yaml,2
2811,69,22,"['process', 'group']",The process group to work on,torch.distributed.all_gather_multigpu.yaml,2
2812,69,22,"['process', 'group']",The process group to work on,torch.distributed.send.yaml,2
2813,69,22,"['process', 'group']",The process group to work on,torch.distributed.get_world_size.yaml,2
2814,69,22,"['process', 'group']",Timeout for operations executed against the process group.,torch.distributed.new_group.yaml,2
2815,69,22,"['process', 'group']",The process group to work on,torch.distributed.scatter.yaml,2
2816,69,22,"['process', 'group']",The process group to work on,torch.distributed.irecv.yaml,2
2817,69,22,"['process', 'group']",The process group to work on,torch.distributed.reduce_multigpu.yaml,2
2818,69,22,"['process', 'group']",The process group to work on,torch.distributed.broadcast.yaml,2
2819,69,22,"['process', 'group']",The process group to work on,torch.distributed.get_rank.yaml,2
2820,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,4
2821,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,4
2822,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,4
2823,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,4
2824,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,4
2825,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,4
2826,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,4
2827,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,4
2828,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,4
2829,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,4
2830,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,4
2831,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,4
2832,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,4
2833,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,4
2834,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,4
2835,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,4
2836,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,4
2837,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,4
2838,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,4
2839,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,4
2840,70,21,"['default', 'current', 'SOME_DTYPE', 'default']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,4
2841,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2842,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2843,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2844,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2845,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2846,71,21,"['default', 'tensor', 'type']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,3
2847,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2848,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2849,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2850,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2851,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2852,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2853,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2854,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2855,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2856,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2857,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2858,71,21,"['default', 'tensor', 'type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2859,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2860,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2861,71,21,"['default', 'tensor', 'type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2862,72,21,"['input', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
2863,72,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
2864,72,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2
2865,72,21,"['input', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
2866,72,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2
2867,72,21,"['input', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
2868,72,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2
2869,72,21,"['input', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
2870,72,21,"['input', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
2871,72,21,"['input', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
2872,72,21,"['input', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
2873,72,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
2874,72,21,"['input', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
2875,72,21,"['input', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2
2876,72,21,"['input', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
2877,72,21,"['input', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
2878,72,21,"['input', 'dimensions']","m-elements tuple, where  m/2  <=  input dimensions and m  is even.",torch.nn.functional.pad.yaml,2
2879,72,21,"['input', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
2880,72,21,"['input', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
2881,72,21,"['input', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
2882,72,21,"['input', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2
2883,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2884,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2885,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2886,73,21,"['tensor', 'SOME_DTYPE', 'tensor']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,3
2887,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2888,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2889,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2890,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2891,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2892,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2893,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2894,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2895,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2896,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2897,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2898,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2899,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2900,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2901,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2902,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2903,73,21,"['tensor', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2904,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2905,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2906,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,3
2907,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2908,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2909,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2910,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2911,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2912,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2913,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2914,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2915,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2916,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2917,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2918,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2919,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2920,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2921,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2922,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2923,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2924,74,21,"['SOME_DTYPE', 'SOME_DTYPE', 'cuda']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2925,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2926,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2927,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2928,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2929,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2930,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,3
2931,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2932,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2933,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2934,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2935,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2936,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2937,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2938,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2939,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2940,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2941,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2942,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2943,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2944,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2945,75,21,"['SOME_DTYPE', 'SOME_DTYPE', 'tensor']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2946,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,3
2947,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,3
2948,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,3
2949,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,3
2950,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,3
2951,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,3
2952,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,3
2953,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,3
2954,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,3
2955,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,3
2956,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,3
2957,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,3
2958,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,3
2959,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,3
2960,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,3
2961,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,3
2962,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,3
2963,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,3
2964,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,3
2965,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,3
2966,76,21,"['SOME_DTYPE', 'tensor', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,3
2967,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
2968,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
2969,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
2970,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
2971,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
2972,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
2973,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
2974,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
2975,77,21,"['SOME_DTYPE', 'see', 'torch']","When a module is passed to `torch.jit.trace`, only the `forward` method is run and traced (see `torch.jit.trace` for details).",torch.jit.trace.yaml,3
2976,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
2977,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
2978,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
2979,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
2980,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
2981,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
2982,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
2983,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
2984,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
2985,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
2986,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
2987,77,21,"['SOME_DTYPE', 'see', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
2988,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,4
2989,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,4
2990,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,4
2991,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,4
2992,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,4
2993,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,4
2994,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,4
2995,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,4
2996,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,4
2997,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,4
2998,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,4
2999,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,4
3000,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,4
3001,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,4
3002,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,4
3003,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,4
3004,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,4
3005,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,4
3006,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,4
3007,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,4
3008,78,21,"['SOME_DTYPE', 'cpu', 'current', 'SOME_DTYPE']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,4
3009,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
3010,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
3011,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
3012,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
3013,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
3014,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
3015,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,3
3016,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,3
3017,79,21,"['none', 'type', 'torch']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,3
3018,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,3
3019,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
3020,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
3021,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
3022,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,3
3023,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
3024,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
3025,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,3
3026,79,21,"['none', 'type', 'torch']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,3
3027,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
3028,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
3029,79,21,"['none', 'type', 'torch']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
3030,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,2
3031,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,2
3032,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,2
3033,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,2
3034,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,2
3035,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,2
3036,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,2
3037,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,2
3038,80,21,"['types', 'types']",The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes.,torch.jit.trace.yaml,2
3039,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,2
3040,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,2
3041,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,2
3042,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,2
3043,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,2
3044,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,2
3045,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,2
3046,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,2
3047,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,2
3048,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,2
3049,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,2
3050,80,21,"['types', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,2
3051,81,21,"['n', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3052,81,21,"['n', 'n']","flow-field of shape (N, H_out, W_out, 2)  (4-D case) or (N, D_out, H_out, W_out, 3)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
3053,81,21,"['n', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
3054,81,21,"['n', 'n']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2
3055,81,21,"['n', 'n']",the square matrix of shape (n  times n)  for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2
3056,81,21,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3057,81,21,"['n', 'n']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
3058,81,21,"['n', 'n']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2
3059,81,21,"['n', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
3060,81,21,"['n', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
3061,81,21,"['n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3062,81,21,"['n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3063,81,21,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3064,81,21,"['n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3065,81,21,"['n', 'n']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
3066,81,21,"['n', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
3067,81,21,"['n', 'n']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
3068,81,21,"['n', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
3069,81,21,"['n', 'n']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
3070,81,21,"['n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3071,81,21,"['n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3072,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,11
3073,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.arange.yaml,11
3074,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,11
3075,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.rand.yaml,11
3076,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.eye.yaml,11
3077,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.ones.yaml,11
3078,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randperm.yaml,11
3079,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.triu_indices.yaml,11
3080,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tril_indices.yaml,11
3081,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,11
3082,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,11
3083,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,11
3084,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.as_tensor.yaml,11
3085,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,11
3086,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,11
3087,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.tensor.yaml,11
3088,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if None, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.sparse_coo_tensor.yaml,11
3089,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,11
3090,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.randn.yaml,11
3091,82,20,"['default', 'none', 'uses', 'current', 'SOME_DTYPE', 'default', 'tensor', 'type', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses the current device for the default tensor type (see `torch.set_default_tensor_type()`).",torch.full.yaml,11
3092,83,20,"['input', '_channels']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,2
3093,83,20,"['input', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,2
3094,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2
3095,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
3096,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
3097,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2
3098,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
3099,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2
3100,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
3101,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2
3102,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
3103,83,20,"['input', '_channels']","input tensor (minibatch , in _channels , iH , iW) ",torch.nn.functional.avg_pool2d.yaml,2
3104,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2
3105,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
3106,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2
3107,83,20,"['input', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
3108,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2
3109,83,20,"['input', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
3110,83,20,"['input', '_channels']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2
3111,83,20,"['input', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
3112,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.empty_strided.yaml,11
3113,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.arange.yaml,11
3114,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hann_window.yaml,11
3115,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.rand.yaml,11
3116,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.eye.yaml,11
3117,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.ones.yaml,11
3118,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randperm.yaml,11
3119,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.triu_indices.yaml,11
3120,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tril_indices.yaml,11
3121,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.linspace.yaml,11
3122,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.hamming_window.yaml,11
3123,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.blackman_window.yaml,11
3124,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.as_tensor.yaml,11
3125,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.bartlett_window.yaml,11
3126,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.logspace.yaml,11
3127,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.tensor.yaml,11
3128,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.sparse_coo_tensor.yaml,11
3129,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.zeros.yaml,11
3130,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.randn.yaml,11
3131,84,20,"['SOME_DTYPE', 'cpu', 'cpu', 'tensor', 'types', 'current', 'cuda', 'SOME_DTYPE', 'cuda', 'tensor', 'types']",`device` will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.,torch.full.yaml,11
3132,85,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool3d.yaml,2
3133,85,19,"['whether', 'return']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2
3134,85,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool1d.yaml,2
3135,85,19,"['whether', 'return']",controls whether to return normalized results.,torch.irfft.yaml,2
3136,85,19,"['whether', 'return']",controls whether to return largest or smallest elements,torch.topk.yaml,2
3137,85,19,"['whether', 'return']",controls whether to return the elements in sorted order,torch.topk.yaml,2
3138,85,19,"['whether', 'return']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool2d.yaml,2
3139,85,19,"['whether', 'return']",controls whether to return normalized results.,torch.fft.yaml,2
3140,85,19,"['whether', 'return']",controls whether to return normalized results.,torch.ifft.yaml,2
3141,85,19,"['whether', 'return']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2
3142,85,19,"['whether', 'return']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2
3143,85,19,"['whether', 'return']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2
3144,85,19,"['whether', 'return']",Whether to also return the counts for each unique element.,torch.unique.yaml,2
3145,85,19,"['whether', 'return']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
3146,85,19,"['whether', 'return']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2
3147,85,19,"['whether', 'return']",Whether to also return the counts for each unique element.,torch.unique_consecutive.yaml,2
3148,85,19,"['whether', 'return']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
3149,85,19,"['whether', 'return']",controls whether to return normalized results.,torch.rfft.yaml,2
3150,85,19,"['whether', 'return']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,2
3151,86,19,"['SOME_DTYPE', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
3152,86,19,"['SOME_DTYPE', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
3153,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.l1_unstructured.yaml,2
3154,86,19,"['SOME_DTYPE', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
3155,86,19,"['SOME_DTYPE', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
3156,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_unstructured.yaml,2
3157,86,19,"['SOME_DTYPE', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
3158,86,19,"['SOME_DTYPE', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
3159,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.ln_structured.yaml,2
3160,86,19,"['SOME_DTYPE', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2
3161,86,19,"['SOME_DTYPE', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3162,86,19,"['SOME_DTYPE', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3163,86,19,"['SOME_DTYPE', 'prune']","parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3164,86,19,"['SOME_DTYPE', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
3165,86,19,"['SOME_DTYPE', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
3166,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_structured.yaml,2
3167,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.remove.yaml,2
3168,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.custom_from_mask.yaml,2
3169,86,19,"['SOME_DTYPE', 'prune']",module containing the tensor to prune.,torch.nn.utils.prune.identity.yaml,2
3170,87,19,"['torch', 'SOME_DTYPE']","Otherwise, the dtype is inferred to be torch.int64.",torch.arange.yaml,2
3171,87,19,"['torch', 'SOME_DTYPE']",Only `torch.strided` (dense layout) is supported.,torch.hann_window.yaml,2
3172,87,19,"['torch', 'SOME_DTYPE']",Default: `torch.int64`.,torch.randperm.yaml,2
3173,87,19,"['torch', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,2
3174,87,19,"['torch', 'SOME_DTYPE']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
3175,87,19,"['torch', 'SOME_DTYPE']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2
3176,87,19,"['torch', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,2
3177,87,19,"['torch', 'SOME_DTYPE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,2
3178,87,19,"['torch', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
3179,87,19,"['torch', 'SOME_DTYPE']",Only `torch.strided` (dense layout) is supported.,torch.hamming_window.yaml,2
3180,87,19,"['torch', 'SOME_DTYPE']",Only `torch.strided` (dense layout) is supported.,torch.blackman_window.yaml,2
3181,87,19,"['torch', 'SOME_DTYPE']",Only `torch.strided` (dense layout) is supported.,torch.bartlett_window.yaml,2
3182,87,19,"['torch', 'SOME_DTYPE']",A simplified version of `map_location` in `torch.save` used to dynamically remap storages to an alternative set of devices.,torch.jit.load.yaml,2
3183,87,19,"['torch', 'SOME_DTYPE']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
3184,87,19,"['torch', 'SOME_DTYPE']",Will be cast to a `torch.LongTensor` internally.,torch.sparse_coo_tensor.yaml,2
3185,87,19,"['torch', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
3186,87,19,"['torch', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
3187,87,19,"['torch', 'SOME_DTYPE']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
3188,87,19,"['torch', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
3189,88,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
3190,88,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2
3191,88,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
3192,88,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
3193,88,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2
3194,88,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
3195,88,18,"['default', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,2
3196,88,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
3197,88,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2
3198,88,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
3199,88,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
3200,88,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2
3201,88,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
3202,88,18,"['default', 'input']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
3203,88,18,"['default', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2
3204,88,18,"['default', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
3205,88,18,"['default', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2
3206,88,18,"['default', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
3207,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
3208,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
3209,89,17,"['output', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,2
3210,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
3211,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
3212,89,17,"['output', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,2
3213,89,17,"['output', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,2
3214,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
3215,89,17,"['output', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
3216,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
3217,89,17,"['output', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
3218,89,17,"['output', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
3219,89,17,"['output', 'SOME_DTYPE']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
3220,89,17,"['output', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,2
3221,89,17,"['output', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,2
3222,89,17,"['output', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
3223,89,17,"['output', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,2
3224,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2
3225,90,17,"['true', 'input']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2
3226,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2
3227,90,17,"['true', 'input']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2
3228,90,17,"['true', 'input']","if `True`, the input is expected to contain sequences sorted by length in a decreasing order.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2
3229,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2
3230,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2
3231,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2
3232,90,17,"['true', 'input']","if `True`, checks that the input contains sequences sorted by length in a decreasing order.",torch.nn.utils.rnn.pack_sequence.yaml,2
3233,90,17,"['true', 'input']","if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.",torch.autograd.gradcheck.yaml,2
3234,90,17,"['true', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
3235,90,17,"['true', 'input']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2
3236,90,17,"['true', 'input']","if `True` the loss is computed as  exp(input) - target * input , if `False` then loss is input - target *  log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2
3237,90,17,"['true', 'input']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2
3238,90,17,"['true', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
3239,90,17,"['true', 'input']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2
3240,90,17,"['true', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
3241,91,17,"['input', 'shape']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
3242,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
3243,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
3244,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
3245,91,17,"['input', 'shape']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
3246,91,17,"['input', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,2
3247,91,17,"['input', 'shape']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
3248,91,17,"['input', 'shape']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
3249,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
3250,91,17,"['input', 'shape']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
3251,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
3252,91,17,"['input', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2
3253,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
3254,91,17,"['input', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
3255,91,17,"['input', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
3256,91,17,"['input', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3257,91,17,"['input', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
3258,92,17,"['input', 'm']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
3259,92,17,"['input', 'm']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
3260,92,17,"['input', 'm']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
3261,92,17,"['input', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3262,92,17,"['input', 'm']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3263,92,17,"['input', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
3264,92,17,"['input', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
3265,92,17,"['input', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
3266,92,17,"['input', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
3267,92,17,"['input', 'm']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3268,92,17,"['input', 'm']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3269,92,17,"['input', 'm']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3270,92,17,"['input', 'm']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3271,92,17,"['input', 'm']","m-elements tuple, where  m/2  <=  input dimensions and m  is even.",torch.nn.functional.pad.yaml,2
3272,92,17,"['input', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3273,92,17,"['input', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3274,92,17,"['input', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
3275,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,2
3276,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,2
3277,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,2
3278,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,2
3279,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,2
3280,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,2
3281,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,2
3282,93,17,"['tensor', 'dim']",It should match `devices` in length and sum to `tensor.size(dim)`.,torch.cuda.comm.scatter.yaml,2
3283,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,2
3284,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,2
3285,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,2
3286,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,2
3287,93,17,"['tensor', 'dim']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,2
3288,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,2
3289,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,2
3290,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,2
3291,93,17,"['tensor', 'dim']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,2
3292,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,2
3293,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,2
3294,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,2
3295,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,2
3296,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,2
3297,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,2
3298,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,2
3299,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,2
3300,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,2
3301,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,2
3302,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,2
3303,94,17,"['whether', 'output']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,2
3304,94,17,"['whether', 'output']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2
3305,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,2
3306,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,2
3307,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,2
3308,94,17,"['whether', 'output']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,2
3309,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.empty_strided.yaml,2
3310,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.arange.yaml,2
3311,95,17,"['torch', 'strided']",Only `torch.strided` (dense layout) is supported.,torch.hann_window.yaml,2
3312,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.rand.yaml,2
3313,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.eye.yaml,2
3314,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.ones.yaml,2
3315,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.randperm.yaml,2
3316,95,17,"['torch', 'strided']",currently only support `torch.strided`.,torch.triu_indices.yaml,2
3317,95,17,"['torch', 'strided']",currently only support `torch.strided`.,torch.tril_indices.yaml,2
3318,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.linspace.yaml,2
3319,95,17,"['torch', 'strided']",Only `torch.strided` (dense layout) is supported.,torch.hamming_window.yaml,2
3320,95,17,"['torch', 'strided']",Only `torch.strided` (dense layout) is supported.,torch.blackman_window.yaml,2
3321,95,17,"['torch', 'strided']",Only `torch.strided` (dense layout) is supported.,torch.bartlett_window.yaml,2
3322,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.logspace.yaml,2
3323,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.zeros.yaml,2
3324,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.randn.yaml,2
3325,95,17,"['torch', 'strided']",Default: `torch.strided`.,torch.full.yaml,2
3326,96,17,"['process', 'group', 'work']",The process group to work on.,torch.distributed.get_backend.yaml,3
3327,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_reduce.yaml,3
3328,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.isend.yaml,3
3329,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.barrier.yaml,3
3330,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_gather.yaml,3
3331,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.gather.yaml,3
3332,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.recv.yaml,3
3333,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.broadcast_multigpu.yaml,3
3334,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.reduce.yaml,3
3335,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.all_gather_multigpu.yaml,3
3336,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.send.yaml,3
3337,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.get_world_size.yaml,3
3338,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.scatter.yaml,3
3339,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.irecv.yaml,3
3340,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.reduce_multigpu.yaml,3
3341,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.broadcast.yaml,3
3342,96,17,"['process', 'group', 'work']",The process group to work on,torch.distributed.get_rank.yaml,3
3343,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,2
3344,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,2
3345,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,2
3346,97,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy.yaml,2
3347,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,2
3348,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,2
3349,97,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.nll_loss.yaml,2
3350,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,2
3351,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,2
3352,97,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.cross_entropy.yaml,2
3353,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,2
3354,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3355,97,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3356,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3357,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,2
3358,97,17,"['deprecated', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.poisson_nll_loss.yaml,2
3359,97,17,"['deprecated', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,2
3360,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.std2.yaml,2
3361,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.prod2.yaml,2
3362,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.mean2.yaml,2
3363,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.sum2.yaml,2
3364,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.nn.functional.normalize.yaml,2
3365,98,16,"['dimension', 'reduce']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,2
3366,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.min2.yaml,2
3367,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.argmin2.yaml,2
3368,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,2
3369,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.median2.yaml,2
3370,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,2
3371,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.max2.yaml,2
3372,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.argmax2.yaml,2
3373,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.var2.yaml,2
3374,98,16,"['dimension', 'reduce']",the dimension to reduce.,torch.mode.yaml,2
3375,98,16,"['dimension', 'reduce']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,2
3376,99,16,"['dimension', 'along']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz.yaml,2
3377,99,16,"['dimension', 'along']",A dimension along which softmax will be computed.,torch.nn.functional.gumbel_softmax.yaml,2
3378,99,16,"['dimension', 'along']",the dimension to sort along,torch.topk.yaml,2
3379,99,16,"['dimension', 'along']",dimension along which to split the tensor.,torch.split.yaml,2
3380,99,16,"['dimension', 'along']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz2.yaml,2
3381,99,16,"['dimension', 'along']",A dimension along which log_softmax will be computed.,torch.nn.functional.log_softmax.yaml,2
3382,99,16,"['dimension', 'along']",A dimension along which to chunk the tensor.,torch.cuda.comm.scatter.yaml,2
3383,99,16,"['dimension', 'along']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2
3384,99,16,"['dimension', 'along']",the dimension to sort along,torch.argsort.yaml,2
3385,99,16,"['dimension', 'along']",the dimension to sort along,torch.sort.yaml,2
3386,99,16,"['dimension', 'along']",the dimension along which to narrow,torch.narrow.yaml,2
3387,99,16,"['dimension', 'along']",the dimension to find the kth value along,torch.kthvalue.yaml,2
3388,99,16,"['dimension', 'along']",A dimension along which softmin will be computed (so every slice along dim will sum to 1).,torch.nn.functional.softmin.yaml,2
3389,99,16,"['dimension', 'along']",dimension along which to split the tensor,torch.chunk.yaml,2
3390,99,16,"['dimension', 'along']",A dimension along which softmax will be computed.,torch.nn.functional.softmax.yaml,2
3391,99,16,"['dimension', 'along']",a dimension along which the tensors will be concatenated.,torch.cuda.comm.gather.yaml,2
3392,100,16,"['input', 'tensor', 'size']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
3393,100,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
3394,100,16,"['input', 'tensor', 'size']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
3395,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,3
3396,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
3397,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
3398,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
3399,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
3400,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
3401,100,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
3402,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
3403,100,16,"['input', 'tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
3404,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
3405,100,16,"['input', 'tensor', 'size']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
3406,100,16,"['input', 'tensor', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
3407,100,16,"['input', 'tensor', 'size']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
3408,101,16,"['input', 'batch']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3409,101,16,"['input', 'batch']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2
3410,101,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3411,101,16,"['input', 'batch']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3412,101,16,"['input', 'batch']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
3413,101,16,"['input', 'batch']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3414,101,16,"['input', 'batch']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3415,101,16,"['input', 'batch']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3416,101,16,"['input', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3417,101,16,"['input', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3418,101,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3419,101,16,"['input', 'batch']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3420,101,16,"['input', 'batch']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3421,101,16,"['input', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3422,101,16,"['input', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3423,101,16,"['input', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3424,102,16,"['tensor', '_channels']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,2
3425,102,16,"['tensor', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,2
3426,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,2
3427,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,2
3428,102,16,"['tensor', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2
3429,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,2
3430,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,2
3431,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,2
3432,102,16,"['tensor', '_channels']","input tensor (minibatch , in _channels , iH , iW) ",torch.nn.functional.avg_pool2d.yaml,2
3433,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,2
3434,102,16,"['tensor', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2
3435,102,16,"['tensor', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,2
3436,102,16,"['tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2
3437,102,16,"['tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
3438,102,16,"['tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2
3439,102,16,"['tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
3440,103,16,"['tensor', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3441,103,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3442,103,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,2
3443,103,16,"['tensor', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3444,103,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,2
3445,103,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,2
3446,103,16,"['tensor', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3447,103,16,"['tensor', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3448,103,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3449,103,16,"['tensor', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
3450,103,16,"['tensor', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2
3451,103,16,"['tensor', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3452,103,16,"['tensor', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
3453,103,16,"['tensor', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3454,103,16,"['tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3455,103,16,"['tensor', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,2
3456,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.std2.yaml,5
3457,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.prod2.yaml,5
3458,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.mean2.yaml,5
3459,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.sum2.yaml,5
3460,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.min2.yaml,5
3461,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.argmin2.yaml,5
3462,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.var_mean2.yaml,5
3463,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.median2.yaml,5
3464,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.logsumexp.yaml,5
3465,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.max2.yaml,5
3466,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.argmax2.yaml,5
3467,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensors have `dim` retained or not.,torch.norm.yaml,5
3468,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.kthvalue.yaml,5
3469,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.var2.yaml,5
3470,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.mode.yaml,5
3471,104,16,"['whether', 'output', 'tensor', 'dim', 'retained']",whether the output tensor has `dim` retained or not.,torch.std_mean2.yaml,5
3472,105,16,"['size', 'zero']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3473,105,16,"['size', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3474,105,16,"['size', 'zero']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3475,105,16,"['size', 'zero']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3476,105,16,"['size', 'zero']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
3477,105,16,"['size', 'zero']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3478,105,16,"['size', 'zero']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3479,105,16,"['size', 'zero']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3480,105,16,"['size', 'zero']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3481,105,16,"['size', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3482,105,16,"['size', 'zero']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3483,105,16,"['size', 'zero']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3484,105,16,"['size', 'zero']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3485,105,16,"['size', 'zero']",If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.,torch.sparse_coo_tensor.yaml,2
3486,105,16,"['size', 'zero']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3487,105,16,"['size', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3488,106,16,"['size', 'batch']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3489,106,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3490,106,16,"['size', 'batch']",the size of the original signal (without batch dimension).,torch.irfft.yaml,2
3491,106,16,"['size', 'batch']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3492,106,16,"['size', 'batch']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3493,106,16,"['size', 'batch']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
3494,106,16,"['size', 'batch']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3495,106,16,"['size', 'batch']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3496,106,16,"['size', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3497,106,16,"['size', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3498,106,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3499,106,16,"['size', 'batch']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3500,106,16,"['size', 'batch']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3501,106,16,"['size', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3502,106,16,"['size', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3503,106,16,"['size', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3504,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
3505,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
3506,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
3507,107,16,"['SOME_DTYPE', 'given']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
3508,107,16,"['SOME_DTYPE', 'given']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
3509,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
3510,107,16,"['SOME_DTYPE', 'given']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
3511,107,16,"['SOME_DTYPE', 'given']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
3512,107,16,"['SOME_DTYPE', 'given']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
3513,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
3514,107,16,"['SOME_DTYPE', 'given']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
3515,107,16,"['SOME_DTYPE', 'given']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
3516,107,16,"['SOME_DTYPE', 'given']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
3517,107,16,"['SOME_DTYPE', 'given']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
3518,107,16,"['SOME_DTYPE', 'given']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
3519,107,16,"['SOME_DTYPE', 'given']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
3520,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
3521,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
3522,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
3523,108,16,"['given', 'SOME_DTYPE']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
3524,108,16,"['given', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,2
3525,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
3526,108,16,"['given', 'SOME_DTYPE']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
3527,108,16,"['given', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,2
3528,108,16,"['given', 'SOME_DTYPE']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
3529,108,16,"['given', 'SOME_DTYPE']","This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",torch.distributed.init_process_group.yaml,2
3530,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
3531,108,16,"['given', 'SOME_DTYPE']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
3532,108,16,"['given', 'SOME_DTYPE']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,2
3533,108,16,"['given', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
3534,108,16,"['given', 'SOME_DTYPE']","This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",torch.distributed.new_group.yaml,2
3535,108,16,"['given', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
3536,109,16,"['zero', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3537,109,16,"['zero', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3538,109,16,"['zero', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3539,109,16,"['zero', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3540,109,16,"['zero', 'dimensions']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
3541,109,16,"['zero', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3542,109,16,"['zero', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3543,109,16,"['zero', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3544,109,16,"['zero', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3545,109,16,"['zero', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3546,109,16,"['zero', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3547,109,16,"['zero', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3548,109,16,"['zero', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3549,109,16,"['zero', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
3550,109,16,"['zero', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3551,109,16,"['zero', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3552,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2
3553,110,16,"['_channels', 'groups']","filters of shape (in _channels ,  out _channels/groups , kH , kW) ",torch.nn.functional.conv_transpose2d.yaml,2
3554,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2
3555,110,16,"['_channels', 'groups']","filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.functional.conv2d.yaml,2
3556,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2
3557,110,16,"['_channels', 'groups']","filters of shape (in _channels ,  out _channels/groups , kT , kH , kW) ",torch.nn.functional.conv_transpose3d.yaml,2
3558,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2
3559,110,16,"['_channels', 'groups']","filters of shape (in _channels ,  out _channels/groups , kW) ",torch.nn.functional.conv_transpose1d.yaml,2
3560,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2
3561,110,16,"['_channels', 'groups']","filters of shape (out _channels ,  in _channels/groups , kW) ",torch.nn.functional.conv1d.yaml,2
3562,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2
3563,110,16,"['_channels', 'groups']","filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",torch.nn.functional.conv3d.yaml,2
3564,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2
3565,110,16,"['_channels', 'groups']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,2
3566,110,16,"['_channels', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2
3567,110,16,"['_channels', 'groups']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,2
3568,111,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,2
3569,111,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
3570,111,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
3571,111,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,2
3572,111,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
3573,111,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
3574,111,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,2
3575,111,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
3576,111,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
3577,111,16,"['parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2
3578,111,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3579,111,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3580,111,16,"['parameters', 'prune']","parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
3581,111,16,"['parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,2
3582,111,16,"['parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
3583,111,16,"['parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
3584,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,4
3585,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,4
3586,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,4
3587,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,4
3588,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,4
3589,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,4
3590,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,4
3591,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,4
3592,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,4
3593,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,4
3594,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,4
3595,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,4
3596,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,4
3597,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,4
3598,112,15,"['default', 'none', 'defaults', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,4
3599,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,3
3600,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,3
3601,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,3
3602,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,3
3603,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,3
3604,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,3
3605,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,3
3606,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,3
3607,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,3
3608,113,15,"['default', 'uses', 'global']",By default uses the same backend as the global group.,torch.distributed.new_group.yaml,3
3609,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,3
3610,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,3
3611,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,3
3612,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,3
3613,113,15,"['default', 'uses', 'global']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,3
3614,114,15,"['SOME_STRUCTURE', 'output']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
3615,114,15,"['SOME_STRUCTURE', 'output']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
3616,114,15,"['SOME_STRUCTURE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
3617,114,15,"['SOME_STRUCTURE', 'output']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2
3618,114,15,"['SOME_STRUCTURE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
3619,114,15,"['SOME_STRUCTURE', 'output']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2
3620,114,15,"['SOME_STRUCTURE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
3621,114,15,"['SOME_STRUCTURE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
3622,114,15,"['SOME_STRUCTURE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
3623,114,15,"['SOME_STRUCTURE', 'output']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2
3624,114,15,"['SOME_STRUCTURE', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
3625,114,15,"['SOME_STRUCTURE', 'output']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
3626,114,15,"['SOME_STRUCTURE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
3627,114,15,"['SOME_STRUCTURE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
3628,114,15,"['SOME_STRUCTURE', 'output']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
3629,115,15,"['input', 'tensor', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
3630,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
3631,115,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.irfft.yaml,3
3632,115,15,"['input', 'tensor', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
3633,115,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.fft.yaml,3
3634,115,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` `+ 1` dimensions,torch.ifft.yaml,3
3635,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
3636,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
3637,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
3638,115,15,"['input', 'tensor', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,3
3639,115,15,"['input', 'tensor', 'dimensions']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,3
3640,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
3641,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
3642,115,15,"['input', 'tensor', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
3643,115,15,"['input', 'tensor', 'dimensions']",the input tensor of at least `signal_ndim` dimensions,torch.rfft.yaml,3
3644,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,3
3645,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,3
3646,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,3
3647,116,15,"['input', 'tensor', 'shape']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,3
3648,116,15,"['input', 'tensor', 'shape']",input tensor of any shape,torch.nn.functional.normalize.yaml,3
3649,116,15,"['input', 'tensor', 'shape']",input tensor of shape B  times P  times M .,torch.cdist.yaml,3
3650,116,15,"['input', 'tensor', 'shape']",input tensor of shape B  times R  times M .,torch.cdist.yaml,3
3651,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,3
3652,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,3
3653,116,15,"['input', 'tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,3
3654,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,3
3655,116,15,"['input', 'tensor', 'shape']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,3
3656,116,15,"['input', 'tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,3
3657,116,15,"['input', 'tensor', 'shape']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
3658,116,15,"['input', 'tensor', 'shape']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,3
3659,117,15,"['input', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3660,117,15,"['input', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
3661,117,15,"['input', 'n']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2
3662,117,15,"['input', 'n']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
3663,117,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3664,117,15,"['input', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3665,117,15,"['input', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
3666,117,15,"['input', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
3667,117,15,"['input', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3668,117,15,"['input', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3669,117,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3670,117,15,"['input', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3671,117,15,"['input', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3672,117,15,"['input', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3673,117,15,"['input', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
3674,118,15,"['tensor', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
3675,118,15,"['tensor', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
3676,118,15,"['tensor', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
3677,118,15,"['tensor', 'input']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
3678,118,15,"['tensor', 'input']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
3679,118,15,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.kl_div.yaml,2
3680,118,15,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy.yaml,2
3681,118,15,"['tensor', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
3682,118,15,"['tensor', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
3683,118,15,"['tensor', 'input']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2
3684,118,15,"['tensor', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
3685,118,15,"['tensor', 'input']",A Tensor that is input to `functions`,torch.utils.checkpoint.checkpoint_sequential.yaml,2
3686,118,15,"['tensor', 'input']",Tensor of the same shape as input,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3687,118,15,"['tensor', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
3688,118,15,"['tensor', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
3689,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
3690,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2
3691,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
3692,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2
3693,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
3694,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
3695,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2
3696,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
3697,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2
3698,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
3699,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2
3700,119,15,"['tensor', 'inputs']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
3701,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
3702,119,15,"['tensor', 'inputs']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
3703,119,15,"['tensor', 'inputs']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2
3704,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,4
3705,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,4
3706,120,15,"['size', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,4
3707,120,15,"['size', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,4
3708,120,15,"['size', 'zero', 'batch', 'dimensions']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,4
3709,120,15,"['size', 'zero', 'batch', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,4
3710,120,15,"['size', 'zero', 'batch', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,4
3711,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,4
3712,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,4
3713,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,4
3714,120,15,"['size', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,4
3715,120,15,"['size', 'zero', 'batch', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,4
3716,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,4
3717,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,4
3718,120,15,"['size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,4
3719,121,15,"['size', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3720,121,15,"['size', 'm']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
3721,121,15,"['size', 'm']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
3722,121,15,"['size', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
3723,121,15,"['size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
3724,121,15,"['size', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
3725,121,15,"['size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
3726,121,15,"['size', 'm']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
3727,121,15,"['size', 'm']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
3728,121,15,"['size', 'm']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,2
3729,121,15,"['size', 'm']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3730,121,15,"['size', 'm']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
3731,121,15,"['size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3732,121,15,"['size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3733,121,15,"['size', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
3734,122,15,"['SOME_DTYPE', 'containing']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2
3735,122,15,"['SOME_DTYPE', 'containing']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,2
3736,122,15,"['SOME_DTYPE', 'containing']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2
3737,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.l1_unstructured.yaml,2
3738,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.random_unstructured.yaml,2
3739,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.ln_structured.yaml,2
3740,122,15,"['SOME_DTYPE', 'containing']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2
3741,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.random_structured.yaml,2
3742,122,15,"['SOME_DTYPE', 'containing']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2
3743,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.remove.yaml,2
3744,122,15,"['SOME_DTYPE', 'containing']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2
3745,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune,torch.nn.utils.prune.custom_from_mask.yaml,2
3746,122,15,"['SOME_DTYPE', 'containing']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2
3747,122,15,"['SOME_DTYPE', 'containing']",module containing the tensor to prune.,torch.nn.utils.prune.identity.yaml,2
3748,122,15,"['SOME_DTYPE', 'containing']",Model containing the modules to be fused,torch.quantization.fuse_modules.yaml,2
3749,123,14,"['output', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
3750,123,14,"['output', 'SOME_STRUCTURE']",Output list.,torch.distributed.all_gather.yaml,2
3751,123,14,"['output', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
3752,123,14,"['output', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
3753,123,14,"['output', 'SOME_STRUCTURE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
3754,123,14,"['output', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
3755,123,14,"['output', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
3756,123,14,"['output', 'SOME_STRUCTURE']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,2
3757,123,14,"['output', 'SOME_STRUCTURE']",optional output tuple.,torch.lu.yaml,2
3758,123,14,"['output', 'SOME_STRUCTURE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
3759,123,14,"['output', 'SOME_STRUCTURE']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
3760,123,14,"['output', 'SOME_STRUCTURE']",optional output tuple.,torch.solve.yaml,2
3761,123,14,"['output', 'SOME_STRUCTURE']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,2
3762,123,14,"['output', 'SOME_STRUCTURE']",the output tuple of tensors,torch.svd.yaml,2
3763,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.empty_strided.yaml,8
3764,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.arange.yaml,8
3765,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hann_window.yaml,8
3766,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.rand.yaml,8
3767,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.eye.yaml,8
3768,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.ones.yaml,8
3769,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.linspace.yaml,8
3770,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.hamming_window.yaml,8
3771,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.blackman_window.yaml,8
3772,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.bartlett_window.yaml,8
3773,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.logspace.yaml,8
3774,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.zeros.yaml,8
3775,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.randn.yaml,8
3776,124,14,"['default', 'none', 'uses', 'global', 'default', 'see', 'torch', 'set_default_tensor_type']","Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).",torch.full.yaml,8
3777,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.rand.yaml,2
3778,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.ones.yaml,2
3779,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.as_tensor.yaml,2
3780,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.tensor.yaml,2
3781,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
3782,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,2
3783,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
3784,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",list of list of module names to fuse.,torch.quantization.fuse_modules.yaml,2
3785,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
3786,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.zeros.yaml,2
3787,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",Can be a variable number of arguments or a collection like a list or tuple.,torch.randn.yaml,2
3788,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",torch.roll.yaml,2
3789,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
3790,125,14,"['SOME_STRUCTURE', 'SOME_STRUCTURE']",This method will throw `ValueError` if `total_length` is less than the max sequence length in `sequence`.,torch.nn.utils.rnn.pad_packed_sequence.yaml,2
3791,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,5
3792,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,5
3793,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,5
3794,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,5
3795,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,5
3796,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,5
3797,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,5
3798,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,5
3799,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,5
3800,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,5
3801,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,5
3802,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,5
3803,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,5
3804,126,14,"['input', 'size', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,5
3805,127,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
3806,127,14,"['input', 'output']",Input and output of the collective.,torch.distributed.all_reduce.yaml,2
3807,127,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
3808,127,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
3809,127,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
3810,127,14,"['input', 'output']",Input and output of the collective.,torch.distributed.reduce.yaml,2
3811,127,14,"['input', 'output']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2
3812,127,14,"['input', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
3813,127,14,"['input', 'output']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
3814,127,14,"['input', 'output']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,2
3815,127,14,"['input', 'output']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
3816,127,14,"['input', 'output']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2
3817,127,14,"['input', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
3818,127,14,"['input', 'output']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
3819,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2
3820,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2
3821,128,14,"['tensor', 'value']",the tensor or value to compare,torch.le.yaml,2
3822,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2
3823,128,14,"['tensor', 'value']",the tensor or value to compare,torch.gt.yaml,2
3824,128,14,"['tensor', 'value']",the tensor or value to compare,torch.ne.yaml,2
3825,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2
3826,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2
3827,128,14,"['tensor', 'value']",the tensor or value to compare,torch.eq.yaml,2
3828,128,14,"['tensor', 'value']",the tensor or value to compare,torch.ge.yaml,2
3829,128,14,"['tensor', 'value']",the tensor or value to compare,torch.lt.yaml,2
3830,128,14,"['tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
3831,128,14,"['tensor', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2
3832,128,14,"['tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
3833,129,14,"['tensor', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
3834,129,14,"['tensor', 'n']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
3835,129,14,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
3836,129,14,"['tensor', 'n']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
3837,129,14,"['tensor', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
3838,129,14,"['tensor', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
3839,129,14,"['tensor', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
3840,129,14,"['tensor', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
3841,129,14,"['tensor', 'n']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,2
3842,129,14,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
3843,129,14,"['tensor', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
3844,129,14,"['tensor', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
3845,129,14,"['tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
3846,129,14,"['tensor', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
3847,130,14,"['tensor', 'compare']",the tensor to compare,torch.le.yaml,2
3848,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.le.yaml,2
3849,130,14,"['tensor', 'compare']",the tensor to compare,torch.gt.yaml,2
3850,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.gt.yaml,2
3851,130,14,"['tensor', 'compare']",the tensor to compare,torch.ne.yaml,2
3852,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.ne.yaml,2
3853,130,14,"['tensor', 'compare']",the tensor to compare,torch.eq.yaml,2
3854,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.eq.yaml,2
3855,130,14,"['tensor', 'compare']",the tensor to compare,torch.ge.yaml,2
3856,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.ge.yaml,2
3857,130,14,"['tensor', 'compare']",the tensor to compare,torch.lt.yaml,2
3858,130,14,"['tensor', 'compare']",the tensor or value to compare,torch.lt.yaml,2
3859,130,14,"['tensor', 'compare']",first tensor to compare,torch.allclose.yaml,2
3860,130,14,"['tensor', 'compare']",second tensor to compare,torch.allclose.yaml,2
3861,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,3
3862,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,3
3863,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,3
3864,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,3
3865,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,3
3866,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,3
3867,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,3
3868,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,3
3869,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,3
3870,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,3
3871,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,3
3872,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3
3873,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,3
3874,131,14,"['SOME_DTYPE', 'SOME_DTYPE', 'default']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,3
3875,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hvp.yaml,2
3876,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2
3877,132,14,"['false', 'inputs']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2
3878,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vjp.yaml,2
3879,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2
3880,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jvp.yaml,2
3881,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2
3882,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vhp.yaml,2
3883,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2
3884,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jacobian.yaml,2
3885,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2
3886,132,14,"['false', 'inputs']","If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error.",torch.autograd.grad.yaml,2
3887,132,14,"['false', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hessian.yaml,2
3888,132,14,"['false', 'inputs']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2
3889,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
3890,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
3891,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
3892,133,14,"['none', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
3893,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
3894,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
3895,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
3896,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
3897,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
3898,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
3899,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3900,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3901,133,14,"['none', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
3902,133,14,"['none', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
3903,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
3904,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
3905,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
3906,134,14,"['none', 'sum']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
3907,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
3908,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
3909,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
3910,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
3911,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
3912,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
3913,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3914,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3915,134,14,"['none', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
3916,134,14,"['none', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
3917,135,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
3918,135,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
3919,135,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
3920,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2
3921,135,14,"['specified', 'tensor']","If not specified, the tensor will be divided into equal chunks.",torch.cuda.comm.scatter.yaml,2
3922,135,14,"['specified', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2
3923,135,14,"['specified', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2
3924,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2
3925,135,14,"['specified', 'tensor']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2
3926,135,14,"['specified', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
3927,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2
3928,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2
3929,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2
3930,135,14,"['specified', 'tensor']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2
3931,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hvp.yaml,2
3932,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hvp.yaml,2
3933,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vjp.yaml,2
3934,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vjp.yaml,2
3935,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jvp.yaml,2
3936,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jvp.yaml,2
3937,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vhp.yaml,2
3938,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.vhp.yaml,2
3939,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jacobian.yaml,2
3940,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.jacobian.yaml,2
3941,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.backward.yaml,2
3942,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.grad.yaml,2
3943,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hessian.yaml,2
3944,136,14,"['defaults', 'false']",Defaults to `False`.,torch.autograd.functional.hessian.yaml,2
3945,137,14,"['n', 'dimensional']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
3946,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,2
3947,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,2
3948,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,2
3949,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,2
3950,137,14,"['n', 'dimensional']",N-dimensional tensor,torch.nn.functional.pad.yaml,2
3951,137,14,"['n', 'dimensional']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
3952,137,14,"['n', 'dimensional']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
3953,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,2
3954,137,14,"['n', 'dimensional']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
3955,137,14,"['n', 'dimensional']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
3956,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,2
3957,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,2
3958,137,14,"['n', 'dimensional']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,2
3959,138,14,"['dimensional', 'tensor']",any number of 1 dimensional tensors.,torch.cartesian_prod.yaml,2
3960,138,14,"['dimensional', 'tensor']",a 2-dimensional torch.Tensor,torch.nn.init.eye_.yaml,2
3961,138,14,"['dimensional', 'tensor']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
3962,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,2
3963,138,14,"['dimensional', 'tensor']","a {3, 4, 5}-dimensional torch.Tensor",torch.nn.init.dirac_.yaml,2
3964,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,2
3965,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,2
3966,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,2
3967,138,14,"['dimensional', 'tensor']",N-dimensional tensor,torch.nn.functional.pad.yaml,2
3968,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,2
3969,138,14,"['dimensional', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
3970,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,2
3971,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,2
3972,138,14,"['dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,2
3973,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
3974,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
3975,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
3976,139,14,"['reduction', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
3977,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
3978,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
3979,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
3980,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
3981,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
3982,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
3983,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3984,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
3985,139,14,"['reduction', 'output']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
3986,139,14,"['reduction', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
3987,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3
3988,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3
3989,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,3
3990,140,14,"['reduction', 'output', 'sum']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,3
3991,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3
3992,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3
3993,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3
3994,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3
3995,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3
3996,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3
3997,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
3998,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
3999,140,14,"['reduction', 'output', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3
4000,140,14,"['reduction', 'output', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3
4001,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
4002,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
4003,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
4004,141,14,"['reduction', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
4005,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
4006,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
4007,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
4008,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
4009,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
4010,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
4011,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4012,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4013,141,14,"['reduction', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
4014,141,14,"['reduction', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
4015,142,14,"['controls', 'whether']",controls whether to return normalized results.,torch.irfft.yaml,2
4016,142,14,"['controls', 'whether']","controls whether `input` was halfed to avoid redundancy, e.g., by `rfft()`.",torch.irfft.yaml,2
4017,142,14,"['controls', 'whether']",controls whether to return largest or smallest elements,torch.topk.yaml,2
4018,142,14,"['controls', 'whether']",controls whether to return the elements in sorted order,torch.topk.yaml,2
4019,142,14,"['controls', 'whether']",controls whether to return normalized results.,torch.fft.yaml,2
4020,142,14,"['controls', 'whether']",controls whether to return normalized results.,torch.ifft.yaml,2
4021,142,14,"['controls', 'whether']",controls whether eigenvectors have to be computed,torch.symeig.yaml,2
4022,142,14,"['controls', 'whether']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,2
4023,142,14,"['controls', 'whether']",controls whether pivoting is done.,torch.lu.yaml,2
4024,142,14,"['controls', 'whether']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2
4025,142,14,"['controls', 'whether']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2
4026,142,14,"['controls', 'whether']",Controls whether to enable flush denormal mode or not,torch.set_flush_denormal.yaml,2
4027,142,14,"['controls', 'whether']",controls whether to return normalized results.,torch.rfft.yaml,2
4028,142,14,"['controls', 'whether']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,2
4029,143,13,"['default', 'losses']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2
4030,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,2
4031,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2
4032,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4033,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2
4034,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,2
4035,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2
4036,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4037,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2
4038,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4039,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4040,143,13,"['default', 'losses']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4041,143,13,"['default', 'losses']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2
4042,144,13,"['input', 'size', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4043,144,13,"['input', 'size', 'm']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,3
4044,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,3
4045,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4046,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4047,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4048,144,13,"['input', 'size', 'm']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,3
4049,144,13,"['input', 'size', 'm']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3
4050,144,13,"['input', 'size', 'm']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,3
4051,144,13,"['input', 'size', 'm']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,3
4052,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4053,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4054,144,13,"['input', 'size', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
4055,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,3
4056,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,3
4057,145,13,"['tensor', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,3
4058,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,3
4059,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,3
4060,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,3
4061,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,3
4062,145,13,"['tensor', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,3
4063,145,13,"['tensor', 'shape', '_channels']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,3
4064,145,13,"['tensor', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3
4065,145,13,"['tensor', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,3
4066,145,13,"['tensor', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3
4067,145,13,"['tensor', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,3
4068,146,13,"['false', 'instead']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
4069,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
4070,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2
4071,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4072,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2
4073,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
4074,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2
4075,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4076,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2
4077,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4078,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4079,146,13,"['false', 'instead']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4080,146,13,"['false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2
4081,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,7
4082,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,7
4083,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,7
4084,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,7
4085,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_name.yaml,7
4086,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,7
4087,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,7
4088,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.get_device_capability.yaml,7
4089,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,7
4090,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,7
4091,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","It uses the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.synchronize.yaml,7
4092,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,7
4093,147,13,"['current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,7
4094,148,13,"['losses', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
4095,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,2
4096,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2
4097,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4098,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2
4099,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,2
4100,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2
4101,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4102,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2
4103,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4104,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4105,148,13,"['losses', 'summed']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4106,148,13,"['losses', 'summed']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2
4107,149,12,"['output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
4108,149,12,"['output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
4109,149,12,"['output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
4110,149,12,"['output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
4111,149,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
4112,149,12,"['output', 'size']",the target output image size.,torch.nn.functional.affine_grid.yaml,2
4113,149,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
4114,149,12,"['output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
4115,149,12,"['output', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4116,149,12,"['output', 'size']",output spatial size.,torch.nn.quantized.functional.interpolate.yaml,2
4117,149,12,"['output', 'size']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4118,149,12,"['output', 'size']",output spatial size.,torch.nn.functional.interpolate.yaml,2
4119,150,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2
4120,150,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2
4121,150,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2
4122,150,12,"['true', 'output']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2
4123,150,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2
4124,150,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2
4125,150,12,"['true', 'output']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2
4126,150,12,"['true', 'output']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2
4127,150,12,"['true', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
4128,150,12,"['true', 'output']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
4129,150,12,"['true', 'output']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2
4130,150,12,"['true', 'output']","if `True`, the output will be in `B x T x *` format.",torch.nn.utils.rnn.pad_packed_sequence.yaml,2
4131,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,3
4132,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,3
4133,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,3
4134,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,3
4135,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,3
4136,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,3
4137,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,3
4138,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,3
4139,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4140,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4141,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,3
4142,151,12,"['default', 'losses', 'averaged']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,3
4143,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,3
4144,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,3
4145,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,3
4146,152,12,"['SOME_STRUCTURE', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,3
4147,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,3
4148,152,12,"['SOME_STRUCTURE', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,3
4149,152,12,"['SOME_STRUCTURE', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,3
4150,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,3
4151,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,3
4152,152,12,"['SOME_STRUCTURE', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,3
4153,152,12,"['SOME_STRUCTURE', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,3
4154,152,12,"['SOME_STRUCTURE', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
4155,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
4156,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.kl_div.yaml,2
4157,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4158,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy.yaml,2
4159,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
4160,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.nll_loss.yaml,2
4161,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4162,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.cross_entropy.yaml,2
4163,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4164,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4165,153,12,"['reduce', 'false']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4166,153,12,"['reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.poisson_nll_loss.yaml,2
4167,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,5
4168,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,5
4169,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,5
4170,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,5
4171,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,5
4172,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,5
4173,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,5
4174,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor (minibatch , in _channels , iH , iW) ",torch.nn.functional.avg_pool2d.yaml,5
4175,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,5
4176,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,5
4177,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,5
4178,154,12,"['input', 'tensor', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,5
4179,155,12,"['input', 'tensor', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
4180,155,12,"['input', 'tensor', 'n']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,3
4181,155,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
4182,155,12,"['input', 'tensor', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4183,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4184,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
4185,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
4186,155,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
4187,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4188,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4189,155,12,"['input', 'tensor', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
4190,155,12,"['input', 'tensor', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
4191,156,12,"['tensor', 'size', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
4192,156,12,"['tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
4193,156,12,"['tensor', 'size', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4194,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4195,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
4196,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
4197,156,12,"['tensor', 'size', 'n']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,3
4198,156,12,"['tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
4199,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4200,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4201,156,12,"['tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
4202,156,12,"['tensor', 'size', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
4203,157,12,"['tensor', 'm']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
4204,157,12,"['tensor', 'm']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
4205,157,12,"['tensor', 'm']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
4206,157,12,"['tensor', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
4207,157,12,"['tensor', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
4208,157,12,"['tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
4209,157,12,"['tensor', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
4210,157,12,"['tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
4211,157,12,"['tensor', 'm']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,2
4212,157,12,"['tensor', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
4213,157,12,"['tensor', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
4214,157,12,"['tensor', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
4215,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,4
4216,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,4
4217,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,4
4218,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padW,).",torch.nn.functional.avg_pool1d.yaml,4
4219,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,4
4220,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,4
4221,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple `(padW,)`.",torch.nn.functional.conv_transpose1d.yaml,4
4222,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,4
4223,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,4
4224,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,4
4225,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,4
4226,158,12,"['single', 'number', 'SOME_STRUCTURE', 'padw']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,4
4227,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,4
4228,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,4
4229,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,4
4230,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sW,).",torch.nn.functional.avg_pool1d.yaml,4
4231,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,4
4232,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,4
4233,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple `(sW,)`.",torch.nn.functional.conv_transpose1d.yaml,4
4234,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,4
4235,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,4
4236,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,4
4237,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,4
4238,159,12,"['single', 'number', 'SOME_STRUCTURE', 'sw']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,4
4239,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,2
4240,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,2
4241,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,2
4242,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,2
4243,160,12,"['SOME_DTYPE', 'input']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
4244,160,12,"['SOME_DTYPE', 'input']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
4245,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,2
4246,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,2
4247,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,2
4248,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,2
4249,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,2
4250,160,12,"['SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,2
4251,161,12,"['SOME_DTYPE', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
4252,161,12,"['SOME_DTYPE', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
4253,161,12,"['SOME_DTYPE', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
4254,161,12,"['SOME_DTYPE', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
4255,161,12,"['SOME_DTYPE', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
4256,161,12,"['SOME_DTYPE', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
4257,161,12,"['SOME_DTYPE', 'parameters']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2
4258,161,12,"['SOME_DTYPE', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
4259,161,12,"['SOME_DTYPE', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
4260,161,12,"['SOME_DTYPE', 'parameters']","In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by `model.state_dict().values()`",torch.onnx.export.yaml,2
4261,161,12,"['SOME_DTYPE', 'parameters']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
4262,161,12,"['SOME_DTYPE', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
4263,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.max_memory_reserved.yaml,2
4264,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.memory_reserved.yaml,2
4265,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.reset_max_memory_allocated.yaml,2
4266,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.memory_stats.yaml,2
4267,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.memory_allocated.yaml,2
4268,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.current_stream.yaml,2
4269,162,12,"['selected', 'SOME_DTYPE']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
4270,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.set_device.yaml,2
4271,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.max_memory_allocated.yaml,2
4272,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.memory_summary.yaml,2
4273,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.default_stream.yaml,2
4274,162,12,"['selected', 'SOME_DTYPE']",selected device.,torch.cuda.reset_max_memory_cached.yaml,2
4275,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3
4276,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3
4277,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3
4278,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3
4279,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3
4280,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3
4281,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3
4282,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3
4283,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4284,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4285,163,12,"['none', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3
4286,163,12,"['none', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3
4287,164,12,"['torch', 'tensor']",a 2-dimensional torch.Tensor,torch.nn.init.eye_.yaml,2
4288,164,12,"['torch', 'tensor']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
4289,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,2
4290,164,12,"['torch', 'tensor']","a {3, 4, 5}-dimensional torch.Tensor",torch.nn.init.dirac_.yaml,2
4291,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,2
4292,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,2
4293,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,2
4294,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,2
4295,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,2
4296,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,2
4297,164,12,"['torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,2
4298,164,12,"['torch', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
4299,165,12,"['set', 'points']",the ending value for the set of points,torch.arange.yaml,2
4300,165,12,"['set', 'points']",the starting value for the set of points.,torch.arange.yaml,2
4301,165,12,"['set', 'points']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2
4302,165,12,"['set', 'points']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
4303,165,12,"['set', 'points']",the ending value for the set of points,torch.linspace.yaml,2
4304,165,12,"['set', 'points']",the starting value for the set of points,torch.linspace.yaml,2
4305,165,12,"['set', 'points']",the ending value for the set of points,torch.logspace.yaml,2
4306,165,12,"['set', 'points']",the starting value for the set of points,torch.logspace.yaml,2
4307,165,12,"['set', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
4308,165,12,"['set', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4309,165,12,"['set', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
4310,165,12,"['set', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4311,166,12,"['set', 'false']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
4312,166,12,"['set', 'false']",This flag should be set to `False` when this redirect causes issues.,torch.utils.cpp_extension.load_inline.yaml,2
4313,166,12,"['set', 'false']", Set this to False if you want to export an untrained model.,torch.onnx.export.yaml,2
4314,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2
4315,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2
4316,166,12,"['set', 'false']",Set to `True` for reduced QR decomposition and `False` for complete QR decomposition.,torch.qr.yaml,2
4317,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2
4318,166,12,"['set', 'false']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4319,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2
4320,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4321,166,12,"['set', 'false']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2
4322,166,12,"['set', 'false']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4323,167,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,2
4324,167,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,2
4325,167,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
4326,167,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,2
4327,167,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv2d.yaml,2
4328,167,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
4329,167,12,"['sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
4330,167,12,"['sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,2
4331,167,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv1d.yaml,2
4332,167,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv3d.yaml,2
4333,167,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv2d.yaml,2
4334,167,12,"['sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv3d.yaml,2
4335,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3
4336,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3
4337,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3
4338,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,3
4339,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3
4340,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,3
4341,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3
4342,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,3
4343,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4344,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4345,168,12,"['reduction', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3
4346,168,12,"['reduction', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,3
4347,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,3
4348,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,3
4349,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,3
4350,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,3
4351,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,3
4352,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,3
4353,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,3
4354,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,3
4355,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4356,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4357,169,12,"['losses', 'summed', 'minibatch']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,3
4358,169,12,"['losses', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,3
4359,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,3
4360,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.kl_div.yaml,3
4361,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,3
4362,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy.yaml,3
4363,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,3
4364,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.nll_loss.yaml,3
4365,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,3
4366,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.cross_entropy.yaml,3
4367,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4368,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
4369,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,3
4370,170,12,"['deprecated', 'see', 'reduction']",Deprecated (see `reduction`).,torch.nn.functional.poisson_nll_loss.yaml,3
4371,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
4372,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2
4373,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4374,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2
4375,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
4376,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2
4377,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4378,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2
4379,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4380,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4381,171,12,"['loss', 'batch']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4382,171,12,"['loss', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2
4383,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
4384,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2
4385,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4386,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2
4387,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
4388,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2
4389,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4390,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2
4391,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4392,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4393,172,12,"['loss', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4394,172,12,"['loss', 'element']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2
4395,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.empty_strided.yaml,3
4396,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.arange.yaml,3
4397,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.rand.yaml,3
4398,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.eye.yaml,3
4399,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.ones.yaml,3
4400,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.randperm.yaml,3
4401,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.linspace.yaml,3
4402,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.logspace.yaml,3
4403,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.zeros.yaml,3
4404,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.randn.yaml,3
4405,173,11,"['default', 'torch', 'strided']",Default: `torch.strided`.,torch.full.yaml,3
4406,174,11,"['input', 'tensor', 'size', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,4
4407,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,4
4408,174,11,"['input', 'tensor', 'size', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,4
4409,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,4
4410,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,4
4411,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,4
4412,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,4
4413,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,4
4414,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,4
4415,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,4
4416,174,11,"['input', 'tensor', 'size', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,4
4417,175,11,"['input', 'tensor', 'm']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,3
4418,175,11,"['input', 'tensor', 'm']",input tensor of shape B  times P  times M .,torch.cdist.yaml,3
4419,175,11,"['input', 'tensor', 'm']",input tensor of shape B  times R  times M .,torch.cdist.yaml,3
4420,175,11,"['input', 'tensor', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4421,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,3
4422,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4423,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4424,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4425,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4426,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4427,175,11,"['input', 'tensor', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
4428,176,11,"['input', 'n', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
4429,176,11,"['input', 'n', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
4430,176,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
4431,176,11,"['input', 'n', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3
4432,176,11,"['input', 'n', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4433,176,11,"['input', 'n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
4434,176,11,"['input', 'n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
4435,176,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
4436,176,11,"['input', 'n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4437,176,11,"['input', 'n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4438,176,11,"['input', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
4439,177,11,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.hvp.yaml,2
4440,177,11,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
4441,177,11,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.jvp.yaml,2
4442,177,11,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
4443,177,11,"['size', 'input']",Must be the same size as the input of `func`.,torch.autograd.functional.vhp.yaml,2
4444,177,11,"['size', 'input']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
4445,177,11,"['size', 'input']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
4446,177,11,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
4447,177,11,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
4448,177,11,"['size', 'input']",Should be of same size as input tensor.,torch.bincount.yaml,2
4449,177,11,"['size', 'input']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
4450,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
4451,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,2
4452,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,2
4453,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
4454,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
4455,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
4456,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
4457,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,2
4458,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']","If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated.",torch.norm.yaml,2
4459,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
4460,178,11,"['SOME_DTYPE', 'SOME_STRUCTURE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
4461,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,3
4462,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,3
4463,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,3
4464,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,3
4465,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,3
4466,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,3
4467,179,11,"['SOME_DTYPE', 'parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,3
4468,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,3
4469,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,3
4470,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,3
4471,179,11,"['SOME_DTYPE', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,3
4472,180,11,"['second', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,2
4473,180,11,"['second', 'tensor']",the second input tensor,torch.min22.yaml,2
4474,180,11,"['second', 'tensor']",the second input tensor,torch.max22.yaml,2
4475,180,11,"['second', 'tensor']",the second input tensor,torch.add.yaml,2
4476,180,11,"['second', 'tensor']",the second input tensor,torch.bitwise_and.yaml,2
4477,180,11,"['second', 'tensor']",the second input tensor,torch.cross.yaml,2
4478,180,11,"['second', 'tensor']",the second multiplicand tensor,torch.mul.yaml,2
4479,180,11,"['second', 'tensor']",the second tensor to be multiplied,torch.matmul.yaml,2
4480,180,11,"['second', 'tensor']",the second input tensor,torch.atan2.yaml,2
4481,180,11,"['second', 'tensor']",second tensor to compare,torch.allclose.yaml,2
4482,180,11,"['second', 'tensor']",the second input tensor,torch.bitwise_or.yaml,2
4483,181,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
4484,181,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
4485,181,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
4486,181,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2
4487,181,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2
4488,181,11,"['specified', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
4489,181,11,"['specified', 'input']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2
4490,181,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2
4491,181,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2
4492,181,11,"['specified', 'input']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2
4493,181,11,"['specified', 'input']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2
4494,182,11,"['set', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.empty_strided.yaml,2
4495,182,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
4496,182,11,"['set', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
4497,182,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
4498,182,11,"['set', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
4499,182,11,"['set', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
4500,182,11,"['set', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4501,182,11,"['set', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
4502,182,11,"['set', 'tensor']","If set, returned tensor would be allocated in the pinned memory.",torch.tensor.yaml,2
4503,182,11,"['set', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
4504,182,11,"['set', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4505,183,11,"['set', 'true']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2
4506,183,11,"['set', 'true']",Set it to `True` to force CUDA headers and libraries to be included.,torch.utils.cpp_extension.load_inline.yaml,2
4507,183,11,"['set', 'true']"," At the moment, ONNX is oriented towards exporting models for inference only, so you will generally not need to set this to True.",torch.onnx.export.yaml,2
4508,183,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout.yaml,2
4509,183,11,"['set', 'true']","if set to `True`, returns an info IntTensor.",torch.lu.yaml,2
4510,183,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout2d.yaml,2
4511,183,11,"['set', 'true']",Set it to True` to force CUDA headers and libraries to be included.,torch.utils.cpp_extension.load.yaml,2
4512,183,11,"['set', 'true']",Set to `True` for reduced QR decomposition and `False` for complete QR decomposition.,torch.qr.yaml,2
4513,183,11,"['set', 'true']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout3d.yaml,2
4514,183,11,"['set', 'true']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
4515,183,11,"['set', 'true']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
4516,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,2
4517,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,2
4518,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,2
4519,184,11,"['returns', 'SOME_DTYPE']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,2
4520,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,2
4521,184,11,"['returns', 'SOME_DTYPE']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,2
4522,184,11,"['returns', 'SOME_DTYPE']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2
4523,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,2
4524,184,11,"['returns', 'SOME_DTYPE']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,2
4525,184,11,"['returns', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,2
4526,184,11,"['returns', 'SOME_DTYPE']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,2
4527,185,11,"['floating', 'point']","If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see `get_default_dtype()`.",torch.arange.yaml,2
4528,185,11,"['floating', 'point']",the floating point dtype to make the default,torch.set_default_dtype.yaml,2
4529,185,11,"['floating', 'point']",Only floating point types are supported.,torch.hann_window.yaml,2
4530,185,11,"['floating', 'point']",A floating point value to determine the cutoff for small singular values.,torch.pinverse.yaml,2
4531,185,11,"['floating', 'point']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
4532,185,11,"['floating', 'point']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2
4533,185,11,"['floating', 'point']",Only floating point types are supported.,torch.hamming_window.yaml,2
4534,185,11,"['floating', 'point']",Only floating point types are supported.,torch.blackman_window.yaml,2
4535,185,11,"['floating', 'point']",Only floating point types are supported.,torch.bartlett_window.yaml,2
4536,185,11,"['floating', 'point']",the floating point tensor type or its name,torch.set_default_tensor_type.yaml,2
4537,185,11,"['floating', 'point']"," Note that when scale_factor is floating-point, the recomputed scale_factor may differ from the one passed in due to rounding and precision issues.",torch.nn.functional.interpolate.yaml,2
4538,186,11,"['batch', 'matrices']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
4539,186,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,2
4540,186,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,2
4541,186,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.addbmm.yaml,2
4542,186,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.addbmm.yaml,2
4543,186,11,"['batch', 'matrices']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
4544,186,11,"['batch', 'matrices']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
4545,186,11,"['batch', 'matrices']",the first batch of matrices to be multiplied,torch.bmm.yaml,2
4546,186,11,"['batch', 'matrices']",the second batch of matrices to be multiplied,torch.bmm.yaml,2
4547,186,11,"['batch', 'matrices']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
4548,186,11,"['batch', 'matrices']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
4549,187,11,"['matrix', 'multiplied']",matrix to be multiplied,torch.mv.yaml,2
4550,187,11,"['matrix', 'multiplied']",the matrix to be multiplied.,torch.ormqr.yaml,2
4551,187,11,"['matrix', 'multiplied']",the first matrix to be multiplied,torch.addmm.yaml,2
4552,187,11,"['matrix', 'multiplied']",the second matrix to be multiplied,torch.addmm.yaml,2
4553,187,11,"['matrix', 'multiplied']",a sparse matrix to be multiplied,torch.sparse.addmm.yaml,2
4554,187,11,"['matrix', 'multiplied']",a dense matrix be multiplied,torch.sparse.addmm.yaml,2
4555,187,11,"['matrix', 'multiplied']",the first sparse matrix to be multiplied,torch.sparse.mm.yaml,2
4556,187,11,"['matrix', 'multiplied']",the second dense matrix to be multiplied,torch.sparse.mm.yaml,2
4557,187,11,"['matrix', 'multiplied']",matrix to be multiplied,torch.addmv.yaml,2
4558,187,11,"['matrix', 'multiplied']",the first matrix to be multiplied,torch.mm.yaml,2
4559,187,11,"['matrix', 'multiplied']",the second matrix to be multiplied,torch.mm.yaml,2
4560,188,11,"['dimensional', 'torch', 'tensor']",a 2-dimensional torch.Tensor,torch.nn.init.eye_.yaml,3
4561,188,11,"['dimensional', 'torch', 'tensor']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,3
4562,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,3
4563,188,11,"['dimensional', 'torch', 'tensor']","a {3, 4, 5}-dimensional torch.Tensor",torch.nn.init.dirac_.yaml,3
4564,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,3
4565,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,3
4566,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,3
4567,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,3
4568,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,3
4569,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,3
4570,188,11,"['dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,3
4571,189,11,"['m', 'm']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
4572,189,11,"['m', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,2
4573,189,11,"['m', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
4574,189,11,"['m', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
4575,189,11,"['m', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,2
4576,189,11,"['m', 'm']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
4577,189,11,"['m', 'm']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
4578,189,11,"['m', 'm']","m-elements tuple, where  m/2  <=  input dimensions and m  is even.",torch.nn.functional.pad.yaml,2
4579,189,11,"['m', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
4580,189,11,"['m', 'm']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
4581,189,11,"['m', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
4582,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
4583,190,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
4584,190,10,"['output', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
4585,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
4586,190,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
4587,190,10,"['output', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
4588,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
4589,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
4590,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4591,190,10,"['output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
4592,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.ones_like.yaml,5
4593,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.ones_like.yaml,5
4594,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.empty_like.yaml,5
4595,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.empty_like.yaml,5
4596,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.zeros_like.yaml,5
4597,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.zeros_like.yaml,5
4598,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.randn_like.yaml,5
4599,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.randn_like.yaml,5
4600,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the device of `input`.",torch.rand_like.yaml,5
4601,191,10,"['default', 'none', 'defaults', 'SOME_DTYPE', 'input']","Default: if `None`, defaults to the layout of `input`.",torch.rand_like.yaml,5
4602,192,10,"['input', 'n', 'batch']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
4603,192,10,"['input', 'n', 'batch']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3
4604,192,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
4605,192,10,"['input', 'n', 'batch']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4606,192,10,"['input', 'n', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
4607,192,10,"['input', 'n', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
4608,192,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
4609,192,10,"['input', 'n', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4610,192,10,"['input', 'n', 'batch']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4611,192,10,"['input', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
4612,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2
4613,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2
4614,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2
4615,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2
4616,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2
4617,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2
4618,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2
4619,193,10,"['input', 'number']",an input tensor or number,torch.result_type.yaml,2
4620,193,10,"['input', 'number']",an input tensor or number,torch.result_type.yaml,2
4621,193,10,"['input', 'number']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2
4622,194,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3
4623,194,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3
4624,194,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3
4625,194,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3
4626,194,10,"['input', 'output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3
4627,194,10,"['input', 'output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
4628,194,10,"['input', 'output', 'tensor']",Input and output GPU tensors of the collective.,torch.distributed.reduce_multigpu.yaml,3
4629,194,10,"['input', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3
4630,194,10,"['input', 'output', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3
4631,194,10,"['input', 'output', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
4632,195,10,"['tensor', 'zero']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,2
4633,195,10,"['tensor', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,2
4634,195,10,"['tensor', 'zero']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
4635,195,10,"['tensor', 'zero']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,2
4636,195,10,"['tensor', 'zero']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,2
4637,195,10,"['tensor', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,2
4638,195,10,"['tensor', 'zero']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
4639,195,10,"['tensor', 'zero']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
4640,195,10,"['tensor', 'zero']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
4641,195,10,"['tensor', 'zero']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,2
4642,196,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.le.yaml,2
4643,196,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.gt.yaml,2
4644,196,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.ne.yaml,2
4645,196,10,"['tensor', 'must']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
4646,196,10,"['tensor', 'must']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
4647,196,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.ge.yaml,2
4648,196,10,"['tensor', 'must']",the output tensor that must be a BoolTensor,torch.lt.yaml,2
4649,196,10,"['tensor', 'must']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2
4650,196,10,"['tensor', 'must']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
4651,196,10,"['tensor', 'must']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
4652,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_reduce.yaml,4
4653,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.barrier.yaml,4
4654,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_gather.yaml,4
4655,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.gather.yaml,4
4656,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.broadcast_multigpu.yaml,4
4657,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.reduce.yaml,4
4658,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.all_gather_multigpu.yaml,4
4659,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.scatter.yaml,4
4660,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.reduce_multigpu.yaml,4
4661,197,10,"['whether', 'op', 'async', 'op']",Whether this op should be an async op,torch.distributed.broadcast.yaml,4
4662,198,10,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
4663,198,10,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
4664,198,10,"['size', 'output']",Must be the same size as the output of `func`.,torch.autograd.functional.vjp.yaml,2
4665,198,10,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
4666,198,10,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
4667,198,10,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
4668,198,10,"['size', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
4669,198,10,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
4670,198,10,"['size', 'output']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
4671,198,10,"['size', 'output']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
4672,199,10,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
4673,199,10,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
4674,199,10,"['single', 'element']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
4675,199,10,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
4676,199,10,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
4677,199,10,"['single', 'element']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
4678,199,10,"['single', 'element']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2
4679,199,10,"['single', 'element']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2
4680,199,10,"['single', 'element']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2
4681,199,10,"['single', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
4682,200,10,"['SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
4683,200,10,"['SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,2
4684,200,10,"['SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
4685,200,10,"['SOME_DTYPE', 'output']",a device on which the output will be placed (default: current device).,torch.cuda.comm.reduce_add.yaml,2
4686,200,10,"['SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,2
4687,200,10,"['SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,2
4688,200,10,"['SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
4689,200,10,"['SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,2
4690,200,10,"['SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,2
4691,200,10,"['SOME_DTYPE', 'output']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
4692,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2
4693,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2
4694,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2
4695,201,10,"['false', 'return']","If False, return a symmetric window.",torch.hann_window.yaml,2
4696,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2
4697,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2
4698,201,10,"['false', 'return']","If False, return a symmetric window.",torch.hamming_window.yaml,2
4699,201,10,"['false', 'return']","If False, return a symmetric window.",torch.blackman_window.yaml,2
4700,201,10,"['false', 'return']","If False, return a symmetric window.",torch.bartlett_window.yaml,2
4701,201,10,"['false', 'return']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2
4702,202,10,"['second', 'input']",the second input tensor,torch.bitwise_xor.yaml,2
4703,202,10,"['second', 'input']",the second input tensor,torch.min22.yaml,2
4704,202,10,"['second', 'input']",the second input tensor,torch.max22.yaml,2
4705,202,10,"['second', 'input']",the second input tensor,torch.add.yaml,2
4706,202,10,"['second', 'input']",Second input (of size matching x1).,torch.nn.functional.cosine_similarity.yaml,2
4707,202,10,"['second', 'input']",the second input tensor,torch.bitwise_and.yaml,2
4708,202,10,"['second', 'input']",the second input tensor,torch.cross.yaml,2
4709,202,10,"['second', 'input']",the second input tensor,torch.atan2.yaml,2
4710,202,10,"['second', 'input']",the second input tensor,torch.bitwise_or.yaml,2
4711,202,10,"['second', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2
4712,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,5
4713,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,5
4714,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,5
4715,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,5
4716,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,5
4717,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,5
4718,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,5
4719,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,5
4720,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,5
4721,203,10,"['specified', 'input', 'tensor', 'casted', 'dtype']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,5
4722,204,10,"['number', 'tensor']",any number of 1 dimensional tensors.,torch.cartesian_prod.yaml,2
4723,204,10,"['number', 'tensor']","the divisor, which may be either a number or a tensor of the same shape as the dividend",torch.fmod.yaml,2
4724,204,10,"['number', 'tensor']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
4725,204,10,"['number', 'tensor']",the divisor that may be either a number or a Tensor of the same shape as the dividend,torch.remainder.yaml,2
4726,204,10,"['number', 'tensor']",Has to be between 0 and the number of dimensions of concatenated tensors (inclusive),torch.stack.yaml,2
4727,204,10,"['number', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
4728,204,10,"['number', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
4729,204,10,"['number', 'tensor']",any number of tensors of the same type,torch.broadcast_tensors.yaml,2
4730,204,10,"['number', 'tensor']",The number of places by which the elements of the tensor are shifted.,torch.roll.yaml,2
4731,204,10,"['number', 'tensor']",the number to fill the output tensor with.,torch.full.yaml,2
4732,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.hvp.yaml,2
4733,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.vjp.yaml,2
4734,205,10,"['inputs', 'function']",inputs to the function,torch.autograd.gradgradcheck.yaml,2
4735,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.jvp.yaml,2
4736,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.vhp.yaml,2
4737,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.jacobian.yaml,2
4738,205,10,"['inputs', 'function']",A tuple of example inputs that will be passed to the function while tracing.,torch.jit.trace.yaml,2
4739,205,10,"['inputs', 'function']",inputs to the function,torch.autograd.gradcheck.yaml,2
4740,205,10,"['inputs', 'function']",inputs to the function `func`.,torch.autograd.functional.hessian.yaml,2
4741,205,10,"['inputs', 'function']",tuple containing inputs to the `function`,torch.utils.checkpoint.checkpoint.yaml,2
4742,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,8
4743,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,8
4744,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,8
4745,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_stats.yaml,8
4746,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,8
4747,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns the currently selected `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.current_stream.yaml,8
4748,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,8
4749,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns printout for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_summary.yaml,8
4750,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,8
4751,206,10,"['returns', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,8
4752,207,10,"['must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,2
4753,207,10,"['must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,2
4754,207,10,"['must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,2
4755,207,10,"['must', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
4756,207,10,"['must', 'SOME_DTYPE']",Must be a ByteTensor,torch.eq.yaml,2
4757,207,10,"['must', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
4758,207,10,"['must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,2
4759,207,10,"['must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,2
4760,207,10,"['must', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
4761,207,10,"['must', 'SOME_DTYPE']","the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.",torch.pca_lowrank.yaml,2
4762,208,10,"['n', 'dimensional', 'tensor']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,3
4763,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,3
4764,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,3
4765,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,3
4766,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,3
4767,208,10,"['n', 'dimensional', 'tensor']",N-dimensional tensor,torch.nn.functional.pad.yaml,3
4768,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,3
4769,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,3
4770,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,3
4771,208,10,"['n', 'dimensional', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,3
4772,209,10,"['n', 'torch']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,2
4773,209,10,"['n', 'torch']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2
4774,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,2
4775,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,2
4776,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,2
4777,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,2
4778,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,2
4779,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,2
4780,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,2
4781,209,10,"['n', 'torch']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,2
4782,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
4783,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
4784,210,10,"['per', 'element']",the tensor of per-element means,torch.normal.yaml,2
4785,210,10,"['per', 'element']",the tensor of per-element standard deviations,torch.normal.yaml,2
4786,210,10,"['per', 'element']",the tensor of per-element means,torch.normal22.yaml,2
4787,210,10,"['per', 'element']",the tensor of per-element standard deviations,torch.normal2.yaml,2
4788,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
4789,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
4790,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
4791,210,10,"['per', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
4792,211,10,"['m', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
4793,211,10,"['m', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
4794,211,10,"['m', 'dimensions']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
4795,211,10,"['m', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
4796,211,10,"['m', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
4797,211,10,"['m', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
4798,211,10,"['m', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
4799,211,10,"['m', 'dimensions']","m-elements tuple, where  m/2  <=  input dimensions and m  is even.",torch.nn.functional.pad.yaml,2
4800,211,10,"['m', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
4801,211,10,"['m', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
4802,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2
4803,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2
4804,212,9,"['split', 'input']",dimension on which to split the input.,torch.nn.functional.glu.yaml,2
4805,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2
4806,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2
4807,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2
4808,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2
4809,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2
4810,212,9,"['split', 'input']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2
4811,213,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2
4812,213,9,"['true', 'computed']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2
4813,213,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2
4814,213,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2
4815,213,9,"['true', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2
4816,213,9,"['true', 'computed']","If `True`, the Jacobian will be computed in a differentiable manner.",torch.autograd.functional.jacobian.yaml,2
4817,213,9,"['true', 'computed']","If `True`, the Hessian will be computed in a differentiable manner.",torch.autograd.functional.hessian.yaml,2
4818,213,9,"['true', 'computed']","if `True` the loss is computed as  exp(input) - target * input , if `False` then loss is input - target *  log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2
4819,213,9,"['true', 'computed']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2
4820,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,3
4821,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,3
4822,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,3
4823,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,3
4824,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,3
4825,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,3
4826,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,3
4827,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,3
4828,214,9,"['SOME_STRUCTURE', 'SOME_DTYPE', 'output']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
4829,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.std2.yaml,2
4830,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.mean2.yaml,2
4831,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.sum2.yaml,2
4832,215,9,"['dimension', 'dimensions']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,2
4833,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,2
4834,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,2
4835,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.var2.yaml,2
4836,215,9,"['dimension', 'dimensions']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,2
4837,215,9,"['dimension', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
4838,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,6
4839,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,6
4840,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,6
4841,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,6
4842,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,6
4843,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor (minibatch , in _channels , iH , iW) ",torch.nn.functional.avg_pool2d.yaml,6
4844,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,6
4845,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,6
4846,216,9,"['input', 'tensor', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,6
4847,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,5
4848,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,5
4849,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,5
4850,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,5
4851,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,5
4852,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,5
4853,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,5
4854,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,5
4855,217,9,"['input', 'tensor', 'size', 'n', 'n']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,5
4856,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,7
4857,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,7
4858,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,7
4859,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,7
4860,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,7
4861,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,7
4862,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,7
4863,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,7
4864,218,9,"['input', 'tensor', 'size', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,7
4865,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,6
4866,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.avg_pool1d.yaml,6
4867,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,6
4868,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,6
4869,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv_transpose1d.yaml,6
4870,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iW) ",torch.nn.functional.conv1d.yaml,6
4871,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,6
4872,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,6
4873,219,9,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,6
4874,220,9,"['input', 'size', 'm', 'm']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,4
4875,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,4
4876,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4
4877,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,4
4878,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4
4879,220,9,"['input', 'size', 'm', 'm']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,4
4880,220,9,"['input', 'size', 'm', 'm']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,4
4881,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,4
4882,220,9,"['input', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,4
4883,221,9,"['tensor', 'values']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,2
4884,221,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
4885,221,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
4886,221,9,"['tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
4887,221,9,"['tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
4888,221,9,"['tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4889,221,9,"['tensor', 'values']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
4890,221,9,"['tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
4891,221,9,"['tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4892,222,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
4893,222,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
4894,222,9,"['tensor', 'single']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
4895,222,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
4896,222,9,"['tensor', 'single']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2
4897,222,9,"['tensor', 'single']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2
4898,222,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
4899,222,9,"['tensor', 'single']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
4900,222,9,"['tensor', 'single']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
4901,223,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
4902,223,9,"['tensor', 'containing']",the output tensor containing indices,torch.nonzero.yaml,2
4903,223,9,"['tensor', 'containing']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
4904,223,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
4905,223,9,"['tensor', 'containing']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
4906,223,9,"['tensor', 'containing']",Tensor containing indices into the embedding matrix,torch.nn.functional.embedding.yaml,2
4907,223,9,"['tensor', 'containing']",the tensor containing the binary mask to index with,torch.masked_select.yaml,2
4908,223,9,"['tensor', 'containing']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2
4909,223,9,"['tensor', 'containing']",the input tensor containing probabilities,torch.multinomial.yaml,2
4910,224,9,"['tensor', 'size', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,3
4911,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,3
4912,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4913,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,3
4914,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,3
4915,224,9,"['tensor', 'size', 'm']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,3
4916,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
4917,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
4918,224,9,"['tensor', 'size', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,3
4919,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,2
4920,225,9,"['tensor', 'operation']",Tensors that participate in the collective operation.,torch.distributed.broadcast_multigpu.yaml,2
4921,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,2
4922,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,2
4923,225,9,"['tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4924,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,2
4925,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,2
4926,225,9,"['tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,2
4927,225,9,"['tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4928,226,9,"['target', 'output']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
4929,226,9,"['target', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
4930,226,9,"['target', 'output']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
4931,226,9,"['target', 'output']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
4932,226,9,"['target', 'output']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
4933,226,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
4934,226,9,"['target', 'output']",the target output image size.,torch.nn.functional.affine_grid.yaml,2
4935,226,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
4936,226,9,"['target', 'output']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
4937,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,5
4938,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,5
4939,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,5
4940,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,5
4941,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,5
4942,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,5
4943,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,5
4944,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,5
4945,227,9,"['size', 'm', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,5
4946,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
4947,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
4948,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
4949,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
4950,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
4951,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
4952,228,9,"['single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
4953,228,9,"['single', 'SOME_DTYPE']",a single vector represents the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2
4954,228,9,"['single', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
4955,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,5
4956,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.avg_pool2d.yaml,5
4957,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple `(padH, padW)`.",torch.nn.functional.conv_transpose2d.yaml,5
4958,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.conv2d.yaml,5
4959,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple `(padT, padH, padW)`.",torch.nn.functional.conv_transpose3d.yaml,5
4960,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.functional.avg_pool2d.yaml,5
4961,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padT, padH, padW).",torch.nn.functional.conv3d.yaml,5
4962,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padH, padW).",torch.nn.quantized.functional.conv2d.yaml,5
4963,229,9,"['single', 'number', 'SOME_STRUCTURE', 'padh', 'padw']","Can be a single number or a tuple (padD, padH, padW).",torch.nn.quantized.functional.conv3d.yaml,5
4964,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.avg_pool3d.yaml,5
4965,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.avg_pool2d.yaml,5
4966,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple `(sH, sW)`.",torch.nn.functional.conv_transpose2d.yaml,5
4967,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.conv2d.yaml,5
4968,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple `(sT, sH, sW)`.",torch.nn.functional.conv_transpose3d.yaml,5
4969,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.functional.avg_pool2d.yaml,5
4970,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sT, sH, sW).",torch.nn.functional.conv3d.yaml,5
4971,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sH, sW).",torch.nn.quantized.functional.conv2d.yaml,5
4972,230,9,"['single', 'number', 'SOME_STRUCTURE', 'sh', 'sw']","Can be a single number or a tuple (sD, sH, sW).",torch.nn.quantized.functional.conv3d.yaml,5
4973,231,9,"['SOME_DTYPE', 'name']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2
4974,231,9,"['SOME_DTYPE', 'name']",device for which to return the name.,torch.cuda.get_device_name.yaml,2
4975,231,9,"['SOME_DTYPE', 'name']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
4976,231,9,"['SOME_DTYPE', 'name']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2
4977,231,9,"['SOME_DTYPE', 'name']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2
4978,231,9,"['SOME_DTYPE', 'name']",a string of entrypoint name defined in repo's hubconf.py,torch.hub.load.yaml,2
4979,231,9,"['SOME_DTYPE', 'name']",a string of entrypoint name defined in repo's hubconf.py,torch.hub.help.yaml,2
4980,231,9,"['SOME_DTYPE', 'name']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2
4981,231,9,"['SOME_DTYPE', 'name']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2
4982,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,2
4983,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,2
4984,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,2
4985,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,2
4986,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,2
4987,232,9,"['false', 'tensor']","If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor.",torch.lu.yaml,2
4988,232,9,"['false', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
4989,232,9,"['false', 'tensor']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,2
4990,232,9,"['false', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
4991,233,9,"['first', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,2
4992,233,9,"['first', 'tensor']",the first input tensor,torch.add.yaml,2
4993,233,9,"['first', 'tensor']",the first input tensor,torch.bitwise_and.yaml,2
4994,233,9,"['first', 'tensor']",the first multiplicand tensor,torch.mul.yaml,2
4995,233,9,"['first', 'tensor']",the first tensor to be multiplied,torch.matmul.yaml,2
4996,233,9,"['first', 'tensor']",the first input tensor,torch.atan2.yaml,2
4997,233,9,"['first', 'tensor']",first tensor to compare,torch.allclose.yaml,2
4998,233,9,"['first', 'tensor']",the first input tensor,torch.bitwise_or.yaml,2
4999,233,9,"['first', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
5000,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,6
5001,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,6
5002,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,6
5003,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,6
5004,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,6
5005,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,6
5006,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,6
5007,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,6
5008,234,9,"['specified', 'input', 'tensor', 'casted', 'dtype', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,6
5009,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.prod2.yaml,5
5010,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.sum2.yaml,5
5011,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.sum.yaml,5
5012,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.log_softmax.yaml,5
5013,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.cumprod.yaml,5
5014,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.prod.yaml,5
5015,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.softmin.yaml,5
5016,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.cumsum.yaml,5
5017,235,9,"['useful', 'preventing', 'data', 'type', 'overflows']",This is useful for preventing data type overflows.,torch.nn.functional.softmax.yaml,5
5018,236,9,"['number', 'default']","Can be a single number or a tuple (padT, padH, padW), Default: 0",torch.nn.functional.avg_pool3d.yaml,2
5019,236,9,"['number', 'default']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2
5020,236,9,"['number', 'default']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5021,236,9,"['number', 'default']",the number of columns with default being `n`,torch.eye.yaml,2
5022,236,9,"['number', 'default']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2
5023,236,9,"['number', 'default']",The number of characters per line for the purpose of inserting line breaks (default = 80).,torch.set_printoptions.yaml,2
5024,236,9,"['number', 'default']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2
5025,236,9,"['number', 'default']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2
5026,236,9,"['number', 'default']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2
5027,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,2
5028,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,2
5029,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,2
5030,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,2
5031,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,2
5032,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,2
5033,237,9,"['number', 'groups']",number of groups in the conv layer (default: 1),torch.nn.init.dirac_.yaml,2
5034,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,2
5035,237,9,"['number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,2
5036,238,9,"['number', 'dimensions']",Ellipses  u2026 represent a fixed number of dimensions.,torch.einsum.yaml,2
5037,238,9,"['number', 'dimensions']",number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively,torch.tensordot.yaml,2
5038,238,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.irfft.yaml,2
5039,238,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.fft.yaml,2
5040,238,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.ifft.yaml,2
5041,238,9,"['number', 'dimensions']",Has to be between 0 and the number of dimensions of concatenated tensors (inclusive),torch.stack.yaml,2
5042,238,9,"['number', 'dimensions']",the number of dimensions,torch.mvlgamma.yaml,2
5043,238,9,"['number', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
5044,238,9,"['number', 'dimensions']",the number of dimensions in each signal.,torch.rfft.yaml,2
5045,239,9,"['number', 'elements']",number of elements to combine,torch.combinations.yaml,2
5046,239,9,"['number', 'elements']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5047,239,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
5048,239,9,"['number', 'elements']",Total number of array elements which trigger summarization rather than full repr (default = 1000).,torch.set_printoptions.yaml,2
5049,239,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
5050,239,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
5051,239,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5052,239,9,"['number', 'elements']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
5053,239,9,"['number', 'elements']",The number of places by which the elements of the tensor are shifted.,torch.roll.yaml,2
5054,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5055,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5056,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5057,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5058,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5059,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5060,240,9,"['python', 'function']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2
5061,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5062,240,9,"['python', 'function']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5063,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5064,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5065,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5066,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5067,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5068,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5069,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5070,241,9,"['python', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5071,241,9,"['python', 'tensor']",any python sequence of tensors of the same type.,torch.cat.yaml,2
5072,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5073,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5074,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5075,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5076,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5077,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5078,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5079,242,9,"['function', 'takes']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5080,242,9,"['function', 'takes']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
5081,243,9,"['function', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5082,243,9,"['function', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5083,243,9,"['function', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5084,243,9,"['function', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5085,243,9,"['function', 'SOME_STRUCTURE']",a function or a dict specifying how to remap storage locations (see torch.load),torch.utils.model_zoo.load_url.yaml,2
5086,243,9,"['function', 'SOME_STRUCTURE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,2
5087,243,9,"['function', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5088,243,9,"['function', 'SOME_STRUCTURE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
5089,243,9,"['function', 'SOME_STRUCTURE']",a function or a dict specifying how to remap storage locations (see torch.load),torch.hub.load_state_dict_from_url.yaml,2
5090,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5091,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5092,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5093,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5094,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5095,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5096,244,9,"['returns', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2
5097,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5098,244,9,"['returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5099,245,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5100,245,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5101,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
5102,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
5103,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
5104,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
5105,245,9,"['returns', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5106,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5107,245,9,"['returns', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
5108,246,9,"['kh', 'kw']","Can be a single number or a tuple (kT, kH, kW)",torch.nn.functional.avg_pool3d.yaml,2
5109,246,9,"['kh', 'kw']","Can be a single number or a tuple (kH, kW)",torch.nn.quantized.functional.avg_pool2d.yaml,2
5110,246,9,"['kh', 'kw']","filters of shape (in _channels ,  out _channels/groups , kH , kW) ",torch.nn.functional.conv_transpose2d.yaml,2
5111,246,9,"['kh', 'kw']","filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.functional.conv2d.yaml,2
5112,246,9,"['kh', 'kw']","filters of shape (in _channels ,  out _channels/groups , kT , kH , kW) ",torch.nn.functional.conv_transpose3d.yaml,2
5113,246,9,"['kh', 'kw']","Can be a single number or a tuple (kH, kW)",torch.nn.functional.avg_pool2d.yaml,2
5114,246,9,"['kh', 'kw']","filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",torch.nn.functional.conv3d.yaml,2
5115,246,9,"['kh', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,2
5116,246,9,"['kh', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,2
5117,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,4
5118,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,4
5119,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,4
5120,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv2d.yaml,4
5121,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,4
5122,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv1d.yaml,4
5123,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.functional.conv3d.yaml,4
5124,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv2d.yaml,4
5125,247,9,"['implicit', 'paddings', 'sides', 'input']",implicit paddings on both sides of the input.,torch.nn.quantized.functional.conv3d.yaml,4
5126,248,9,"['n', 'dimensional', 'torch', 'tensor']","an n-dimensional torch.Tensor, where n  >= 2 ",torch.nn.init.orthogonal_.yaml,4
5127,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_normal_.yaml,4
5128,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.zeros_.yaml,4
5129,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.uniform_.yaml,4
5130,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.normal_.yaml,4
5131,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.constant_.yaml,4
5132,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.xavier_uniform_.yaml,4
5133,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.sparse_.yaml,4
5134,248,9,"['n', 'dimensional', 'torch', 'tensor']",an n-dimensional torch.Tensor,torch.nn.init.ones_.yaml,4
5135,249,9,"['|', '|']",padding mode for outside grid values `'zeros'` | `'border'` | `'reflection'`.,torch.nn.functional.grid_sample.yaml,2
5136,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
5137,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
5138,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
5139,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
5140,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
5141,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5142,249,9,"['|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
5143,249,9,"['|', '|']",algorithm used for upsampling: `'nearest'` | `'linear'` | `'bilinear'` | `'bicubic'` | `'trilinear'` | `'area'`.,torch.nn.functional.interpolate.yaml,2
5144,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.l1_unstructured.yaml,2
5145,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_unstructured.yaml,2
5146,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.ln_structured.yaml,2
5147,250,9,"['name', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
5148,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_structured.yaml,2
5149,250,9,"['name', 'SOME_DTYPE']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
5150,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.remove.yaml,2
5151,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.custom_from_mask.yaml,2
5152,250,9,"['name', 'SOME_DTYPE']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.identity.yaml,2
5153,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,3
5154,251,9,"['reduction', 'output', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,3
5155,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,3
5156,251,9,"['reduction', 'output', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,3
5157,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,3
5158,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,3
5159,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,3
5160,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
5161,251,9,"['reduction', 'output', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,3
5162,252,9,"['m', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,2
5163,252,9,"['m', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
5164,252,9,"['m', 'n']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,2
5165,252,9,"['m', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
5166,252,9,"['m', 'n']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
5167,252,9,"['m', 'n']",the m  by n  matrix A ,torch.lstsq.yaml,2
5168,252,9,"['m', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
5169,252,9,"['m', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,2
5170,252,9,"['m', 'n']","By default, `q = min(6, m, n)`.",torch.pca_lowrank.yaml,2
5171,253,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.irfft.yaml,3
5172,253,9,"['controls', 'whether', 'return']",controls whether to return largest or smallest elements,torch.topk.yaml,3
5173,253,9,"['controls', 'whether', 'return']",controls whether to return the elements in sorted order,torch.topk.yaml,3
5174,253,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.fft.yaml,3
5175,253,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.ifft.yaml,3
5176,253,9,"['controls', 'whether', 'return']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,3
5177,253,9,"['controls', 'whether', 'return']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,3
5178,253,9,"['controls', 'whether', 'return']",controls whether to return normalized results.,torch.rfft.yaml,3
5179,253,9,"['controls', 'whether', 'return']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,3
5180,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose2d.yaml,7
5181,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv2d.yaml,7
5182,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose3d.yaml,7
5183,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv_transpose1d.yaml,7
5184,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv1d.yaml,7
5185,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.functional.conv3d.yaml,7
5186,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv2d.yaml,7
5187,254,8,"['split', 'input', 'groups', '_channels', 'divisible', 'number', 'groups']","split input into groups, in _channels  should be divisible by the number of groups.",torch.nn.quantized.functional.conv3d.yaml,7
5188,255,8,"['output', 'tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,3
5189,255,8,"['output', 'tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,3
5190,255,8,"['output', 'tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,3
5191,255,8,"['output', 'tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,3
5192,255,8,"['output', 'tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,3
5193,255,8,"['output', 'tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,3
5194,255,8,"['output', 'tensor', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,3
5195,255,8,"['output', 'tensor', 'SOME_DTYPE']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,3
5196,256,8,"['output', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
5197,256,8,"['output', 'single']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
5198,256,8,"['output', 'single']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
5199,256,8,"['output', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
5200,256,8,"['output', 'single']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
5201,256,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
5202,256,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
5203,256,8,"['output', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
5204,257,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
5205,257,8,"['output', 'values']",interpolation mode to calculate output values `'bilinear'` | `'nearest'`.,torch.nn.functional.grid_sample.yaml,2
5206,257,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
5207,257,8,"['output', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
5208,257,8,"['output', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
5209,257,8,"['output', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5210,257,8,"['output', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
5211,257,8,"['output', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5212,258,8,"['output', '|']",interpolation mode to calculate output values `'bilinear'` | `'nearest'`.,torch.nn.functional.grid_sample.yaml,2
5213,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,2
5214,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,2
5215,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,2
5216,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,2
5217,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,2
5218,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5219,258,8,"['output', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,2
5220,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
5221,259,8,"['output', 'summed']",The indices not apprearing in the output are summed over after multiplying the operands entries.,torch.einsum.yaml,2
5222,259,8,"['output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5223,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
5224,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
5225,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
5226,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5227,259,8,"['output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
5228,260,8,"['default', 'e']",Default: 1e-12,torch.nn.functional.normalize.yaml,2
5229,260,8,"['default', 'e']",Default: 1e-8,torch.nn.functional.cosine_similarity.yaml,2
5230,260,8,"['default', 'e']",Default: 1e-15,torch.pinverse.yaml,2
5231,260,8,"['default', 'e']",Default: 1e-08,torch.allclose.yaml,2
5232,260,8,"['default', 'e']",Default: 1e-05,torch.allclose.yaml,2
5233,260,8,"['default', 'e']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,2
5234,260,8,"['default', 'e']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,2
5235,260,8,"['default', 'e']",Default: 1e-8,torch.nn.functional.poisson_nll_loss.yaml,2
5236,261,8,"['default', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
5237,261,8,"['default', 'specified']",Default is the number of X  columns (when specified) or 1.,torch.lobpcg.yaml,2
5238,261,8,"['default', 'specified']","Default is ""env://"" if no `init_method` or `store` is specified.",torch.distributed.init_process_group.yaml,2
5239,261,8,"['default', 'specified']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2
5240,261,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.load.yaml,2
5241,261,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.help.yaml,2
5242,261,8,"['default', 'specified']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
5243,261,8,"['default', 'specified']",The default branch is master if not specified.,torch.hub.list.yaml,2
5244,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.as_tensor.yaml,3
5245,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.tensor.yaml,3
5246,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,3
5247,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']","Can be a list, tuple, NumPy `ndarray`, scalar, and other types.",torch.sparse_coo_tensor.yaml,3
5248,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,3
5249,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']",list of list of module names to fuse.,torch.quantization.fuse_modules.yaml,3
5250,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,3
5251,262,8,"['SOME_STRUCTURE', 'SOME_STRUCTURE', 'SOME_DTYPE']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
5252,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.std2.yaml,3
5253,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.mean2.yaml,3
5254,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.sum2.yaml,3
5255,263,8,"['dimension', 'dimensions', 'reduce']",a dimension or a list of dimensions to reduce.,torch.sparse.sum.yaml,3
5256,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.var_mean2.yaml,3
5257,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.logsumexp.yaml,3
5258,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.var2.yaml,3
5259,263,8,"['dimension', 'dimensions', 'reduce']",the dimension or dimensions to reduce.,torch.std_mean2.yaml,3
5260,264,8,"['input', 'tensor', 'size', 'm']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,4
5261,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,4
5262,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4
5263,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,4
5264,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,4
5265,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,4
5266,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,4
5267,264,8,"['input', 'tensor', 'size', 'm']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,4
5268,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,3
5269,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,3
5270,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,3
5271,265,8,"['input', 'tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
5272,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,3
5273,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,3
5274,265,8,"['input', 'tensor', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,3
5275,265,8,"['input', 'tensor', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
5276,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2
5277,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2
5278,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2
5279,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2
5280,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2
5281,266,8,"['input', 'independent']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5282,266,8,"['input', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2
5283,266,8,"['input', 'independent']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5284,267,8,"['input', 'times']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,2
5285,267,8,"['input', 'times']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
5286,267,8,"['input', 'times']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
5287,267,8,"['input', 'times']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
5288,267,8,"['input', 'times']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
5289,267,8,"['input', 'times']",whether to pad `input` on both sides so that the t -th frame is centered at time t  times hop _length .,torch.stft.yaml,2
5290,267,8,"['input', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
5291,267,8,"['input', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
5292,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,6
5293,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,6
5294,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,6
5295,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,6
5296,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,6
5297,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,6
5298,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,6
5299,268,8,"['input', 'size', 'm', 'zero', 'batch', 'dimensions']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,6
5300,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std2.yaml,4
5301,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var_mean.yaml,4
5302,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var.yaml,4
5303,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std_mean.yaml,4
5304,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var_mean2.yaml,4
5305,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.var2.yaml,4
5306,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std_mean2.yaml,4
5307,269,8,"['whether', 'use', 'unbiased', 'estimation']",whether to use the unbiased estimation or not,torch.std.yaml,4
5308,270,8,"['target', 'output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,3
5309,270,8,"['target', 'output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,3
5310,270,8,"['target', 'output', 'size']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,3
5311,270,8,"['target', 'output', 'size']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,3
5312,270,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3
5313,270,8,"['target', 'output', 'size']",the target output image size.,torch.nn.functional.affine_grid.yaml,3
5314,270,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3
5315,270,8,"['target', 'output', 'size']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3
5316,271,8,"['size', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
5317,271,8,"['size', 'single']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
5318,271,8,"['size', 'single']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
5319,271,8,"['size', 'single']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
5320,271,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
5321,271,8,"['size', 'single']",size of a single chunk or list of sizes for each chunk,torch.split.yaml,2
5322,271,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
5323,271,8,"['size', 'single']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
5324,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
5325,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,2
5326,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
5327,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,2
5328,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
5329,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
5330,272,8,"['size', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
5331,272,8,"['size', 'SOME_DTYPE']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
5332,273,8,"['size', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,2
5333,273,8,"['size', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,2
5334,273,8,"['size', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,2
5335,273,8,"['size', 'SOME_STRUCTURE']",size of a single chunk or list of sizes for each chunk,torch.split.yaml,2
5336,273,8,"['size', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,2
5337,273,8,"['size', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,2
5338,273,8,"['size', 'SOME_STRUCTURE']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2
5339,273,8,"['size', 'SOME_STRUCTURE']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2
5340,274,8,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
5341,274,8,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
5342,274,8,"['size', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
5343,274,8,"['size', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
5344,274,8,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
5345,274,8,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
5346,274,8,"['size', 'size']",If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements.,torch.sparse_coo_tensor.yaml,2
5347,274,8,"['size', 'size']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
5348,275,8,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,2
5349,275,8,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,2
5350,275,8,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,2
5351,275,8,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,2
5352,275,8,"['size', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,2
5353,275,8,"['size', 'tensor']",Size of the sparse tensor.,torch.sparse_coo_tensor.yaml,2
5354,275,8,"['size', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,2
5355,275,8,"['size', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
5356,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,4
5357,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,4
5358,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,4
5359,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple `(dW,)`.",torch.nn.functional.conv_transpose1d.yaml,4
5360,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,4
5361,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,4
5362,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,4
5363,276,8,"['single', 'number', 'SOME_STRUCTURE', 'dw']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,4
5364,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']",dictionary that maps float modules to quantized modules to be replaced.,torch.quantization.prepare_qat.yaml,3
5365,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3
5366,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,3
5367,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']","the inputs to the model, e.g., such that `model(*args)` is a valid invocation of the model.",torch.onnx.export.yaml,3
5368,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,3
5369,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3
5370,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3
5371,277,8,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_DTYPE']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,3
5372,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,3
5373,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,3
5374,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,3
5375,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,3
5376,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,3
5377,278,8,"['false', 'tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
5378,278,8,"['false', 'tensor', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,3
5379,278,8,"['false', 'tensor', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
5380,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_xor.yaml,3
5381,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.min22.yaml,3
5382,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.max22.yaml,3
5383,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.add.yaml,3
5384,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_and.yaml,3
5385,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.cross.yaml,3
5386,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.atan2.yaml,3
5387,279,8,"['second', 'input', 'tensor']",the second input tensor,torch.bitwise_or.yaml,3
5388,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
5389,280,8,"['none', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5390,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
5391,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
5392,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
5393,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5394,280,8,"['none', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
5395,280,8,"['none', 'output']","if not `None`, the output will be padded to have length `total_length`.",torch.nn.utils.rnn.pad_packed_sequence.yaml,2
5396,281,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.hann_window.yaml,2
5397,281,8,"['returned', 'window']",the size of returned window,torch.hann_window.yaml,2
5398,281,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.hamming_window.yaml,2
5399,281,8,"['returned', 'window']",the size of returned window,torch.hamming_window.yaml,2
5400,281,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.blackman_window.yaml,2
5401,281,8,"['returned', 'window']",the size of returned window,torch.blackman_window.yaml,2
5402,281,8,"['returned', 'window']",the desired layout of returned window tensor.,torch.bartlett_window.yaml,2
5403,281,8,"['returned', 'window']",the size of returned window,torch.bartlett_window.yaml,2
5404,282,8,"['number', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5405,282,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
5406,282,8,"['number', 'output']",Number of digits of precision for floating point output (default = 4).,torch.set_printoptions.yaml,2
5407,282,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
5408,282,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
5409,282,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5410,282,8,"['number', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
5411,282,8,"['number', 'output']",the number to fill the output tensor with.,torch.full.yaml,2
5412,283,8,"['shape', 'output', 'tensor']",the shape of the output tensor,torch.empty_strided.yaml,3
5413,283,8,"['shape', 'output', 'tensor']",the shape of the output tensor,torch.as_strided.yaml,3
5414,283,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,3
5415,283,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,3
5416,283,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,3
5417,283,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,3
5418,283,8,"['shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,3
5419,283,8,"['shape', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
5420,284,8,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
5421,284,8,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
5422,284,8,"['two', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,2
5423,284,8,"['two', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,2
5424,284,8,"['two', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,2
5425,284,8,"['two', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
5426,284,8,"['two', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
5427,284,8,"['two', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
5428,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,7
5429,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,7
5430,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,7
5431,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,7
5432,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,7
5433,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,7
5434,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,7
5435,285,8,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,7
5436,286,8,"['function', 'SOME_DTYPE']",This function is a no-op if this argument is a negative integer.,torch.cuda.get_device_name.yaml,2
5437,286,8,"['function', 'SOME_DTYPE']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2
5438,286,8,"['function', 'SOME_DTYPE']","a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has `PRUNING_TYPE='unstructured'`.",torch.nn.utils.prune.global_unstructured.yaml,2
5439,286,8,"['function', 'SOME_DTYPE']",This function is a no-op if this argument is a negative integer.,torch.cuda.get_device_capability.yaml,2
5440,286,8,"['function', 'SOME_DTYPE']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2
5441,286,8,"['function', 'SOME_DTYPE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,2
5442,286,8,"['function', 'SOME_DTYPE']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2
5443,286,8,"['function', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,2
5444,287,8,"['vector', 'product']",The vector for which the Hessian vector product is computed.,torch.autograd.functional.hvp.yaml,2
5445,287,8,"['vector', 'product']",The vector for which the vector Jacobian product is computed.,torch.autograd.functional.vjp.yaml,2
5446,287,8,"['vector', 'product']",the first vector of the outer product,torch.addr.yaml,2
5447,287,8,"['vector', 'product']",the second vector of the outer product,torch.addr.yaml,2
5448,287,8,"['vector', 'product']",The vector for which the Jacobian vector product is computed.,torch.autograd.functional.jvp.yaml,2
5449,287,8,"['vector', 'product']",The vector for which the vector Hessian product is computed.,torch.autograd.functional.vhp.yaml,2
5450,287,8,"['vector', 'product']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,2
5451,287,8,"['vector', 'product']","The ""vector"" in the Jacobian-vector product.",torch.autograd.grad.yaml,2
5452,288,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
5453,288,8,"['optional', 'tensor']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,2
5454,288,8,"['optional', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
5455,288,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
5456,288,8,"['optional', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
5457,288,8,"['optional', 'tensor']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,2
5458,288,8,"['optional', 'tensor']",the optional destination tensor,torch.lstsq.yaml,2
5459,288,8,"['optional', 'tensor']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2
5460,289,8,"['zero', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,2
5461,289,8,"['zero', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,2
5462,289,8,"['zero', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
5463,289,8,"['zero', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,2
5464,289,8,"['zero', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
5465,289,8,"['zero', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
5466,289,8,"['zero', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,2
5467,289,8,"['zero', 'input']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
5468,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose2d.yaml,3
5469,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv2d.yaml,3
5470,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose3d.yaml,3
5471,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv_transpose1d.yaml,3
5472,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv1d.yaml,3
5473,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.functional.conv3d.yaml,3
5474,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.quantized.functional.conv2d.yaml,3
5475,290,8,"['stride', 'convolving', 'kernel']",the stride of the convolving kernel.,torch.nn.quantized.functional.conv3d.yaml,3
5476,291,8,"['e', 'g']",The logarithmized probabilities of the outputs (e.g. obtained with `torch.nn.functional.log_softmax()`).,torch.nn.functional.ctc_loss.yaml,2
5477,291,8,"['e', 'g']","controls whether `input` was halfed to avoid redundancy, e.g., by `rfft()`.",torch.irfft.yaml,2
5478,291,8,"['e', 'g']","the inputs to the model, e.g., such that `model(*args)` is a valid invocation of the model.",torch.onnx.export.yaml,2
5479,291,8,"['e', 'g']","Full path where object will be saved, e.g. /tmp/temporary_file",torch.hub.download_url_to_file.yaml,2
5480,291,8,"['e', 'g']","This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",torch.distributed.init_process_group.yaml,2
5481,291,8,"['e', 'g']","(Python 3 only) optional keyword arguments passed over to `pickle_module.load()` and `pickle_module.Unpickler()`, e.g., `errors=...`.",torch.load.yaml,2
5482,291,8,"['e', 'g']",The compiler executable name to check (e.g. `g++`).,torch.utils.cpp_extension.check_compiler_abi_compatibility.yaml,2
5483,291,8,"['e', 'g']","This field should be given as a lowercase string (e.g., `""gloo""`), which can also be accessed via `Backend` attributes (e.g., `Backend.GLOO`).",torch.distributed.new_group.yaml,2
5484,292,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,3
5485,292,8,"['bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,3
5486,292,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,3
5487,292,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,3
5488,292,8,"['bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,3
5489,292,8,"['bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,3
5490,292,8,"['bias', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3
5491,292,8,"['bias', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3
5492,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose2d.yaml,3
5493,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv2d.yaml,3
5494,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose3d.yaml,3
5495,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv_transpose1d.yaml,3
5496,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv1d.yaml,3
5497,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.functional.conv3d.yaml,3
5498,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.quantized.functional.conv2d.yaml,3
5499,293,8,"['spacing', 'kernel', 'elements']",the spacing between kernel elements.,torch.nn.quantized.functional.conv3d.yaml,3
5500,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (in _channels ,  out _channels/groups , kH , kW) ",torch.nn.functional.conv_transpose2d.yaml,6
5501,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.functional.conv2d.yaml,6
5502,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (in _channels ,  out _channels/groups , kT , kH , kW) ",torch.nn.functional.conv_transpose3d.yaml,6
5503,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (in _channels ,  out _channels/groups , kW) ",torch.nn.functional.conv_transpose1d.yaml,6
5504,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (out _channels ,  in _channels/groups , kW) ",torch.nn.functional.conv1d.yaml,6
5505,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",torch.nn.functional.conv3d.yaml,6
5506,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,6
5507,294,8,"['filters', 'shape', '_channels', '_channels', 'groups', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,6
5508,295,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,3
5509,295,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,3
5510,295,7,"['output', 'tensor', 'values']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,3
5511,295,7,"['output', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3
5512,295,7,"['output', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
5513,295,7,"['output', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3
5514,295,7,"['output', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
5515,296,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2
5516,296,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2
5517,296,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
5518,296,7,"['output', 'shape']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2
5519,296,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
5520,296,7,"['output', 'shape']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
5521,296,7,"['output', 'shape']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2
5522,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,2
5523,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,2
5524,297,7,"['true', 'outputs']","if `grad_outputs` is `None` and `gen_non_contig_grad_outputs` is `True`, the randomly generated gradient outputs are made to be noncontiguous",torch.autograd.gradgradcheck.yaml,2
5525,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,2
5526,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,2
5527,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,2
5528,297,7,"['true', 'outputs']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,2
5529,298,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2
5530,298,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2
5531,298,7,"['true', 'compute']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2
5532,298,7,"['true', 'compute']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2
5533,298,7,"['true', 'compute']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2
5534,298,7,"['true', 'compute']","If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products.",torch.autograd.backward.yaml,2
5535,298,7,"['true', 'compute']","If `True`, graph of the derivative will be constructed, allowing to compute higher order derivative products.",torch.autograd.grad.yaml,2
5536,299,7,"['value', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
5537,299,7,"['value', 'input']",Specifies a target value that is ignored and does not contribute to the input gradient.,torch.nn.functional.nll_loss.yaml,2
5538,299,7,"['value', 'input']","optional, weight for each value in the input tensor.",torch.bincount.yaml,2
5539,299,7,"['value', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5540,299,7,"['value', 'input']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
5541,299,7,"['value', 'input']",Specifies a target value that is ignored and does not contribute to the input gradient.,torch.nn.functional.cross_entropy.yaml,2
5542,299,7,"['value', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5543,300,7,"['elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5544,300,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,2
5545,300,7,"['elements', 'output']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2
5546,300,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,2
5547,300,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,2
5548,300,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5549,300,7,"['elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,2
5550,301,7,"['default', 'dtype']","If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see `get_default_dtype()`.",torch.arange.yaml,2
5551,301,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2
5552,301,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2
5553,301,7,"['default', 'dtype']",Default: dtype of `input`.,torch.sparse.sum.yaml,2
5554,301,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2
5555,301,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2
5556,301,7,"['default', 'dtype']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2
5557,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.ctc_loss.yaml,2
5558,302,7,"['default', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,2
5559,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.binary_cross_entropy.yaml,2
5560,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.nll_loss.yaml,2
5561,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.cross_entropy.yaml,2
5562,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5563,302,7,"['default', 'mean']",Default: `'mean'`,torch.nn.functional.poisson_nll_loss.yaml,2
5564,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3
5565,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3
5566,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,3
5567,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,3
5568,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3
5569,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,3
5570,303,7,"['SOME_STRUCTURE', 'tensor', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,3
5571,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']",dictionary that maps float modules to quantized modules to be replaced.,torch.quantization.prepare_qat.yaml,3
5572,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']",a dictionary that maps from nn module to nnq module,torch.quantization.swap_module.yaml,3
5573,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,3
5574,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,3
5575,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,3
5576,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,3
5577,304,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,3
5578,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,3
5579,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']","iterable of ints, specifying among which devices the tensor should be scattered.",torch.cuda.comm.scatter.yaml,3
5580,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,3
5581,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,3
5582,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,3
5583,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,3
5584,305,7,"['SOME_STRUCTURE', 'SOME_DTYPE', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
5585,306,7,"['dimension', 'tensor']",dimension along which to split the tensor.,torch.split.yaml,2
5586,306,7,"['dimension', 'tensor']",A dimension along which to chunk the tensor.,torch.cuda.comm.scatter.yaml,2
5587,306,7,"['dimension', 'tensor']",dimension along which to split the tensor,torch.chunk.yaml,2
5588,306,7,"['dimension', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
5589,306,7,"['dimension', 'tensor']",the dimension over which the tensors are concatenated,torch.cat.yaml,2
5590,306,7,"['dimension', 'tensor']",a dimension along which the tensors will be concatenated.,torch.cuda.comm.gather.yaml,2
5591,306,7,"['dimension', 'tensor']",the dimension to slice over to get the sub-tensors,torch.renorm.yaml,2
5592,307,7,"['input', 'points']","Geometrically, we consider the pixels of the input  as squares rather than points.",torch.nn.functional.grid_sample.yaml,2
5593,307,7,"['input', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2
5594,307,7,"['input', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
5595,307,7,"['input', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5596,307,7,"['input', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2
5597,307,7,"['input', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
5598,307,7,"['input', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5599,308,7,"['input', 'SOME_DTYPE']",input model to be modified in-place,torch.quantization.prepare_qat.yaml,2
5600,308,7,"['input', 'SOME_DTYPE']",input model,torch.quantization.quantize.yaml,2
5601,308,7,"['input', 'SOME_DTYPE']",input module,torch.quantization.swap_module.yaml,2
5602,308,7,"['input', 'SOME_DTYPE']",input module,torch.quantization.propagate_qconfig_.yaml,2
5603,308,7,"['input', 'SOME_DTYPE']",input module with qconfig attributes for all the leaf modules that we want to quantize,torch.quantization.add_observer_.yaml,2
5604,308,7,"['input', 'SOME_DTYPE']",input model,torch.quantization.quantize_qat.yaml,2
5605,308,7,"['input', 'SOME_DTYPE']",input model to be modified in-place,torch.quantization.prepare.yaml,2
5606,309,7,"['input', 'input']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
5607,309,7,"['input', 'input']","if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.",torch.autograd.gradcheck.yaml,2
5608,309,7,"['input', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5609,309,7,"['input', 'input']","if `True` the loss is computed as  exp(input) - target * input , if `False` then loss is input - target *  log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2
5610,309,7,"['input', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2
5611,309,7,"['input', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5612,309,7,"['input', 'input']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
5613,310,7,"['tensor', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hvp.yaml,2
5614,310,7,"['tensor', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.vhp.yaml,2
5615,310,7,"['tensor', 'element']",the tensor of per-element means,torch.normal.yaml,2
5616,310,7,"['tensor', 'element']",the tensor of per-element standard deviations,torch.normal.yaml,2
5617,310,7,"['tensor', 'element']",the tensor of per-element means,torch.normal22.yaml,2
5618,310,7,"['tensor', 'element']",the tensor of per-element standard deviations,torch.normal2.yaml,2
5619,310,7,"['tensor', 'element']",a Python function that takes Tensor inputs and returns a Tensor with a single element.,torch.autograd.functional.hessian.yaml,2
5620,311,7,"['tensor', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
5621,311,7,"['tensor', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
5622,311,7,"['tensor', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
5623,311,7,"['tensor', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
5624,311,7,"['tensor', 'SOME_STRUCTURE']"," If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,2
5625,311,7,"['tensor', 'SOME_STRUCTURE']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2
5626,311,7,"['tensor', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
5627,312,7,"['tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,3
5628,312,7,"['tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,3
5629,312,7,"['tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,3
5630,312,7,"['tensor', 'must', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,3
5631,312,7,"['tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,3
5632,312,7,"['tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,3
5633,312,7,"['tensor', 'must', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,3
5634,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,5
5635,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_max_pool1d.yaml,5
5636,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,5
5637,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer),torch.nn.functional.adaptive_avg_pool1d.yaml,5
5638,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,5
5639,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,5
5640,313,7,"['target', 'output', 'size', 'single', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,5
5641,314,7,"['size', 'input', 'size']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3
5642,314,7,"['size', 'input', 'size']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3
5643,314,7,"['size', 'input', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,3
5644,314,7,"['size', 'input', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,3
5645,314,7,"['size', 'input', 'size']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3
5646,314,7,"['size', 'input', 'size']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3
5647,314,7,"['size', 'input', 'size']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3
5648,315,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
5649,315,7,"['single', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
5650,315,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
5651,315,7,"['single', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_norm_.yaml,2
5652,315,7,"['single', 'tensor']",an iterable of Tensors or a single Tensor that will have gradients normalized,torch.nn.utils.clip_grad_value_.yaml,2
5653,315,7,"['single', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
5654,315,7,"['single', 'tensor']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2
5655,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.l1_unstructured.yaml,4
5656,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_unstructured.yaml,4
5657,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.ln_structured.yaml,4
5658,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.random_structured.yaml,4
5659,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.remove.yaml,4
5660,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune,torch.nn.utils.prune.custom_from_mask.yaml,4
5661,316,7,"['SOME_DTYPE', 'containing', 'tensor', 'prune']",module containing the tensor to prune.,torch.nn.utils.prune.identity.yaml,4
5662,317,7,"['SOME_DTYPE', 'place']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,2
5663,317,7,"['SOME_DTYPE', 'place']",input model to be modified in-place,torch.quantization.prepare_qat.yaml,2
5664,317,7,"['SOME_DTYPE', 'place']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,2
5665,317,7,"['SOME_DTYPE', 'place']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,2
5666,317,7,"['SOME_DTYPE', 'place']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,2
5667,317,7,"['SOME_DTYPE', 'place']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,2
5668,317,7,"['SOME_DTYPE', 'place']",input model to be modified in-place,torch.quantization.prepare.yaml,2
5669,318,7,"['false', 'losses']",Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.,torch.nn.functional.ctc_loss.yaml,2
5670,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,2
5671,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,2
5672,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,2
5673,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,2
5674,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5675,318,7,"['false', 'losses']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,2
5676,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
5677,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
5678,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
5679,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
5680,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5681,319,7,"['false', 'loss']","if `True` the loss is computed as  exp(input) - target * input , if `False` then loss is input - target *  log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2
5682,319,7,"['false', 'loss']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
5683,320,7,"['first', 'input']",the first input tensor,torch.bitwise_xor.yaml,2
5684,320,7,"['first', 'input']",the first input tensor,torch.add.yaml,2
5685,320,7,"['first', 'input']",First input.,torch.nn.functional.cosine_similarity.yaml,2
5686,320,7,"['first', 'input']",the first input tensor,torch.bitwise_and.yaml,2
5687,320,7,"['first', 'input']",the first input tensor,torch.atan2.yaml,2
5688,320,7,"['first', 'input']",the first input tensor,torch.bitwise_or.yaml,2
5689,320,7,"['first', 'input']","For example, in LSTM, if user passes `(activation, hidden)`, `function` should correctly use the first input as `activation` and the second input as `hidden`",torch.utils.checkpoint.checkpoint.yaml,2
5690,321,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,2
5691,321,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.addbmm.yaml,2
5692,321,7,"['first', 'multiplied']",the first matrix to be multiplied,torch.addmm.yaml,2
5693,321,7,"['first', 'multiplied']",the first tensor to be multiplied,torch.matmul.yaml,2
5694,321,7,"['first', 'multiplied']",the first sparse matrix to be multiplied,torch.sparse.mm.yaml,2
5695,321,7,"['first', 'multiplied']",the first batch of matrices to be multiplied,torch.bmm.yaml,2
5696,321,7,"['first', 'multiplied']",the first matrix to be multiplied,torch.mm.yaml,2
5697,322,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,2
5698,322,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.addbmm.yaml,2
5699,322,7,"['second', 'multiplied']",the second matrix to be multiplied,torch.addmm.yaml,2
5700,322,7,"['second', 'multiplied']",the second tensor to be multiplied,torch.matmul.yaml,2
5701,322,7,"['second', 'multiplied']",the second dense matrix to be multiplied,torch.sparse.mm.yaml,2
5702,322,7,"['second', 'multiplied']",the second batch of matrices to be multiplied,torch.bmm.yaml,2
5703,322,7,"['second', 'multiplied']",the second matrix to be multiplied,torch.mm.yaml,2
5704,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,4
5705,323,7,"['none', 'reduction', 'applied', 'mean']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,4
5706,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,4
5707,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,4
5708,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,4
5709,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,4
5710,323,7,"['none', 'reduction', 'applied', 'mean']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,4
5711,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,5
5712,324,7,"['none', 'reduction', 'applied', 'mean', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,5
5713,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,5
5714,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,5
5715,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,5
5716,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,5
5717,324,7,"['none', 'reduction', 'applied', 'mean', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,5
5718,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7
5719,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7
5720,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7
5721,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7
5722,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7
5723,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
5724,325,7,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7
5725,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7
5726,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7
5727,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7
5728,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7
5729,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7
5730,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
5731,326,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7
5732,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,8
5733,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8
5734,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8
5735,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8
5736,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8
5737,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
5738,327,7,"['none', 'reduction', 'applied', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8
5739,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,6
5740,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,6
5741,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,6
5742,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,6
5743,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,6
5744,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6
5745,328,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,6
5746,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7
5747,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7
5748,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7
5749,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7
5750,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7
5751,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
5752,329,7,"['none', 'reduction', 'applied', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7
5753,330,7,"['dtype', 'input']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,2
5754,330,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,2
5755,330,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,2
5756,330,7,"['dtype', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,2
5757,330,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,2
5758,330,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,2
5759,330,7,"['dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,2
5760,331,7,"['number', 'd']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2
5761,331,7,"['number', 'd']",number of columns in the 2-D matrix.,torch.triu_indices.yaml,2
5762,331,7,"['number', 'd']",number of rows in the 2-D matrix.,torch.triu_indices.yaml,2
5763,331,7,"['number', 'd']",number of columns in the 2-D matrix.,torch.tril_indices.yaml,2
5764,331,7,"['number', 'd']",number of rows in the 2-D matrix.,torch.tril_indices.yaml,2
5765,331,7,"['number', 'd']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5766,331,7,"['number', 'd']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5767,332,7,"['set', 'input']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2
5768,332,7,"['set', 'input']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
5769,332,7,"['set', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
5770,332,7,"['set', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5771,332,7,"['set', 'input']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
5772,332,7,"['set', 'input']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
5773,332,7,"['set', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5774,333,7,"['set', 'false', 'instead']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,3
5775,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,3
5776,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,3
5777,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,3
5778,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,3
5779,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
5780,333,7,"['set', 'false', 'instead']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,3
5781,334,7,"['d', 'tensor']",a sequence of 2 or more 2-D tensors whose product is to be determined.,torch.chain_matmul.yaml,2
5782,334,7,"['d', 'tensor']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
5783,334,7,"['d', 'tensor']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
5784,334,7,"['d', 'tensor']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2
5785,334,7,"['d', 'tensor']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2
5786,334,7,"['d', 'tensor']",1-d int tensor,torch.bincount.yaml,2
5787,334,7,"['d', 'tensor']",the input 2-D tensor,torch.matrix_rank.yaml,2
5788,335,7,"['zero', 'padding']","when True, will include the zero-padding in the averaging calculation",torch.nn.functional.avg_pool3d.yaml,2
5789,335,7,"['zero', 'padding']","when True, will include the zero-padding in the averaging calculation.",torch.nn.quantized.functional.avg_pool2d.yaml,2
5790,335,7,"['zero', 'padding']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
5791,335,7,"['zero', 'padding']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool1d.yaml,2
5792,335,7,"['zero', 'padding']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
5793,335,7,"['zero', 'padding']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
5794,335,7,"['zero', 'padding']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool2d.yaml,2
5795,336,7,"['zero', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool3d.yaml,3
5796,336,7,"['zero', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.quantized.functional.avg_pool2d.yaml,3
5797,336,7,"['zero', 'sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,3
5798,336,7,"['zero', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool1d.yaml,3
5799,336,7,"['zero', 'sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,3
5800,336,7,"['zero', 'sides', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,3
5801,336,7,"['zero', 'sides', 'input']",implicit zero paddings on both sides of the input.,torch.nn.functional.avg_pool2d.yaml,3
5802,337,7,"['n', 'n', 'batch']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
5803,337,7,"['n', 'n', 'batch']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3
5804,337,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,3
5805,337,7,"['n', 'n', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
5806,337,7,"['n', 'n', 'batch']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,3
5807,337,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,3
5808,337,7,"['n', 'n', 'batch']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,3
5809,338,7,"['n', 'c']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
5810,338,7,"['n', 'c']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,2
5811,338,7,"['n', 'c']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2
5812,338,7,"['n', 'c']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5813,338,7,"['n', 'c']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5814,338,7,"['n', 'c']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5815,338,7,"['n', 'c']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5816,339,7,"['n', 'c', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
5817,339,7,"['n', 'c', 'n']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3
5818,339,7,"['n', 'c', 'n']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3
5819,339,7,"['n', 'c', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
5820,339,7,"['n', 'c', 'n']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
5821,339,7,"['n', 'c', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
5822,339,7,"['n', 'c', 'n']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
5823,340,7,"['n', 'k']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
5824,340,7,"['n', 'k']",Default value for n is k.,torch.lobpcg.yaml,2
5825,340,7,"['n', 'k']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
5826,340,7,"['n', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5827,340,7,"['n', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5828,340,7,"['n', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5829,340,7,"['n', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5830,341,7,"['batch', 'element']",list of sequences lengths of each batch element.,torch.nn.utils.rnn.pack_padded_sequence.yaml,2
5831,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,2
5832,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,2
5833,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,2
5834,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,2
5835,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5836,341,7,"['batch', 'element']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,2
5837,342,7,"['matrix', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
5838,342,7,"['matrix', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
5839,342,7,"['matrix', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
5840,342,7,"['matrix', 'dimensions']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
5841,342,7,"['matrix', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
5842,342,7,"['matrix', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
5843,342,7,"['matrix', 'dimensions']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
5844,343,7,"['upper', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2
5845,343,7,"['upper', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,2
5846,343,7,"['upper', 'triangular']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
5847,343,7,"['upper', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,2
5848,343,7,"['upper', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,2
5849,343,7,"['upper', 'triangular']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2
5850,343,7,"['upper', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2
5851,344,7,"['lower', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2
5852,344,7,"['lower', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,2
5853,344,7,"['lower', 'triangular']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
5854,344,7,"['lower', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,2
5855,344,7,"['lower', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,2
5856,344,7,"['lower', 'triangular']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,2
5857,344,7,"['lower', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2
5858,345,7,"['quantized', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,2
5859,345,7,"['quantized', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2
5860,345,7,"['quantized', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
5861,345,7,"['quantized', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,2
5862,345,7,"['quantized', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2
5863,345,7,"['quantized', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
5864,345,7,"['quantized', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,2
5865,346,7,"['corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,2
5866,346,7,"['corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
5867,346,7,"['corner', 'pixels']","if `True`, consider `-1` and `1` to refer to the centers of the corner pixels rather than the image corners.",torch.nn.functional.affine_grid.yaml,2
5868,346,7,"['corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
5869,346,7,"['corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
5870,346,7,"['corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
5871,346,7,"['corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
5872,347,7,"['like', 'SOME_DTYPE']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2
5873,347,7,"['like', 'SOME_DTYPE']","Note that it should be like (src, dst1, dst2,  u2026), the first element of which is the source device to broadcast from.",torch.cuda.comm.broadcast_coalesced.yaml,2
5874,347,7,"['like', 'SOME_DTYPE']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2
5875,347,7,"['like', 'SOME_DTYPE']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2
5876,347,7,"['like', 'SOME_DTYPE']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2
5877,347,7,"['like', 'SOME_DTYPE']","Note that it should be like (src, dst1, dst2,  u2026), the first element of which is the source device to broadcast from.",torch.cuda.comm.broadcast.yaml,2
5878,347,7,"['like', 'SOME_DTYPE']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2
5879,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,7
5880,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,7
5881,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,7
5882,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,7
5883,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,7
5884,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
5885,348,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,7
5886,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,7
5887,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,7
5888,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,7
5889,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,7
5890,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,7
5891,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
5892,349,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,7
5893,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,8
5894,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`.,torch.nn.functional.kl_div.yaml,8
5895,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,8
5896,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,8
5897,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,8
5898,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
5899,350,7,"['specifies', 'reduction', 'apply', 'output', 'none', '|', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,8
5900,351,7,"['destination', 'rank']",Destination rank.,torch.distributed.isend.yaml,2
5901,351,7,"['destination', 'rank']",Destination rank (default is 0),torch.distributed.gather.yaml,2
5902,351,7,"['destination', 'rank']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
5903,351,7,"['destination', 'rank']",Destination rank,torch.distributed.reduce.yaml,2
5904,351,7,"['destination', 'rank']",Destination rank.,torch.distributed.send.yaml,2
5905,351,7,"['destination', 'rank']",Destination rank,torch.distributed.reduce_multigpu.yaml,2
5906,351,7,"['destination', 'rank']",Destination tensor rank within `tensor_list`,torch.distributed.reduce_multigpu.yaml,2
5907,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,4
5908,352,7,"['reduction', 'output', 'mean', 'sum']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,4
5909,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,4
5910,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,4
5911,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,4
5912,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,4
5913,352,7,"['reduction', 'output', 'mean', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,4
5914,353,7,"['losses', 'batch']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,2
5915,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,2
5916,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,2
5917,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,2
5918,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,2
5919,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5920,353,7,"['losses', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,2
5921,354,7,"['source', 'rank']",Source rank.,torch.distributed.recv.yaml,2
5922,354,7,"['source', 'rank']",Source rank.,torch.distributed.broadcast_multigpu.yaml,2
5923,354,7,"['source', 'rank']",Source tensor rank within `tensor_list`,torch.distributed.broadcast_multigpu.yaml,2
5924,354,7,"['source', 'rank']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
5925,354,7,"['source', 'rank']",Source rank (default is 0),torch.distributed.scatter.yaml,2
5926,354,7,"['source', 'rank']",Source rank.,torch.distributed.irecv.yaml,2
5927,354,7,"['source', 'rank']",Source rank.,torch.distributed.broadcast.yaml,2
5928,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.l1_unstructured.yaml,6
5929,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_unstructured.yaml,6
5930,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.ln_structured.yaml,6
5931,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.random_structured.yaml,6
5932,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.remove.yaml,6
5933,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.custom_from_mask.yaml,6
5934,355,7,"['parameter', 'name', 'within', 'SOME_DTYPE', 'pruning', 'act']",parameter name within `module` on which pruning will act.,torch.nn.utils.prune.identity.yaml,6
5935,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.irfft.yaml,4
5936,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.fft.yaml,4
5937,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.ifft.yaml,4
5938,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,4
5939,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,4
5940,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return normalized results.,torch.rfft.yaml,4
5941,356,7,"['controls', 'whether', 'return', 'results']",controls whether to return half of results to avoid redundancy.,torch.rfft.yaml,4
5942,357,7,"['k', 'k']","the k in ""top-k""",torch.topk.yaml,2
5943,357,7,"['k', 'k']",k for the k-th smallest element,torch.kthvalue.yaml,2
5944,357,7,"['k', 'k']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
5945,357,7,"['k', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5946,357,7,"['k', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
5947,357,7,"['k', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5948,357,7,"['k', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
5949,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.kl_div.yaml,2
5950,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy.yaml,2
5951,358,7,"['multiple', 'per']","If using multiple processes per machine with `nccl` backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks.",torch.distributed.init_process_group.yaml,2
5952,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.nll_loss.yaml,2
5953,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.cross_entropy.yaml,2
5954,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
5955,358,7,"['multiple', 'per']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.poisson_nll_loss.yaml,2
5956,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,3
5957,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,3
5958,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3
5959,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3
5960,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3
5961,359,6,"['output', 'SOME_DTYPE', 'SOME_DTYPE']","output device (-1 means CPU, default: current device)",torch.cuda.comm.gather.yaml,3
5962,360,6,"['output', 'SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,3
5963,360,6,"['output', 'SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, Tensor)",torch.symeig.yaml,3
5964,360,6,"['output', 'SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,3
5965,360,6,"['output', 'SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,3
5966,360,6,"['output', 'SOME_STRUCTURE', 'tensor']","the output tuple of (Tensor, Tensor)",torch.geqrf.yaml,3
5967,360,6,"['output', 'SOME_STRUCTURE', 'tensor']",the output tuple of tensors,torch.svd.yaml,3
5968,361,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,3
5969,361,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,3
5970,361,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,3
5971,361,6,"['true', 'computed', 'differentiable']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,3
5972,361,6,"['true', 'computed', 'differentiable']","If `True`, the Jacobian will be computed in a differentiable manner.",torch.autograd.functional.jacobian.yaml,3
5973,361,6,"['true', 'computed', 'differentiable']","If `True`, the Hessian will be computed in a differentiable manner.",torch.autograd.functional.hessian.yaml,3
5974,362,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,2
5975,362,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,2
5976,362,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,2
5977,362,6,"['true', 'way']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,2
5978,362,6,"['true', 'way']",Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way.,torch.autograd.backward.yaml,2
5979,362,6,"['true', 'way']",Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way.,torch.autograd.grad.yaml,2
5980,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hvp.yaml,8
5981,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vjp.yaml,8
5982,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jvp.yaml,8
5983,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.vhp.yaml,8
5984,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.jacobian.yaml,8
5985,363,6,"['true', 'error', 'raised', 'detect', 'exists', 'input', 'outputs', 'independent']","If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it.",torch.autograd.functional.hessian.yaml,8
5986,364,6,"['true', 'tensor']","If `True`, gradient w.r.t. `weight` will be a sparse tensor.",torch.nn.functional.embedding.yaml,2
5987,364,6,"['true', 'tensor']","If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor.",torch.lu.yaml,2
5988,364,6,"['true', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
5989,364,6,"['true', 'tensor']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2
5990,364,6,"['true', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
5991,364,6,"['true', 'tensor']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
5992,365,6,"['value', 'set', 'points']",the ending value for the set of points,torch.arange.yaml,3
5993,365,6,"['value', 'set', 'points']",the starting value for the set of points.,torch.arange.yaml,3
5994,365,6,"['value', 'set', 'points']",the ending value for the set of points,torch.linspace.yaml,3
5995,365,6,"['value', 'set', 'points']",the starting value for the set of points,torch.linspace.yaml,3
5996,365,6,"['value', 'set', 'points']",the ending value for the set of points,torch.logspace.yaml,3
5997,365,6,"['value', 'set', 'points']",the starting value for the set of points,torch.logspace.yaml,3
5998,366,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,3
5999,366,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,3
6000,366,6,"['default', 'dtype', 'input']",Default: dtype of `input`.,torch.sparse.sum.yaml,3
6001,366,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,3
6002,366,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,3
6003,366,6,"['default', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,3
6004,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.kl_div.yaml,8
6005,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,8
6006,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.nll_loss.yaml,8
6007,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.cross_entropy.yaml,8
6008,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6009,367,6,"['default', 'losses', 'averaged', 'summed', 'observations', 'minibatch', 'depending', 'size_average']","By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,8
6010,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.kl_div.yaml,6
6011,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy.yaml,6
6012,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.nll_loss.yaml,6
6013,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.cross_entropy.yaml,6
6014,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6
6015,368,6,"['default', 'losses', 'averaged', 'loss', 'element', 'batch']","By default, the losses are averaged over each loss element in the batch.",torch.nn.functional.poisson_nll_loss.yaml,6
6016,369,6,"['default', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2
6017,369,6,"['default', 'value']",Default value for n is k.,torch.lobpcg.yaml,2
6018,369,6,"['default', 'value']",Default value equals 30 minutes.,torch.distributed.init_process_group.yaml,2
6019,369,6,"['default', 'value']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2
6020,369,6,"['default', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2
6021,369,6,"['default', 'value']",Default value equals 30 minutes.,torch.distributed.new_group.yaml,2
6022,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,4
6023,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,4
6024,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the tuple of two output tensors (min, min_indices)",torch.min2.yaml,4
6025,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,4
6026,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,4
6027,370,6,"['SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,4
6028,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.rand.yaml,6
6029,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.ones.yaml,6
6030,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.normal222.yaml,6
6031,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.zeros.yaml,6
6032,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']",a sequence of integers defining the shape of the output tensor.,torch.randn.yaml,6
6033,371,6,"['SOME_STRUCTURE', 'SOME_DTYPE', 'defining', 'shape', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,6
6034,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.kl_div.yaml,10
6035,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy.yaml,10
6036,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.nll_loss.yaml,10
6037,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.cross_entropy.yaml,10
6038,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,10
6039,372,6,"['reduce', 'false', 'returns', 'loss', 'per', 'batch', 'element', 'instead', 'ignores', 'size_average']","When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`.",torch.nn.functional.poisson_nll_loss.yaml,10
6040,373,6,"['input', 'tensor', 'times']","input tensor (minibatch , in _channels , iT  times iH , iW) ",torch.nn.functional.avg_pool3d.yaml,3
6041,373,6,"['input', 'tensor', 'times']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,3
6042,373,6,"['input', 'tensor', 'times']",input tensor of shape B  times P  times M .,torch.cdist.yaml,3
6043,373,6,"['input', 'tensor', 'times']",input tensor of shape B  times R  times M .,torch.cdist.yaml,3
6044,373,6,"['input', 'tensor', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
6045,373,6,"['input', 'tensor', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
6046,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,8
6047,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.det.yaml,8
6048,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,8
6049,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions",torch.inverse.yaml,8
6050,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.slogdet.yaml,8
6051,374,6,"['input', 'tensor', 'size', 'n', 'n', 'zero', 'batch', 'dimensions']","the input tensor of size `(*, n, n)` where `*` is zero or more batch dimensions.",torch.logdet.yaml,8
6052,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, m) ",torch.lobpcg.yaml,5
6053,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5
6054,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,5
6055,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, m) .",torch.lobpcg.yaml,5
6056,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,5
6057,375,6,"['input', 'tensor', 'size', 'm', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,5
6058,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv_transpose2d.yaml,7
6059,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.functional.conv2d.yaml,7
6060,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv_transpose3d.yaml,7
6061,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","input tensor of shape (minibatch , in _channels , iT , iH , iW) ",torch.nn.functional.conv3d.yaml,7
6062,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,7
6063,376,6,"['input', 'tensor', 'shape', 'minibatch', '_channels', 'ih', 'iw']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,7
6064,377,6,"['input', 'output', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,3
6065,377,6,"['input', 'output', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3
6066,377,6,"['input', 'output', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
6067,377,6,"['input', 'output', 'points']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,3
6068,377,6,"['input', 'output', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3
6069,377,6,"['input', 'output', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
6070,378,6,"['input', 'matrix']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
6071,378,6,"['input', 'matrix']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
6072,378,6,"['input', 'matrix']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
6073,378,6,"['input', 'matrix']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6074,378,6,"['input', 'matrix']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6075,378,6,"['input', 'matrix']",the input matrix,torch.geqrf.yaml,2
6076,379,6,"['input', 'returned']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2
6077,379,6,"['input', 'returned']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2
6078,379,6,"['input', 'returned']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2
6079,379,6,"['input', 'returned']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6080,379,6,"['input', 'returned']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2
6081,379,6,"['input', 'returned']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6082,380,6,"['tensor', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
6083,380,6,"['tensor', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
6084,380,6,"['tensor', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
6085,380,6,"['tensor', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
6086,380,6,"['tensor', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
6087,380,6,"['tensor', 'specified']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
6088,381,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
6089,381,6,"['tensor', 'indices']",the output tensor containing indices,torch.nonzero.yaml,2
6090,381,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
6091,381,6,"['tensor', 'indices']",Tensor containing indices into the embedding matrix,torch.nn.functional.embedding.yaml,2
6092,381,6,"['tensor', 'indices']",the 1-D tensor containing the indices to index,torch.index_select.yaml,2
6093,381,6,"['tensor', 'indices']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
6094,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.le.yaml,3
6095,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.gt.yaml,3
6096,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.ne.yaml,3
6097,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.eq.yaml,3
6098,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.ge.yaml,3
6099,382,6,"['tensor', 'value', 'compare']",the tensor or value to compare,torch.lt.yaml,3
6100,383,6,"['tensor', 'size', 'm', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,4
6101,383,6,"['tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,4
6102,383,6,"['tensor', 'size', 'm', 'n']","the tensor to factor of size (*, m, n) ",torch.lu.yaml,4
6103,383,6,"['tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,4
6104,383,6,"['tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,4
6105,383,6,"['tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,4
6106,384,6,"['tensor', 'points']",the tensor with the ending points,torch.lerp.yaml,2
6107,384,6,"['tensor', 'points']",the tensor with the starting points,torch.lerp.yaml,2
6108,384,6,"['tensor', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,2
6109,384,6,"['tensor', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6110,384,6,"['tensor', 'points']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,2
6111,384,6,"['tensor', 'points']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6112,385,6,"['tensor', 'dimension']",There should be one index letter per tensor dimension.,torch.einsum.yaml,2
6113,385,6,"['tensor', 'dimension']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
6114,385,6,"['tensor', 'dimension']","If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.",torch.norm.yaml,2
6115,385,6,"['tensor', 'dimension']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
6116,385,6,"['tensor', 'dimension']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
6117,385,6,"['tensor', 'dimension']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2
6118,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3
6119,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3
6120,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3
6121,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3
6122,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']"," If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor.",torch.onnx.export.yaml,3
6123,386,6,"['tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3
6124,387,6,"['whether', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,2
6125,387,6,"['whether', 'triangular']",whether A  is unit triangular.,torch.triangular_solve.yaml,2
6126,387,6,"['whether', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,2
6127,387,6,"['whether', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,2
6128,387,6,"['whether', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,2
6129,387,6,"['whether', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2
6130,388,6,"['whether', 'default']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,2
6131,388,6,"['whether', 'default']",whether or not to display a progress bar to stderr Default: True,torch.hub.download_url_to_file.yaml,2
6132,388,6,"['whether', 'default']",whether to return an abbreviated summary (default: False).,torch.cuda.memory_summary.yaml,2
6133,388,6,"['whether', 'default']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,2
6134,388,6,"['whether', 'default']",controls whether to return half of results to avoid redundancy Default: `True`,torch.stft.yaml,2
6135,388,6,"['whether', 'default']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,2
6136,389,6,"['size', 'single', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,3
6137,389,6,"['size', 'single', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,3
6138,389,6,"['size', 'single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3
6139,389,6,"['size', 'single', 'SOME_STRUCTURE']",size of a single chunk or list of sizes for each chunk,torch.split.yaml,3
6140,389,6,"['size', 'single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3
6141,389,6,"['size', 'single', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3
6142,390,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3
6143,390,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3
6144,390,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3
6145,390,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3
6146,390,6,"['size', 'input', 'tensor']",Should be of same size as input tensor.,torch.bincount.yaml,3
6147,390,6,"['size', 'input', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3
6148,391,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,3
6149,391,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.functional.avg_pool3d.yaml,3
6150,391,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,3
6151,391,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.quantized.functional.avg_pool2d.yaml,3
6152,391,6,"['size', 'pooling', 'region']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,3
6153,391,6,"['size', 'pooling', 'region']",size of the pooling region.,torch.nn.functional.avg_pool2d.yaml,3
6154,392,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
6155,392,6,"['size', 'dimension']",the size of the original signal (without batch dimension).,torch.irfft.yaml,2
6156,392,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
6157,392,6,"['size', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
6158,392,6,"['size', 'dimension']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
6159,392,6,"['size', 'dimension']","If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",torch.roll.yaml,2
6160,393,6,"['size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,3
6161,393,6,"['size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,3
6162,393,6,"['size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,3
6163,393,6,"['size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,3
6164,393,6,"['size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,3
6165,393,6,"['size', 'output', 'tensor']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,3
6166,394,6,"['size', 'window']",the size of the window.,torch.nn.functional.avg_pool1d.yaml,2
6167,394,6,"['size', 'window']",the size of returned window,torch.hann_window.yaml,2
6168,394,6,"['size', 'window']",the size of window frame and STFT filter.,torch.stft.yaml,2
6169,394,6,"['size', 'window']",the size of returned window,torch.hamming_window.yaml,2
6170,394,6,"['size', 'window']",the size of returned window,torch.blackman_window.yaml,2
6171,394,6,"['size', 'window']",the size of returned window,torch.bartlett_window.yaml,2
6172,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple `(dH, dW)`.",torch.nn.functional.conv_transpose2d.yaml,5
6173,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.functional.conv2d.yaml,5
6174,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv_transpose3d.yaml,5
6175,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple (dT, dH, dW).",torch.nn.functional.conv3d.yaml,5
6176,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple (dH, dW).",torch.nn.quantized.functional.conv2d.yaml,5
6177,395,6,"['single', 'number', 'SOME_STRUCTURE', 'dh', 'dw']","Can be a single number or a tuple (dD, dH, dW).",torch.nn.quantized.functional.conv3d.yaml,5
6178,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,3
6179,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,3
6180,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,3
6181,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,3
6182,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,3
6183,396,6,"['SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']","a function, `torch.device`, string or a dict specifying how to remap storage locations",torch.load.yaml,3
6184,397,6,"['SOME_DTYPE', 'file']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,2
6185,397,6,"['SOME_DTYPE', 'file']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,2
6186,397,6,"['SOME_DTYPE', 'file']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,2
6187,397,6,"['SOME_DTYPE', 'file']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,2
6188,397,6,"['SOME_DTYPE', 'file']",module used for unpickling metadata and objects (has to match the `pickle_module` used to serialize file),torch.load.yaml,2
6189,397,6,"['SOME_DTYPE', 'file']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,2
6190,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.hvp.yaml,9
6191,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vjp.yaml,9
6192,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value.",torch.autograd.functional.jvp.yaml,9
6193,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value.",torch.autograd.functional.vhp.yaml,9
6194,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value.",torch.autograd.functional.jacobian.yaml,9
6195,398,6,"['false', 'return', 'tensor', 'zeros', 'said', 'inputs', 'expected', 'mathematical', 'value']","If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value.",torch.autograd.functional.hessian.yaml,9
6196,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,9
6197,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9
6198,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9
6199,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9
6200,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9
6201,399,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9
6202,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9
6203,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9
6204,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9
6205,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9
6206,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9
6207,400,6,"['none', 'reduction', 'applied', 'mean', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9
6208,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,8
6209,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8
6210,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8
6211,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8
6212,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6213,401,6,"['none', 'reduction', 'applied', 'mean', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8
6214,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the output losses will be divided by the target lengths and then the mean over the batch is taken, `'sum'`: the output will be summed.",torch.nn.functional.ctc_loss.yaml,7
6215,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7
6216,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7
6217,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7
6218,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
6219,402,6,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7
6220,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8
6221,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8
6222,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8
6223,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8
6224,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6225,403,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8
6226,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9
6227,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9
6228,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9
6229,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9
6230,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9
6231,404,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9
6232,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8
6233,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8
6234,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8
6235,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8
6236,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6237,405,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'output', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8
6238,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,9
6239,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,9
6240,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,9
6241,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,9
6242,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,9
6243,406,6,"['none', 'reduction', 'applied', 'sum', 'output', 'divided', 'number', 'elements', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,9
6244,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,7
6245,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,7
6246,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,7
6247,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,7
6248,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,7
6249,407,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,7
6250,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']",`'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`,torch.nn.functional.kl_div.yaml,8
6251,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,8
6252,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,8
6253,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,8
6254,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6255,408,6,"['none', 'reduction', 'applied', 'sum', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,8
6256,409,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
6257,409,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
6258,409,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
6259,409,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
6260,409,6,"['added', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
6261,409,6,"['added', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
6262,410,6,"['multiplier', 'alpha']",multiplier for batch1  mathbin{@} batch2  ( alpha ),torch.baddbmm.yaml,2
6263,410,6,"['multiplier', 'alpha']",multiplier for vec1  otimes vec2  ( alpha ),torch.addr.yaml,2
6264,410,6,"['multiplier', 'alpha']",multiplier for batch1 @ batch2 ( alpha ),torch.addbmm.yaml,2
6265,410,6,"['multiplier', 'alpha']",multiplier for mat1 @ mat2  ( alpha ),torch.addmm.yaml,2
6266,410,6,"['multiplier', 'alpha']",multiplier for mat1 @ mat2  ( alpha ),torch.sparse.addmm.yaml,2
6267,410,6,"['multiplier', 'alpha']",multiplier for mat @ vec  ( alpha ),torch.addmv.yaml,2
6268,411,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.baddbmm.yaml,2
6269,411,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addr.yaml,2
6270,411,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addbmm.yaml,2
6271,411,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addmm.yaml,2
6272,411,6,"['multiplier', 'beta']",multiplier for `mat` ( beta ),torch.sparse.addmm.yaml,2
6273,411,6,"['multiplier', 'beta']",multiplier for `input` ( beta ),torch.addmv.yaml,2
6274,412,6,"['type', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
6275,412,6,"['type', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,2
6276,412,6,"['type', 'SOME_DTYPE']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
6277,412,6,"['type', 'SOME_DTYPE']",None or fp32 bias of type torch.float,torch.nn.quantized.functional.linear.yaml,2
6278,412,6,"['type', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv2d.yaml,2
6279,412,6,"['type', 'SOME_DTYPE']",The tensor type must be torch.float.,torch.nn.quantized.functional.conv3d.yaml,2
6280,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,6
6281,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,6
6282,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to :attr:'dtype' while performing the operation.",torch.norm.yaml,6
6283,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,6
6284,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,6
6285,413,6,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,6
6286,414,6,"['number', 'columns']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2
6287,414,6,"['number', 'columns']",the number of columns with default being `n`,torch.eye.yaml,2
6288,414,6,"['number', 'columns']",Default is the number of X  columns (when specified) or 1.,torch.lobpcg.yaml,2
6289,414,6,"['number', 'columns']","If X  is specifed, the value of n (when specified) must be the number of X  columns.",torch.lobpcg.yaml,2
6290,414,6,"['number', 'columns']",number of columns in the 2-D matrix.,torch.triu_indices.yaml,2
6291,414,6,"['number', 'columns']",number of columns in the 2-D matrix.,torch.tril_indices.yaml,2
6292,415,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,4
6293,415,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4
6294,415,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4
6295,415,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4
6296,415,6,"['set', 'points', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4
6297,415,6,"['set', 'points', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4
6298,416,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the extrema (`-1` and `1`) are considered as referring to the center points of the input's corner pixels.",torch.nn.functional.grid_sample.yaml,4
6299,416,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4
6300,416,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4
6301,416,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4
6302,416,6,"['set', 'input', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4
6303,416,6,"['set', 'input', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4
6304,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hvp.yaml,8
6305,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vjp.yaml,8
6306,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jvp.yaml,8
6307,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.vhp.yaml,8
6308,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.jacobian.yaml,8
6309,417,6,"['note', 'strict', 'false', 'result', 'require', 'gradients', 'disconnected', 'inputs']","Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs.",torch.autograd.functional.hessian.yaml,8
6310,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.kl_div.yaml,6
6311,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy.yaml,6
6312,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.nll_loss.yaml,6
6313,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.cross_entropy.yaml,6
6314,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,6
6315,418,6,"['note', 'losses', 'multiple', 'elements', 'per', 'sample']","Note that for some losses, there multiple elements per sample.",torch.nn.functional.poisson_nll_loss.yaml,6
6316,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.hvp.yaml,3
6317,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.vjp.yaml,3
6318,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.jvp.yaml,3
6319,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.vhp.yaml,3
6320,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.jacobian.yaml,3
6321,419,6,"['inputs', 'function', 'func']",inputs to the function `func`.,torch.autograd.functional.hessian.yaml,3
6322,420,6,"['inputs', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
6323,420,6,"['inputs', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
6324,420,6,"['inputs', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
6325,420,6,"['inputs', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
6326,420,6,"['inputs', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
6327,420,6,"['inputs', 'SOME_STRUCTURE']",It should also know how to handle the inputs passed as the tuple.,torch.utils.checkpoint.checkpoint.yaml,2
6328,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3
6329,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3
6330,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3
6331,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3
6332,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3
6333,421,6,"['python', 'SOME_STRUCTURE', 'tensor']",any python sequence of tensors of the same type.,torch.cat.yaml,3
6334,422,6,"['function', 'takes', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3
6335,422,6,"['function', 'takes', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3
6336,422,6,"['function', 'takes', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3
6337,422,6,"['function', 'takes', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3
6338,422,6,"['function', 'takes', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3
6339,422,6,"['function', 'takes', 'SOME_STRUCTURE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,3
6340,423,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,3
6341,423,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,3
6342,423,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,3
6343,423,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,3
6344,423,6,"['returns', 'tensor', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,3
6345,423,6,"['returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,3
6346,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_reserved.yaml,9
6347,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_reserved.yaml,9
6348,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_allocated.yaml,9
6349,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.memory_allocated.yaml,9
6350,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.max_memory_allocated.yaml,9
6351,424,6,"['returns', 'statistic', 'current', 'SOME_DTYPE', 'given', 'current_device', 'SOME_DTYPE', 'none', 'default']","Returns statistic for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.reset_max_memory_cached.yaml,9
6352,425,6,"['returns', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,2
6353,425,6,"['returns', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,2
6354,425,6,"['returns', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,2
6355,425,6,"['returns', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,2
6356,425,6,"['returns', 'SOME_STRUCTURE']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,2
6357,425,6,"['returns', 'SOME_STRUCTURE']","For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",torch.quantization.fuse_modules.yaml,2
6358,426,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
6359,426,6,"['element', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
6360,426,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
6361,426,6,"['element', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
6362,426,6,"['element', 'tensor']","If `src` is the rank, then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in `tensor_list` of other non-src processes.",torch.distributed.broadcast_multigpu.yaml,2
6363,426,6,"['element', 'tensor']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,2
6364,427,6,"['vector', 'vector', 'product']",The vector for which the Hessian vector product is computed.,torch.autograd.functional.hvp.yaml,3
6365,427,6,"['vector', 'vector', 'product']",The vector for which the vector Jacobian product is computed.,torch.autograd.functional.vjp.yaml,3
6366,427,6,"['vector', 'vector', 'product']",The vector for which the Jacobian vector product is computed.,torch.autograd.functional.jvp.yaml,3
6367,427,6,"['vector', 'vector', 'product']",The vector for which the vector Hessian product is computed.,torch.autograd.functional.vhp.yaml,3
6368,427,6,"['vector', 'vector', 'product']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,3
6369,427,6,"['vector', 'vector', 'product']","The ""vector"" in the Jacobian-vector product.",torch.autograd.grad.yaml,3
6370,428,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
6371,428,6,"['argument', 'optional']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
6372,428,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
6373,428,6,"['argument', 'optional']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
6374,428,6,"['argument', 'optional']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.backward.yaml,2
6375,428,6,"['argument', 'optional']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.grad.yaml,2
6376,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose2d.yaml,4
6377,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv2d.yaml,4
6378,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose3d.yaml,4
6379,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv_transpose1d.yaml,4
6380,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias of shape (out _channels) .,torch.nn.functional.conv1d.yaml,4
6381,429,6,"['optional', 'bias', 'shape', '_channels']",optional bias tensor of shape (out _channels) .,torch.nn.functional.conv3d.yaml,4
6382,430,6,"['optional', 'SOME_STRUCTURE']",optional output tuple.,torch.lu.yaml,2
6383,430,6,"['optional', 'SOME_STRUCTURE']",optional list of compiler flags to forward to the build.,torch.utils.cpp_extension.load.yaml,2
6384,430,6,"['optional', 'SOME_STRUCTURE']",optional list of compiler flags to forward to nvcc when building CUDA sources.,torch.utils.cpp_extension.load.yaml,2
6385,430,6,"['optional', 'SOME_STRUCTURE']",optional list of include directories to forward to the build.,torch.utils.cpp_extension.load.yaml,2
6386,430,6,"['optional', 'SOME_STRUCTURE']",optional list of linker flags to forward to the build.,torch.utils.cpp_extension.load.yaml,2
6387,430,6,"['optional', 'SOME_STRUCTURE']",optional output tuple.,torch.solve.yaml,2
6388,431,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
6389,431,6,"['provided', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
6390,431,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
6391,431,6,"['provided', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
6392,431,6,"['provided', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2
6393,431,6,"['provided', 'tensor']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
6394,432,6,"['n', 'n', 'd']","flow-field of shape (N, H_out, W_out, 2)  (4-D case) or (N, D_out, H_out, W_out, 3)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
6395,432,6,"['n', 'n', 'd']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
6396,432,6,"['n', 'n', 'd']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3
6397,432,6,"['n', 'n', 'd']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3
6398,432,6,"['n', 'n', 'd']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6399,432,6,"['n', 'n', 'd']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6400,433,6,"['n', 'd', 'n']","flow-field of shape (N, H_out, W_out, 2)  (4-D case) or (N, D_out, H_out, W_out, 3)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
6401,433,6,"['n', 'd', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
6402,433,6,"['n', 'd', 'n']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3
6403,433,6,"['n', 'd', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3
6404,433,6,"['n', 'd', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6405,433,6,"['n', 'd', 'n']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6406,434,6,"['n', 'times']",the square matrix of shape (n  times n)  for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2
6407,434,6,"['n', 'times']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
6408,434,6,"['n', 'times']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,2
6409,434,6,"['n', 'times']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
6410,434,6,"['n', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
6411,434,6,"['n', 'times']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
6412,435,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.baddbmm.yaml,3
6413,435,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.baddbmm.yaml,3
6414,435,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.addbmm.yaml,3
6415,435,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.addbmm.yaml,3
6416,435,6,"['batch', 'matrices', 'multiplied']",the first batch of matrices to be multiplied,torch.bmm.yaml,3
6417,435,6,"['batch', 'matrices', 'multiplied']",the second batch of matrices to be multiplied,torch.bmm.yaml,3
6418,436,6,"['matrix', 'size']","The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size",torch.nn.functional.embedding.yaml,2
6419,436,6,"['matrix', 'size']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
6420,436,6,"['matrix', 'size']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
6421,436,6,"['matrix', 'size']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
6422,436,6,"['matrix', 'size']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6423,436,6,"['matrix', 'size']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6424,437,6,"['matrix', 'zero']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,2
6425,437,6,"['matrix', 'zero']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
6426,437,6,"['matrix', 'zero']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,2
6427,437,6,"['matrix', 'zero']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6428,437,6,"['matrix', 'zero']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6429,437,6,"['matrix', 'zero']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
6430,438,6,"['quantized', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3
6431,438,6,"['quantized', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,3
6432,438,6,"['quantized', 'shape', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,3
6433,438,6,"['quantized', 'shape', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3
6434,438,6,"['quantized', 'shape', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,3
6435,438,6,"['quantized', 'shape', '_channels']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,3
6436,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.kl_div.yaml,8
6437,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy.yaml,8
6438,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.nll_loss.yaml,8
6439,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.cross_entropy.yaml,8
6440,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,8
6441,439,6,"['field', 'size_average', 'set', 'false', 'losses', 'instead', 'summed', 'minibatch']","If the field `size_average` is set to `False`, the losses are instead summed for each minibatch.",torch.nn.functional.poisson_nll_loss.yaml,8
6442,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","filters of shape (in _channels ,  out _channels/groups , kH , kW) ",torch.nn.functional.conv_transpose2d.yaml,7
6443,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.functional.conv2d.yaml,7
6444,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","filters of shape (in _channels ,  out _channels/groups , kT , kH , kW) ",torch.nn.functional.conv_transpose3d.yaml,7
6445,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","filters of shape (out _channels ,  in _channels/groups , kT , kH , kW) ",torch.nn.functional.conv3d.yaml,7
6446,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kH , kW) ",torch.nn.quantized.functional.conv2d.yaml,7
6447,440,6,"['filters', 'shape', '_channels', '_channels', 'groups', 'kh', 'kw']","quantized filters of shape (out _channels ,  in _channels/groups , kD , kH , kW) ",torch.nn.quantized.functional.conv3d.yaml,7
6448,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.ctc_loss.yaml,9
6449,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy.yaml,9
6450,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.nll_loss.yaml,9
6451,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.cross_entropy.yaml,9
6452,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,9
6453,441,6,"['specifies', 'reduction', 'apply', 'output', 'none', '|', 'mean', '|', 'sum']",Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`.,torch.nn.functional.poisson_nll_loss.yaml,9
6454,442,6,"['match', 'input']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
6455,442,6,"['match', 'input']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
6456,442,6,"['match', 'input']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy.yaml,2
6457,442,6,"['match', 'input']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2
6458,442,6,"['match', 'input']",a manual rescaling weight if provided it's repeated to match input tensor shape,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
6459,442,6,"['match', 'input']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2
6460,443,6,"['rng', 'state']", CPU RNG state is always forked.,torch.random.fork_rng.yaml,2
6461,443,6,"['rng', 'state']", CPU RNG state is always forked.,torch.random.fork_rng2.yaml,2
6462,443,6,"['rng', 'state']",Omit stashing and restoring the RNG state during each checkpoint.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
6463,443,6,"['rng', 'state']",The device to return the RNG state of.,torch.cuda.get_rng_state.yaml,2
6464,443,6,"['rng', 'state']",The device to set the RNG state.,torch.cuda.set_rng_state.yaml,2
6465,443,6,"['rng', 'state']",Omit stashing and restoring the RNG state during each checkpoint.,torch.utils.checkpoint.checkpoint.yaml,2
6466,444,6,"['diagonal', 'consider']",the diagonal to consider,torch.diag.yaml,2
6467,444,6,"['diagonal', 'consider']",which diagonal to consider.,torch.diag_embed.yaml,2
6468,444,6,"['diagonal', 'consider']",the diagonal to consider,torch.tril.yaml,2
6469,444,6,"['diagonal', 'consider']",the diagonal to consider,torch.triu.yaml,2
6470,444,6,"['diagonal', 'consider']",which diagonal to consider.,torch.diagonal.yaml,2
6471,444,6,"['diagonal', 'consider']",the diagonal to consider.,torch.diagflat.yaml,2
6472,445,6,"['non', 'tensor']"," Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.",torch.onnx.export.yaml,2
6473,445,6,"['non', 'tensor']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
6474,445,6,"['non', 'tensor']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,2
6475,445,6,"['non', 'tensor']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
6476,445,6,"['non', 'tensor']","Non-empty tensors provided must have the same shape, except in the cat dimension.",torch.cat.yaml,2
6477,445,6,"['non', 'tensor']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,2
6478,446,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,2
6479,446,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,2
6480,446,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,2
6481,446,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
6482,446,6,"['represents', 'parameters']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,2
6483,446,6,"['represents', 'parameters']",a single vector represents the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2
6484,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.kl_div.yaml,3
6485,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy.yaml,3
6486,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.nll_loss.yaml,3
6487,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.cross_entropy.yaml,3
6488,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,3
6489,447,6,"['ignored', 'reduce', 'false']",Ignored when reduce is `False`.,torch.nn.functional.poisson_nll_loss.yaml,3
6490,448,5,"['output', 'tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.le.yaml,4
6491,448,5,"['output', 'tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.gt.yaml,4
6492,448,5,"['output', 'tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ne.yaml,4
6493,448,5,"['output', 'tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.ge.yaml,4
6494,448,5,"['output', 'tensor', 'must', 'SOME_DTYPE']",the output tensor that must be a BoolTensor,torch.lt.yaml,4
6495,449,5,"['true', 'otherwise']","output will be in `B x T x *` if True, or in `T x B x *` otherwise",torch.nn.utils.rnn.pad_sequence.yaml,2
6496,449,5,"['true', 'otherwise']","`True` to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed",torch.eig.yaml,2
6497,449,5,"['true', 'otherwise']","When True (nonzero), yield x, otherwise yield y",torch.where.yaml,2
6498,449,5,"['true', 'otherwise']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
6499,449,5,"['true', 'otherwise']","if True, center the input tensor, otherwise, assume that the input is centered.",torch.pca_lowrank.yaml,2
6500,450,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.hvp.yaml,3
6501,450,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vjp.yaml,3
6502,450,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.jvp.yaml,3
6503,450,5,"['true', 'output', 'computed']","If `True`, both the output and result will be computed in a differentiable way.",torch.autograd.functional.vhp.yaml,3
6504,450,5,"['true', 'output', 'computed']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,3
6505,451,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2
6506,451,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2
6507,451,5,"['true', 'use']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2
6508,451,5,"['true', 'use']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2
6509,451,5,"['true', 'use']"," If recompute_scale_factor is ``True` or not specified, a new scale_factor will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed output_size were passed-in explicitly).",torch.nn.functional.interpolate.yaml,2
6510,452,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation",torch.nn.functional.avg_pool3d.yaml,2
6511,452,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.quantized.functional.avg_pool2d.yaml,2
6512,452,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool1d.yaml,2
6513,452,5,"['true', 'include']","If True, includes CUDA-specific include paths.",torch.utils.cpp_extension.include_paths.yaml,2
6514,452,5,"['true', 'include']","when True, will include the zero-padding in the averaging calculation.",torch.nn.functional.avg_pool2d.yaml,2
6515,453,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.hann_window.yaml,2
6516,453,5,"['true', 'returns']","if set to `True`, returns an info IntTensor.",torch.lu.yaml,2
6517,453,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.hamming_window.yaml,2
6518,453,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.blackman_window.yaml,2
6519,453,5,"['true', 'returns']","If True, returns a window to be used as periodic function.",torch.bartlett_window.yaml,2
6520,454,5,"['elements', 'SOME_STRUCTURE']","If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor.",torch.lu.yaml,2
6521,454,5,"['elements', 'SOME_STRUCTURE']","If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor.",torch.lu.yaml,2
6522,454,5,"['elements', 'SOME_STRUCTURE']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6523,454,5,"['elements', 'SOME_STRUCTURE']","m-elements tuple, where  m/2  <=  input dimensions and m  is even.",torch.nn.functional.pad.yaml,2
6524,454,5,"['elements', 'SOME_STRUCTURE']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6525,455,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.ones_like.yaml,5
6526,455,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.empty_like.yaml,5
6527,455,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.zeros_like.yaml,5
6528,455,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.randn_like.yaml,5
6529,455,5,"['default', 'none', 'defaults', 'dtype', 'input']","Default: if `None`, defaults to the dtype of `input`.",torch.rand_like.yaml,5
6530,456,5,"['default', 'SOME_DTYPE', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng.yaml,3
6531,456,5,"['default', 'SOME_DTYPE', 'SOME_DTYPE']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng2.yaml,3
6532,456,5,"['default', 'SOME_DTYPE', 'SOME_DTYPE']","Returns the default `Stream` for the current device, given by `current_device()`, if `device` is `None` (default).",torch.cuda.default_stream.yaml,3
6533,456,5,"['default', 'SOME_DTYPE', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
6534,456,5,"['default', 'SOME_DTYPE', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
6535,457,5,"['default', 'torch', 'preserve_format']",Default: `torch.preserve_format`.,torch.ones_like.yaml,3
6536,457,5,"['default', 'torch', 'preserve_format']",Default: `torch.preserve_format`.,torch.empty_like.yaml,3
6537,457,5,"['default', 'torch', 'preserve_format']",Default: `torch.preserve_format`.,torch.zeros_like.yaml,3
6538,457,5,"['default', 'torch', 'preserve_format']",Default: `torch.preserve_format`.,torch.randn_like.yaml,3
6539,457,5,"['default', 'torch', 'preserve_format']",Default: `torch.preserve_format`.,torch.rand_like.yaml,3
6540,458,5,"['default', 'torch', 'SOME_DTYPE']",Default: `torch.int64`.,torch.randperm.yaml,3
6541,458,5,"['default', 'torch', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.triu_indices.yaml,3
6542,458,5,"['default', 'torch', 'SOME_DTYPE']","Default: if `None`, `torch.long`.",torch.tril_indices.yaml,3
6543,458,5,"['default', 'torch', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.get_rng_state.yaml,3
6544,458,5,"['default', 'torch', 'SOME_DTYPE']","Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).",torch.cuda.set_rng_state.yaml,3
6545,459,5,"['SOME_STRUCTURE', 'given']","If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",torch.utils.cpp_extension.load_inline.yaml,2
6546,459,5,"['SOME_STRUCTURE', 'given']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
6547,459,5,"['SOME_STRUCTURE', 'given']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
6548,459,5,"['SOME_STRUCTURE', 'given']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
6549,459,5,"['SOME_STRUCTURE', 'given']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
6550,460,5,"['SOME_STRUCTURE', 'torch']",a function or a dict specifying how to remap storage locations (see torch.load),torch.utils.model_zoo.load_url.yaml,2
6551,460,5,"['SOME_STRUCTURE', 'torch']","tuple of Q and R tensors satisfying `input = torch.matmul(Q, R)`.",torch.qr.yaml,2
6552,460,5,"['SOME_STRUCTURE', 'torch']","For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules",torch.quantization.fuse_modules.yaml,2
6553,460,5,"['SOME_STRUCTURE', 'torch']",a function or a dict specifying how to remap storage locations (see torch.load),torch.hub.load_state_dict_from_url.yaml,2
6554,460,5,"['SOME_STRUCTURE', 'torch']","a list, tuple, or `torch.Size` of integers defining the shape of the output tensor.",torch.full.yaml,2
6555,461,5,"['dimension', 'default']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz.yaml,2
6556,461,5,"['dimension', 'default']","dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`",torch.nn.utils.spectral_norm.yaml,2
6557,461,5,"['dimension', 'default']","The dimension along which to integrate.By default, use the last dimension.",torch.trapz2.yaml,2
6558,461,5,"['dimension', 'default']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2
6559,461,5,"['dimension', 'default']",Number of array items in summary at beginning and end of each dimension (default = 3).,torch.set_printoptions.yaml,2
6560,462,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
6561,462,5,"['dimension', 'input']",dimension on which to split the input.,torch.nn.functional.glu.yaml,2
6562,462,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
6563,462,5,"['dimension', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
6564,462,5,"['dimension', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2
6565,463,5,"['dimension', 'take']",the dimension to take the cross-product in.,torch.cross.yaml,2
6566,463,5,"['dimension', 'take']",first dimension with respect to which to take diagonal.,torch.diag_embed.yaml,2
6567,463,5,"['dimension', 'take']",second dimension with respect to which to take diagonal.,torch.diag_embed.yaml,2
6568,463,5,"['dimension', 'take']",first dimension with respect to which to take diagonal.,torch.diagonal.yaml,2
6569,463,5,"['dimension', 'take']",second dimension with respect to which to take diagonal.,torch.diagonal.yaml,2
6570,464,5,"['input', 'tensor', 'values']",the input tensor of probability values for the Bernoulli distribution,torch.bernoulli.yaml,3
6571,464,5,"['input', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3
6572,464,5,"['input', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
6573,464,5,"['input', 'tensor', 'values']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3
6574,464,5,"['input', 'tensor', 'values']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
6575,465,5,"['input', 'tensor', 'size', 'm', 'n']","The input tensor of size (*, m, n)  where *  is zero or more batch dimensions",torch.pinverse.yaml,5
6576,465,5,"['input', 'tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,5
6577,465,5,"['input', 'tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,5
6578,465,5,"['input', 'tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,5
6579,465,5,"['input', 'tensor', 'size', 'm', 'n']","the input tensor of size (*, m, n) ",torch.pca_lowrank.yaml,5
6580,466,5,"['input', 'batch', 'matrices']","the input tensor A  of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices.",torch.cholesky.yaml,3
6581,466,5,"['input', 'batch', 'matrices']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3
6582,466,5,"['input', 'batch', 'matrices']","the input tensor of size (*, n, n)  where * is zero or more batch dimensions consisting of symmetric matrices.",torch.symeig.yaml,3
6583,466,5,"['input', 'batch', 'matrices']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
6584,466,5,"['input', 'batch', 'matrices']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
6585,467,5,"['input', 'b']","if `True`, the input is expected in `B x T x *` format.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2
6586,467,5,"['input', 'b']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
6587,467,5,"['input', 'b']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
6588,467,5,"['input', 'b']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
6589,467,5,"['input', 'b']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6590,468,5,"['input', 'matrix', 'size', 'm', 'zero', 'batch', 'dimensions']","the input triangular coefficient matrix of size (*, m, m)  where *  is zero or more batch dimensions",torch.triangular_solve.yaml,7
6591,468,5,"['input', 'matrix', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,7
6592,468,5,"['input', 'matrix', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,7
6593,468,5,"['input', 'matrix', 'size', 'm', 'zero', 'batch', 'dimensions']","input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.",torch.solve.yaml,7
6594,468,5,"['input', 'matrix', 'size', 'm', 'zero', 'batch', 'dimensions']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,7
6595,469,5,"['tensor', 'input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,3
6596,469,5,"['tensor', 'input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,3
6597,469,5,"['tensor', 'input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,3
6598,469,5,"['tensor', 'input', 'tensor']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,3
6599,469,5,"['tensor', 'input', 'tensor']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,3
6600,470,5,"['tensor', 'used']",It should contain correctly-sized tensors to be used for output of the collective.,torch.distributed.all_gather.yaml,2
6601,470,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
6602,470,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
6603,470,5,"['tensor', 'used']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
6604,470,5,"['tensor', 'used']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2
6605,471,5,"['tensor', 'data']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
6606,471,5,"['tensor', 'data']",Default is feps ** 0.5 where feps is smallest non-zero floating-point number of the given input tensor A data type.,torch.lobpcg.yaml,2
6607,471,5,"['tensor', 'data']",Tensor to fill with received data.,torch.distributed.recv.yaml,2
6608,471,5,"['tensor', 'data']",Tensor to fill with received data.,torch.distributed.irecv.yaml,2
6609,471,5,"['tensor', 'data']","Data to be sent if `src` is the rank of current process, and tensor to be used to save received data otherwise.",torch.distributed.broadcast.yaml,2
6610,472,5,"['tensor', 'compute']",the tensor to compute OR with,torch.logical_or.yaml,2
6611,472,5,"['tensor', 'compute']",the tensor to compute AND with,torch.logical_and.yaml,2
6612,472,5,"['tensor', 'compute']",the tensor to compute the multivariate log-gamma function,torch.mvlgamma.yaml,2
6613,472,5,"['tensor', 'compute']",the tensor to compute XOR with,torch.logical_xor.yaml,2
6614,472,5,"['tensor', 'compute']",the tensor to compute the digamma function on,torch.digamma.yaml,2
6615,473,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool3d.yaml,3
6616,473,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool1d.yaml,3
6617,473,5,"['whether', 'return', 'indices']",whether to return pooling indices.,torch.nn.functional.adaptive_max_pool2d.yaml,3
6618,473,5,"['whether', 'return', 'indices']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,3
6619,473,5,"['whether', 'return', 'indices']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,3
6620,474,5,"['whether', 'upper', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,3
6621,474,5,"['whether', 'upper', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,3
6622,474,5,"['whether', 'upper', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,3
6623,474,5,"['whether', 'upper', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,3
6624,474,5,"['whether', 'upper', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,3
6625,475,5,"['whether', 'lower', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,3
6626,475,5,"['whether', 'lower', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,3
6627,475,5,"['whether', 'lower', 'triangular']",whether to consider the Cholesky factor as a lower or upper triangular matrix.,torch.cholesky_solve.yaml,3
6628,475,5,"['whether', 'lower', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,3
6629,475,5,"['whether', 'lower', 'triangular']",whether to return a lower (default) or upper triangular matrix,torch.cholesky_inverse.yaml,3
6630,476,5,"['whether', 'input']","controls whether `input` was halfed to avoid redundancy, e.g., by `rfft()`.",torch.irfft.yaml,2
6631,476,5,"['whether', 'input']",whether to pad `input` on both sides so that the t -th frame is centered at time t  times hop _length .,torch.stft.yaml,2
6632,476,5,"['whether', 'input']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6633,476,5,"['whether', 'input']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6634,476,5,"['whether', 'input']",indicates whether `input` is symmetric.,torch.matrix_rank.yaml,2
6635,477,5,"['whether', 'elements']",controls whether to return largest or smallest elements,torch.topk.yaml,2
6636,477,5,"['whether', 'elements']",controls whether to return the elements in sorted order,torch.topk.yaml,2
6637,477,5,"['whether', 'elements']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6638,477,5,"['whether', 'elements']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2
6639,477,5,"['whether', 'elements']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6640,478,5,"['whether', 'unique']",Whether to also return the counts for each unique element.,torch.unique.yaml,2
6641,478,5,"['whether', 'unique']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6642,478,5,"['whether', 'unique']",Whether to sort the unique elements in ascending order before returning as output.,torch.unique.yaml,2
6643,478,5,"['whether', 'unique']",Whether to also return the counts for each unique element.,torch.unique_consecutive.yaml,2
6644,478,5,"['whether', 'unique']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6645,479,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape",torch.nn.functional.avg_pool3d.yaml,2
6646,479,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.quantized.functional.avg_pool2d.yaml,2
6647,479,5,"['use', 'instead']","when True, will use ceil instead of floor to compute the output shape.",torch.nn.functional.avg_pool1d.yaml,2
6648,479,5,"['use', 'instead']",use operator_export_type] export the internal IR directly instead of converting it to ONNX ops.,torch.onnx.export.yaml,2
6649,479,5,"['use', 'instead']","when True, will use ceil instead of floor in the formula to compute the output shape.",torch.nn.functional.avg_pool2d.yaml,2
6650,480,5,"['target', 'output', 'size', 'single', 'SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_max_pool3d.yaml,7
6651,480,5,"['target', 'output', 'size', 'single', 'SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or triple-integer tuple),torch.nn.functional.adaptive_avg_pool3d.yaml,7
6652,480,5,"['target', 'output', 'size', 'single', 'SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.quantized.functional.adaptive_avg_pool2d.yaml,7
6653,480,5,"['target', 'output', 'size', 'single', 'SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_max_pool2d.yaml,7
6654,480,5,"['target', 'output', 'size', 'single', 'SOME_DTYPE', 'SOME_DTYPE', 'SOME_STRUCTURE']",the target output size (single integer or double-integer tuple),torch.nn.functional.adaptive_avg_pool2d.yaml,7
6655,481,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.ones_like.yaml,6
6656,481,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.empty_like.yaml,6
6657,481,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.zeros_like.yaml,6
6658,481,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.randn_like.yaml,6
6659,481,5,"['size', 'input', 'determine', 'size', 'output', 'tensor']",the size of `input` will determine size of the output tensor.,torch.rand_like.yaml,6
6660,482,5,"['SOME_DTYPE', 'SOME_STRUCTURE', 'SOME_DTYPE']","A string, or list of strings, containing C++ source code.",torch.utils.cpp_extension.load_inline.yaml,3
6661,482,5,"['SOME_DTYPE', 'SOME_STRUCTURE', 'SOME_DTYPE']","A string, or list of strings, containing CUDA source code.",torch.utils.cpp_extension.load_inline.yaml,3
6662,482,5,"['SOME_DTYPE', 'SOME_STRUCTURE', 'SOME_DTYPE']","If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated.",torch.norm.yaml,3
6663,482,5,"['SOME_DTYPE', 'SOME_STRUCTURE', 'SOME_DTYPE']",Function that takes in a list of modules and outputs a list of fused modules of the same length.,torch.quantization.fuse_modules.yaml,3
6664,482,5,"['SOME_DTYPE', 'SOME_STRUCTURE', 'SOME_DTYPE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,3
6665,483,5,"['SOME_DTYPE', 'used']",module used for pickling metadata and objects,torch.save.yaml,2
6666,483,5,"['SOME_DTYPE', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.topk.yaml,2
6667,483,5,"['SOME_DTYPE', 'used']","the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers",torch.sort.yaml,2
6668,483,5,"['SOME_DTYPE', 'used']",module used for unpickling metadata and objects (has to match the `pickle_module` used to serialize file),torch.load.yaml,2
6669,483,5,"['SOME_DTYPE', 'used']","the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers",torch.kthvalue.yaml,2
6670,484,5,"['SOME_DTYPE', 'function']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng.yaml,2
6671,484,5,"['SOME_DTYPE', 'function']","The `nn.Module`, function, or class type to compile.",torch.jit.script.yaml,2
6672,484,5,"['SOME_DTYPE', 'function']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng2.yaml,2
6673,484,5,"['SOME_DTYPE', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2
6674,484,5,"['SOME_DTYPE', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2
6675,485,5,"['SOME_DTYPE', 'run']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng.yaml,2
6676,485,5,"['SOME_DTYPE', 'run']"," By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case.",torch.random.fork_rng2.yaml,2
6677,485,5,"['SOME_DTYPE', 'run']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2
6678,485,5,"['SOME_DTYPE', 'run']","When a module is passed to `torch.jit.trace`, only the `forward` method is run and traced (see `torch.jit.trace` for details).",torch.jit.trace.yaml,2
6679,485,5,"['SOME_DTYPE', 'run']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
6680,486,5,"['SOME_DTYPE', 'place', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,3
6681,486,5,"['SOME_DTYPE', 'place', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,3
6682,486,5,"['SOME_DTYPE', 'place', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,3
6683,486,5,"['SOME_DTYPE', 'place', 'SOME_DTYPE']","bool specifying if fusion happens in place on the model, by default a new model is returned",torch.quantization.fuse_modules.yaml,3
6684,486,5,"['SOME_DTYPE', 'place', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,3
6685,487,5,"['SOME_DTYPE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,5
6686,487,5,"['SOME_DTYPE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,5
6687,487,5,"['SOME_DTYPE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,5
6688,487,5,"['SOME_DTYPE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,5
6689,487,5,"['SOME_DTYPE', 'represent', 'fraction', 'parameters', 'prune']","If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,5
6690,488,5,"['SOME_DTYPE', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.l1_unstructured.yaml,6
6691,488,5,"['SOME_DTYPE', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_unstructured.yaml,6
6692,488,5,"['SOME_DTYPE', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.ln_structured.yaml,6
6693,488,5,"['SOME_DTYPE', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.global_unstructured.yaml,6
6694,488,5,"['SOME_DTYPE', 'represents', 'absolute', 'number', 'parameters', 'prune']","If `int`, it represents the absolute number of parameters to prune.",torch.nn.utils.prune.random_structured.yaml,6
6695,489,5,"['SOME_DTYPE', 'supported']",Only `torch.strided` (dense layout) is supported.,torch.hann_window.yaml,2
6696,489,5,"['SOME_DTYPE', 'supported']",(Note: passing keyword arguments to the model is not currently supported.,torch.onnx.export.yaml,2
6697,489,5,"['SOME_DTYPE', 'supported']",Only `torch.strided` (dense layout) is supported.,torch.hamming_window.yaml,2
6698,489,5,"['SOME_DTYPE', 'supported']",Only `torch.strided` (dense layout) is supported.,torch.blackman_window.yaml,2
6699,489,5,"['SOME_DTYPE', 'supported']",Only `torch.strided` (dense layout) is supported.,torch.bartlett_window.yaml,2
6700,490,5,"['false', 'input']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,2
6701,490,5,"['false', 'input']","If `False`, the input will get sorted unconditionally.",torch.nn.utils.rnn.pack_padded_sequence.yaml,2
6702,490,5,"['false', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6703,490,5,"['false', 'input']","if `True` the loss is computed as  exp(input) - target * input , if `False` then loss is input - target *  log(input+eps) .",torch.nn.functional.poisson_nll_loss.yaml,2
6704,490,5,"['false', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6705,491,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_xor.yaml,3
6706,491,5,"['first', 'input', 'tensor']",the first input tensor,torch.add.yaml,3
6707,491,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_and.yaml,3
6708,491,5,"['first', 'input', 'tensor']",the first input tensor,torch.atan2.yaml,3
6709,491,5,"['first', 'input', 'tensor']",the first input tensor,torch.bitwise_or.yaml,3
6710,492,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy.yaml,13
6711,492,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.nll_loss.yaml,13
6712,492,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.cross_entropy.yaml,13
6713,492,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,13
6714,492,5,"['none', 'reduction', 'applied', 'mean', 'sum', 'output', 'divided', 'number', 'elements', 'output', 'sum', 'output', 'summed']","`'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed.",torch.nn.functional.poisson_nll_loss.yaml,13
6715,493,5,"['none', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2
6716,493,5,"['none', 'value']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2
6717,493,5,"['none', 'value']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.backward.yaml,2
6718,493,5,"['none', 'value']","If a None value would be acceptable for all grad_tensors, then this argument is optional.",torch.autograd.grad.yaml,2
6719,493,5,"['none', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2
6720,494,5,"['none', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
6721,494,5,"['none', 'specified']","If None (default) is specified, the value is defined by _Formatter",torch.set_printoptions.yaml,2
6722,494,5,"['none', 'specified']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.backward.yaml,2
6723,494,5,"['none', 'specified']",None values can be specified for scalar Tensors or ones that don't require grad.,torch.autograd.grad.yaml,2
6724,494,5,"['none', 'specified']","List of tensors to scatter (default is None, must be specified on the source rank)",torch.distributed.scatter.yaml,2
6725,495,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.baddbmm.yaml,3
6726,495,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addr.yaml,3
6727,495,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addbmm.yaml,3
6728,495,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addmm.yaml,3
6729,495,5,"['multiplier', 'input', 'beta']",multiplier for `input` ( beta ),torch.addmv.yaml,3
6730,496,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.ones_like.yaml,5
6731,496,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.empty_like.yaml,5
6732,496,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.zeros_like.yaml,5
6733,496,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.randn_like.yaml,5
6734,496,5,"['desired', 'memory', 'format', 'returned', 'tensor']",the desired memory format of returned Tensor.,torch.rand_like.yaml,5
6735,497,5,"['desired', 'seed']",The desired seed.,torch.cuda.manual_seed.yaml,2
6736,497,5,"['desired', 'seed']",The desired seed.,torch.manual_seed.yaml,2
6737,497,5,"['desired', 'seed']",The desired seed.,torch.random.manual_seed2.yaml,2
6738,497,5,"['desired', 'seed']",The desired seed.,torch.random.manual_seed.yaml,2
6739,497,5,"['desired', 'seed']",The desired seed.,torch.cuda.manual_seed_all.yaml,2
6740,498,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,3
6741,498,5,"['data', 'type', 'input']","If dtype is not given, infer the data type from the other input arguments.",torch.arange.yaml,3
6742,498,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,3
6743,498,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,3
6744,498,5,"['data', 'type', 'input']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,3
6745,499,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
6746,499,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
6747,499,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
6748,499,5,"['data', 'specified']","List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)",torch.distributed.gather.yaml,2
6749,499,5,"['data', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
6750,500,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod2.yaml,2
6751,500,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum2.yaml,2
6752,500,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.sum.yaml,2
6753,500,5,"['type', 'specified']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
6754,500,5,"['type', 'specified']","the desired data type of returned tensor.If specified, the input tensor is casted to `dtype` before the operationis performed.",torch.prod.yaml,2
6755,501,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.log_softmax.yaml,7
6756,501,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumprod.yaml,7
6757,501,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmin.yaml,7
6758,501,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.cumsum.yaml,7
6759,501,5,"['specified', 'input', 'tensor', 'casted', 'dtype', 'operation', 'performed']","If specified, the input tensor is casted to `dtype` before the operation is performed.",torch.nn.functional.softmax.yaml,7
6760,502,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,2
6761,502,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,2
6762,502,5,"['specified', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,2
6763,502,5,"['specified', 'used']","When specified, it is used as initial approximation of eigenvectors.",torch.lobpcg.yaml,2
6764,502,5,"['specified', 'used']","When specified, it will be used as preconditioner.",torch.lobpcg.yaml,2
6765,503,5,"['number', 'element']",the number to be divided to each element of `input`,torch.div.yaml,2
6766,503,5,"['number', 'element']","Can be a single number or a one-element tuple (dW,).",torch.nn.functional.conv1d.yaml,2
6767,503,5,"['number', 'element']","Can be a single number or a one-element tuple (padW,).",torch.nn.functional.conv1d.yaml,2
6768,503,5,"['number', 'element']","Can be a single number or a one-element tuple (sW,).",torch.nn.functional.conv1d.yaml,2
6769,503,5,"['number', 'element']",The number of repetitions for each element.repeats is broadcasted to fit the shape of the given axis.,torch.repeat_interleave.yaml,2
6770,504,5,"['number', 'classes']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,2
6771,504,5,"['number', 'classes']",Total number of classes.,torch.nn.functional.one_hot.yaml,2
6772,504,5,"['number', 'classes']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
6773,504,5,"['number', 'classes']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,2
6774,504,5,"['number', 'classes']",Must be a vector with length equal to the number of classes.,torch.nn.functional.binary_cross_entropy_with_logits.yaml,2
6775,505,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,2
6776,505,5,"['values', 'indices']",values selected at indices where `condition` is `True`,torch.where.yaml,2
6777,505,5,"['values', 'indices']",values selected at indices where `condition` is `False`,torch.where.yaml,2
6778,505,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,2
6779,505,5,"['values', 'indices']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,2
6780,506,5,"['torch', 'quint']","Has to be one of the quantized dtypes: `torch.quint8`, `torch.qint8`, `torch.qint32`",torch.quantize_per_channel.yaml,2
6781,506,5,"['torch', 'quint']",Quantized input of type torch.quint8,torch.nn.quantized.functional.linear.yaml,2
6782,506,5,"['torch', 'quint']","Has to be one of the quantized dtypes: `torch.quint8`, `torch.qint8`, `torch.qint32`",torch.quantize_per_tensor.yaml,2
6783,506,5,"['torch', 'quint']",Default: `torch.quint8`,torch.nn.quantized.functional.conv2d.yaml,2
6784,506,5,"['torch', 'quint']",Default: `torch.quint8`,torch.nn.quantized.functional.conv3d.yaml,2
6785,507,5,"['set', 'input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,3
6786,507,5,"['set', 'input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,3
6787,507,5,"['set', 'input', 'tensor']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,3
6788,507,5,"['set', 'input', 'tensor']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,3
6789,507,5,"['set', 'input', 'tensor']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,3
6790,508,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.",torch.nn.functional.grid_sample.yaml,4
6791,508,5,"['set', 'corner', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.quantized.functional.interpolate.yaml,4
6792,508,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,4
6793,508,5,"['set', 'corner', 'corner', 'pixels']","If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels.",torch.nn.functional.interpolate.yaml,4
6794,508,5,"['set', 'corner', 'corner', 'pixels']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,4
6795,509,5,"['set', 'value']","If set to `None` (default), this value is automatically determined based on whether `cuda_sources` is provided.",torch.utils.cpp_extension.load_inline.yaml,2
6796,509,5,"['set', 'value']","If set to `None` (default), this value is automatically determined based on the existence of `.cu` or `.cuh` in `sources`.",torch.utils.cpp_extension.load.yaml,2
6797,509,5,"['set', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6798,509,5,"['set', 'value']","If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.",torch.nn.functional.one_hot.yaml,2
6799,509,5,"['set', 'value']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6800,510,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout.yaml,2
6801,510,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout2d.yaml,2
6802,510,5,"['set', 'operation']","If set to `True`, will do this operation in-place.",torch.nn.functional.dropout3d.yaml,2
6803,510,5,"['set', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6804,510,5,"['set', 'operation']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6805,511,5,"['shape', 'n']","flow-field of shape (N, H_out, W_out, 2)  (4-D case) or (N, D_out, H_out, W_out, 3)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
6806,511,5,"['shape', 'n']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,2
6807,511,5,"['shape', 'n']",the square matrix of shape (n  times n)  for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2
6808,511,5,"['shape', 'n']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
6809,511,5,"['shape', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
6810,512,5,"['shape', 'times']",the square matrix of shape (n  times n)  for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,2
6811,512,5,"['shape', 'times']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
6812,512,5,"['shape', 'times']",input tensor of shape B  times P  times M .,torch.cdist.yaml,2
6813,512,5,"['shape', 'times']",input tensor of shape B  times R  times M .,torch.cdist.yaml,2
6814,512,5,"['shape', 'times']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,2
6815,513,5,"['result', 'SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummax.yaml,5
6816,513,5,"['result', 'SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.cummin.yaml,5
6817,513,5,"['result', 'SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.median2.yaml,5
6818,513,5,"['result', 'SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (max, max_indices)",torch.max2.yaml,5
6819,513,5,"['result', 'SOME_STRUCTURE', 'two', 'output', 'tensor']","the result tuple of two output tensors (values, indices)",torch.mode.yaml,5
6820,514,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy.yaml,12
6821,514,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.nll_loss.yaml,12
6822,514,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.cross_entropy.yaml,12
6823,514,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.binary_cross_entropy_with_logits.yaml,12
6824,514,5,"['note', 'size_average', 'reduce', 'process', 'deprecated', 'meantime', 'specifying', 'either', 'two', 'args', 'override', 'reduction']","Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`.",torch.nn.functional.poisson_nll_loss.yaml,12
6825,515,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,8
6826,515,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,8
6827,515,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,8
6828,515,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,8
6829,515,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'tensor', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,8
6830,516,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,8
6831,516,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,8
6832,516,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,8
6833,516,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,8
6834,516,5,"['python', 'function', 'takes', 'tensor', 'inputs', 'returns', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,8
6835,517,5,"['python', 'function', 'takes', 'tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.vjp.yaml,6
6836,517,5,"['python', 'function', 'takes', 'tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradgradcheck.yaml,6
6837,517,5,"['python', 'function', 'takes', 'tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jvp.yaml,6
6838,517,5,"['python', 'function', 'takes', 'tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.,torch.autograd.functional.jacobian.yaml,6
6839,517,5,"['python', 'function', 'takes', 'tensor', 'SOME_STRUCTURE', 'tensor']",a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors,torch.autograd.gradcheck.yaml,6
6840,518,5,"['function', 'function']",A list of function names for which to generate function bindings.,torch.utils.cpp_extension.load_inline.yaml,2
6841,518,5,"['function', 'function']","If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).",torch.utils.cpp_extension.load_inline.yaml,2
6842,518,5,"['function', 'function']","To do this, each function `foo` is called via an intermediary `_safe_foo` function.",torch.utils.cpp_extension.load_inline.yaml,2
6843,518,5,"['function', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize.yaml,2
6844,518,5,"['function', 'function']","a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop",torch.quantization.quantize_qat.yaml,2
6845,519,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.hvp.yaml,2
6846,519,5,"['func', 'tensor']",This argument is optional when `func`'s output contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vjp.yaml,2
6847,519,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.jvp.yaml,2
6848,519,5,"['func', 'tensor']",This argument is optional when `func`'s input contains a single element and (if it is not provided) will be set as a Tensor containing a single `1`.,torch.autograd.functional.vhp.yaml,2
6849,519,5,"['func', 'tensor']",arguments and returns to `func` must be tensors or (possibly nested) tuples that contain tensors.,torch.jit.trace.yaml,2
6850,520,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.hvp.yaml,2
6851,520,5,"['must', 'size']",Must be the same size as the output of `func`.,torch.autograd.functional.vjp.yaml,2
6852,520,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.jvp.yaml,2
6853,520,5,"['must', 'size']",Must be the same size as the input of `func`.,torch.autograd.functional.vhp.yaml,2
6854,520,5,"['must', 'size']","If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value",torch.roll.yaml,2
6855,521,5,"['containing', 'SOME_DTYPE']",containing module,torch.nn.utils.spectral_norm.yaml,2
6856,521,5,"['containing', 'SOME_DTYPE']",containing module,torch.nn.utils.remove_weight_norm.yaml,2
6857,521,5,"['containing', 'SOME_DTYPE']",containing module,torch.nn.utils.weight_norm.yaml,2
6858,521,5,"['containing', 'SOME_DTYPE']",Model containing the modules to be fused,torch.quantization.fuse_modules.yaml,2
6859,521,5,"['containing', 'SOME_DTYPE']",containing module,torch.nn.utils.remove_spectral_norm.yaml,2
6860,522,5,"['zero', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
6861,522,5,"['zero', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
6862,522,5,"['zero', 'dimension']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
6863,522,5,"['zero', 'dimension']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
6864,522,5,"['zero', 'dimension']","The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values.",torch.sparse_coo_tensor.yaml,2
6865,523,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose2d.yaml,2
6866,523,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose3d.yaml,2
6867,523,5,"['padding', 'input']",`dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input.,torch.nn.functional.conv_transpose1d.yaml,2
6868,523,5,"['padding', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6869,523,5,"['padding', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6870,524,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool3d.yaml,2
6871,524,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.quantized.functional.avg_pool2d.yaml,2
6872,524,5,"['used', 'used']","This option parallels the `align_corners` option in `interpolate()`, and so whichever option is used here should also be used there to resize the input image before grid sampling.",torch.nn.functional.grid_sample.yaml,2
6873,524,5,"['used', 'used']","if specified, it will be used as divisor, otherwise size of the pooling region will be used.",torch.nn.functional.avg_pool2d.yaml,2
6874,524,5,"['used', 'used']",module used for unpickling metadata and objects (has to match the `pickle_module` used to serialize file),torch.load.yaml,2
6875,525,5,"['n', 'n', 'k']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,3
6876,525,5,"['n', 'n', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6877,525,5,"['n', 'n', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6878,525,5,"['n', 'n', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6879,525,5,"['n', 'n', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6880,526,5,"['n', 'c', 'c']","input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)",torch.nn.functional.grid_sample.yaml,3
6881,526,5,"['n', 'c', 'c']","(T, N, C)  where C = number of characters in alphabet including blank, T = input length, and N = batch size.",torch.nn.functional.ctc_loss.yaml,3
6882,526,5,"['n', 'c', 'c']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3
6883,526,5,"['n', 'c', 'c']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6884,526,5,"['n', 'c', 'c']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6885,527,5,"['n', 'times', 'n']",the square matrix of shape (n  times n)  for which the eigenvalues and eigenvectors will be computed,torch.eig.yaml,3
6886,527,5,"['n', 'times', 'n']","(N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))",torch.nn.functional.affine_grid.yaml,3
6887,527,5,"['n', 'times', 'n']",input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D,torch.nn.functional.affine_grid.yaml,3
6888,527,5,"['n', 'times', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,3
6889,527,5,"['n', 'times', 'n']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,3
6890,528,5,"['n', 'm']",input tensor of shape N  times M .,torch.nn.functional.pdist.yaml,2
6891,528,5,"['n', 'm']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
6892,528,5,"['n', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of matrices of dimension m  times n .",torch.qr.yaml,2
6893,528,5,"['n', 'm']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
6894,528,5,"['n', 'm']","the input tensor of size (*, m, n)  where * is zero or more batch dimensions consisting of m  times n  matrices.",torch.svd.yaml,2
6895,529,5,"['n', 'k', 'k']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,3
6896,529,5,"['n', 'k', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6897,529,5,"['n', 'k', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.nll_loss.yaml,3
6898,529,5,"['n', 'k', 'k']","(N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6899,529,5,"['n', 'k', 'k']","(N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.",torch.nn.functional.cross_entropy.yaml,3
6900,530,5,"['upper', 'lower', 'triangular']",flag that indicates whether to return a upper or lower triangular matrix.,torch.cholesky.yaml,3
6901,530,5,"['upper', 'lower', 'triangular']",whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations.,torch.triangular_solve.yaml,3
6902,530,5,"['upper', 'lower', 'triangular']","input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor",torch.cholesky_solve.yaml,3
6903,530,5,"['upper', 'lower', 'triangular']",controls whether to consider upper-triangular or lower-triangular region,torch.symeig.yaml,3
6904,530,5,"['upper', 'lower', 'triangular']","the input 2-D tensor u , a upper or lower triangular Cholesky factor",torch.cholesky_inverse.yaml,3
6905,531,5,"['quantized', 'input']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,2
6906,531,5,"['quantized', 'input']",Quantized input of type torch.quint8,torch.nn.quantized.functional.linear.yaml,2
6907,531,5,"['quantized', 'input']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,2
6908,531,5,"['quantized', 'input']",quantized input,torch.nn.quantized.functional.relu.yaml,2
6909,531,5,"['quantized', 'input']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,2
6910,532,5,"['quantized', 'tensor', '_channels']","quantized input tensor (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.avg_pool2d.yaml,3
6911,532,5,"['quantized', 'tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv2d.yaml,3
6912,532,5,"['quantized', 'tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iH , iW) ",torch.nn.quantized.functional.conv2d.yaml,3
6913,532,5,"['quantized', 'tensor', '_channels']",non-quantized bias tensor of shape (out _channels) .,torch.nn.quantized.functional.conv3d.yaml,3
6914,532,5,"['quantized', 'tensor', '_channels']","quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) ",torch.nn.quantized.functional.conv3d.yaml,3
6915,533,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input  as squares rather than points.",torch.nn.functional.grid_sample.yaml,2
6916,533,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.quantized.functional.interpolate.yaml,2
6917,533,5,"['pixels', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.quantized.functional.interpolate.yaml,2
6918,533,5,"['pixels', 'input']","Geometrically, we consider the pixels of the input and output as squares rather than points.",torch.nn.functional.interpolate.yaml,2
6919,533,5,"['pixels', 'input']","If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same.",torch.nn.functional.interpolate.yaml,2
6920,534,5,"['also', 'SOME_STRUCTURE']",`example_inputs` may also be a single Tensor in which case it is automatically wrapped in a tuple.,torch.jit.trace.yaml,2
6921,534,5,"['also', 'SOME_STRUCTURE']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique.yaml,2
6922,534,5,"['also', 'SOME_STRUCTURE']",Can also be a list of strings if there is only a single list of modules to fuse.,torch.quantization.fuse_modules.yaml,2
6923,534,5,"['also', 'SOME_STRUCTURE']",Whether to also return the indices for where elements in the original input ended up in the returned unique list.,torch.unique_consecutive.yaml,2
6924,534,5,"['also', 'SOME_STRUCTURE']",It should also know how to handle the inputs passed as the tuple.,torch.utils.checkpoint.checkpoint.yaml,2
6925,535,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
6926,535,5,"['one', 'dimension']",There should be one index letter per tensor dimension.,torch.einsum.yaml,2
6927,535,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
6928,535,5,"['one', 'dimension']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
6929,535,5,"['one', 'dimension']","If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension.",torch.norm.yaml,2
6930,536,5,"['one', 'torch']",One of the values from `torch.distributed.ReduceOp` enum.,torch.distributed.all_reduce.yaml,2
6931,536,5,"['one', 'torch']","Has to be one of the quantized dtypes: `torch.quint8`, `torch.qint8`, `torch.qint32`",torch.quantize_per_channel.yaml,2
6932,536,5,"['one', 'torch']",One of the values from `torch.distributed.ReduceOp` enum.,torch.distributed.reduce.yaml,2
6933,536,5,"['one', 'torch']","Has to be one of the quantized dtypes: `torch.quint8`, `torch.qint8`, `torch.qint32`",torch.quantize_per_tensor.yaml,2
6934,536,5,"['one', 'torch']",One of the values from `torch.distributed.ReduceOp` enum.,torch.distributed.reduce_multigpu.yaml,2
6935,537,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose2d.yaml,2
6936,537,5,"['side', 'output']",The right hand side follows after -> and gives the indices for the output.,torch.einsum.yaml,2
6937,537,5,"['side', 'output']","If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.",torch.einsum.yaml,2
6938,537,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose3d.yaml,2
6939,537,5,"['side', 'output']",additional size added to one side of each dimension in the output shape.,torch.nn.functional.conv_transpose1d.yaml,2
6940,538,5,"['file', 'like', 'object', 'implement', 'SOME_DTYPE', 'containing', 'file', 'name']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,8
6941,538,5,"['file', 'like', 'object', 'implement', 'SOME_DTYPE', 'containing', 'file', 'name']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,8
6942,538,5,"['file', 'like', 'object', 'implement', 'SOME_DTYPE', 'containing', 'file', 'name']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,8
6943,538,5,"['file', 'like', 'object', 'implement', 'SOME_DTYPE', 'containing', 'file', 'name']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,8
6944,538,5,"['file', 'like', 'object', 'implement', 'SOME_DTYPE', 'containing', 'file', 'name']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,8
6945,539,5,"['file', 'SOME_DTYPE', 'containing', 'file', 'name']",a file-like object (has to implement write and flush) or a string containing a file name,torch.save.yaml,5
6946,539,5,"['file', 'SOME_DTYPE', 'containing', 'file', 'name']",a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.,torch.onnx.export.yaml,5
6947,539,5,"['file', 'SOME_DTYPE', 'containing', 'file', 'name']",A file-like object (has to implement write and flush) or a string containing a file name.,torch.jit.save.yaml,5
6948,539,5,"['file', 'SOME_DTYPE', 'containing', 'file', 'name']","a file-like object (has to implement `read()`, :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name",torch.load.yaml,5
6949,539,5,"['file', 'SOME_DTYPE', 'containing', 'file', 'name']","a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name",torch.jit.load.yaml,5
6950,540,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diag_embed.yaml,2
6951,540,5,"['main', 'diagonal']",diagonal offset from the main diagonal.,torch.triu_indices.yaml,2
6952,540,5,"['main', 'diagonal']",diagonal offset from the main diagonal.,torch.tril_indices.yaml,2
6953,540,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diagonal.yaml,2
6954,540,5,"['main', 'diagonal']",Default: 0 (main diagonal).,torch.diagflat.yaml,2
6955,541,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.hann_window.yaml,2
6956,541,5,"['window', 'function']",the optional window function.,torch.stft.yaml,2
6957,541,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.hamming_window.yaml,2
6958,541,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.blackman_window.yaml,2
6959,541,5,"['window', 'function']","If True, returns a window to be used as periodic function.",torch.bartlett_window.yaml,2
6960,542,5,"['match', 'size']","float 1D tensor of scales to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
6961,542,5,"['match', 'size']","integer 1D tensor of offset to use, size should match `input.size(axis)`",torch.quantize_per_channel.yaml,2
6962,542,5,"['match', 'size']",It should match `devices` in length and sum to `tensor.size(dim)`.,torch.cuda.comm.scatter.yaml,2
6963,542,5,"['match', 'size']",Has to match input size if it is a tuple.,torch.nn.quantized.functional.interpolate.yaml,2
6964,542,5,"['match', 'size']",Has to match input size if it is a tuple.,torch.nn.functional.interpolate.yaml,2
6965,543,5,"['nn', 'SOME_DTYPE']","The `nn.Module`, function, or class type to compile.",torch.jit.script.yaml,2
6966,543,5,"['nn', 'SOME_DTYPE']",a dictionary that maps from nn module to nnq module,torch.quantization.swap_module.yaml,2
6967,543,5,"['nn', 'SOME_DTYPE']","module must be of type `nn.Module`, and name must be a string.",torch.nn.utils.prune.global_unstructured.yaml,2
6968,543,5,"['nn', 'SOME_DTYPE']",A Python function or `torch.nn.Module` that will be run with `example_inputs`.,torch.jit.trace.yaml,2
6969,543,5,"['nn', 'SOME_DTYPE']",A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.,torch.utils.checkpoint.checkpoint_sequential.yaml,2
6970,544,5,"['hand', 'side']","The left hand side lists the operands dimensions, separated by commas.",torch.einsum.yaml,2
6971,544,5,"['hand', 'side']",The right hand side follows after -> and gives the indices for the output.,torch.einsum.yaml,2
6972,544,5,"['hand', 'side']","If the -> and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side.",torch.einsum.yaml,2
6973,544,5,"['hand', 'side']","If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.",torch.einsum.yaml,2
6974,544,5,"['hand', 'side']",the Right-hand-side input tensor,torch.dist.yaml,2
6975,545,5,"['right', 'hand']",The right hand side follows after -> and gives the indices for the output.,torch.einsum.yaml,2
6976,545,5,"['right', 'hand']","If the -> and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side.",torch.einsum.yaml,2
6977,545,5,"['right', 'hand']","If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output.",torch.einsum.yaml,2
6978,545,5,"['right', 'hand']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
6979,545,5,"['right', 'hand']",the Right-hand-side input tensor,torch.dist.yaml,2
6980,546,5,"['m', 'k']","multiple right-hand sides of size (*, m, k)  where *  is zero of more batch dimensions (b )",torch.triangular_solve.yaml,2
6981,546,5,"['m', 'k']","the input tensor of size (*, m, n)  where k <= n <= m.",torch.lobpcg.yaml,2
6982,546,5,"['m', 'k']","input matrix b  of size (*, m, k) , where *  is zero or more batch dimensions",torch.cholesky_solve.yaml,2
6983,546,5,"['m', 'k']","input matrix B  of size (*, m, k)  , where *  is zero or more batch dimensions.",torch.solve.yaml,2
6984,546,5,"['m', 'k']","The dimensions of Q and R are (*, m, k)  and (*, k, n)  respectively, where k =  min(m, n)  if `some:` is `True` and k = m  otherwise.",torch.qr.yaml,2
6985,547,5,"['p', 'norm']","p value for the p-norm distance to calculate between each vector pair  in [0,  infty] .",torch.nn.functional.pdist.yaml,2
6986,547,5,"['p', 'norm']",type of the used p-norm.,torch.nn.utils.clip_grad_norm_.yaml,2
6987,547,5,"['p', 'norm']","p value for the p-norm distance to calculate between each vector pair  in [0,  infty] .",torch.cdist.yaml,2
6988,547,5,"['p', 'norm']",The p of the p-norm to compute for the `max_norm` option.,torch.nn.functional.embedding.yaml,2
6989,547,5,"['p', 'norm']",See documentation of valid entries for argument `p` in `torch.norm()`.,torch.nn.utils.prune.ln_structured.yaml,2
6990,548,5,"['original', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare_qat.yaml,2
6991,548,5,"['original', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.quantize.yaml,2
6992,548,5,"['original', 'SOME_DTYPE']",correspondence between original module types and quantized counterparts,torch.quantization.quantize.yaml,2
6993,548,5,"['original', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.convert.yaml,2
6994,548,5,"['original', 'SOME_DTYPE']","carry out model transformations in-place, the original module is mutated",torch.quantization.prepare.yaml,2
6995,549,5,"['maps', 'SOME_DTYPE']",dictionary that maps float modules to quantized modules to be replaced.,torch.quantization.prepare_qat.yaml,2
6996,549,5,"['maps', 'SOME_DTYPE']",a dictionary that maps from nn module to nnq module,torch.quantization.swap_module.yaml,2
6997,549,5,"['maps', 'SOME_DTYPE']","a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules",torch.quantization.convert.yaml,2
6998,549,5,"['maps', 'SOME_DTYPE']","dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)",torch.quantization.propagate_qconfig_.yaml,2
6999,549,5,"['maps', 'SOME_DTYPE']",offset in integer value that maps to float zero,torch.quantize_per_tensor.yaml,2
7000,550,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,2
7001,550,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,2
7002,550,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,2
7003,550,5,"['quantity', 'parameters']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,2
7004,550,5,"['quantity', 'parameters']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,2
7005,551,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.l1_unstructured.yaml,3
7006,551,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_unstructured.yaml,3
7007,551,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.ln_structured.yaml,3
7008,551,5,"['quantity', 'parameters', 'prune']",other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters.,torch.nn.utils.prune.global_unstructured.yaml,3
7009,551,5,"['quantity', 'parameters', 'prune']",quantity of parameters to prune.,torch.nn.utils.prune.random_structured.yaml,3
7010,552,5,"['parameters', 'SOME_DTYPE']","parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune.",torch.nn.utils.prune.global_unstructured.yaml,2
7011,552,5,"['parameters', 'SOME_DTYPE']","In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by `model.state_dict().values()`",torch.onnx.export.yaml,2
7012,552,5,"['parameters', 'SOME_DTYPE']",an iterator of Tensors that are the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2
7013,552,5,"['parameters', 'SOME_DTYPE']",a single vector represents the parameters of a model.,torch.nn.utils.vector_to_parameters.yaml,2
7014,552,5,"['parameters', 'SOME_DTYPE']",an iterator of Tensors that are the parameters of a model.,torch.nn.utils.parameters_to_vector.yaml,2
7015,553,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.irfft.yaml,5
7016,553,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.fft.yaml,5
7017,553,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.ifft.yaml,5
7018,553,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return the normalized STFT results Default: `False`,torch.stft.yaml,5
7019,553,5,"['controls', 'whether', 'return', 'normalized', 'results']",controls whether to return normalized results.,torch.rfft.yaml,5
7020,554,5,"['w', 'r']","If `True`, gradient w.r.t. `weight` will be a sparse tensor.",torch.nn.functional.embedding.yaml,2
7021,554,5,"['w', 'r']","The ""vector"" in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors.",torch.autograd.backward.yaml,2
7022,554,5,"['w', 'r']",Usually gradients w.r.t. each output.,torch.autograd.grad.yaml,2
7023,554,5,"['w', 'r']",Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).,torch.autograd.grad.yaml,2
7024,554,5,"['w', 'r']","If `True`, gradient w.r.t. `input` will be a sparse tensor.",torch.gather.yaml,2
7025,555,5,"['flattened', 'input']","If `None`, the argmin of the flattened input is returned.",torch.argmin2.yaml,2
7026,555,5,"['flattened', 'input']","The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.",torch.repeat_interleave.yaml,2
7027,555,5,"['flattened', 'input']","If `None`, the argmax of the flattened input is returned.",torch.argmax2.yaml,2
7028,555,5,"['flattened', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique.yaml,2
7029,555,5,"['flattened', 'input']","If `None`, the unique of the flattened input is returned.",torch.unique_consecutive.yaml,2
