constraints:
  arg_params:
    descp: Dictionary of name to NDArray.
    doc_dtype: dict
    normalized_descp:
    - D_STRUCTURE of name to D_STRUCTURE
    normalized_docdtype: ONE_WORD D_STRUCTURE
  aux_params:
    descp: Dictionary of name to NDArray.
    doc_dtype: dict
    normalized_descp:
    - D_STRUCTURE of name to D_STRUCTURE
    normalized_docdtype: ONE_WORD D_STRUCTURE
  calib_data:
    default: None
    descp: A data iterator initialized by the calibration dataset.
    doc_dtype: DataIter
    normalized_default: DEFAULT None
    normalized_descp:
    - A data iterator initialized by the calibration dataset
    normalized_docdtype: ONE_WORD DataIter
  calib_mode:
    default: entropy
    descp: If calib_mode='none', no calibration will be used and the thresholds for
      requantization after the corresponding layers will be calculated at runtime
      by calling min and max operators. The quantized models generated in this mode
      are normally 10-20% slower than those with calibrations during inference. If
      calib_mode='naive', the min and max values of the layer outputs from a calibration
      dataset will be directly taken as the thresholds for quantization. If calib_mode='entropy'
      (default mode), the thresholds for quantization will be derived such that the
      KL divergence between the distributions of FP32 layer outputs and quantized
      layer outputs is minimized based upon the calibration dataset.
    doc_dtype: str
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - If calib_mode QSTR , no calibration will be used and the thresholds for requantization
      after the corresponding layers will be calculated at runtime by calling min
      and max operators
    - The quantized models generated in this mode are normally CONSTANT_NUM CONSTANT_NUM
      slower than those with calibrations during inference
    - If calib_mode QSTR , the min and max values of the layer outputs from a calibration
      dataset will be directly taken as the thresholds for quantization
    - If calib_mode QSTR BSTR, the thresholds for quantization will be derived such
      that the KL divergence between the distributions of D_TYPE layer outputs and
      quantized layer outputs is minimized based upon the calibration dataset
    normalized_docdtype: ONE_WORD D_TYPE
  ctx:
    default: cpu(0)
    descp: Defines the device that users want to run forward propagation on the calibration
      dataset for collecting layer output statistics. Currently, only supports single
      context.
    doc_dtype: Context
    normalized_default: cpu CONSTANT_NUM
    normalized_descp:
    - Defines the device that users want to run forward propagation on the calibration
      dataset for collecting layer output statistics
    - Currently, only supports single context
    normalized_docdtype: ONE_WORD Context
  data_names:
    default: (data,)
    descp: Data names required for creating a Module object to run forward propagation
      on the calibration dataset.
    doc_dtype: a list of strs
    normalized_default: DEFAULT BSTR
    normalized_descp:
    - Data names required for creating a Module object to run forward propagation
      on the calibration dataset
    normalized_docdtype: a D_STRUCTURE of strs
  excluded_op_names:
    default: None
    descp: A list of strings representing the names of the operators that users want
      to excluding from being quantized.
    doc_dtype: list of strings
    normalized_default: DEFAULT None
    normalized_descp:
    - A D_STRUCTURE of D_TYPE representing the names of the operators that users want
      to excluding from being quantized
    normalized_docdtype: D_STRUCTURE of D_TYPE
  excluded_sym_names:
    default: None
    descp: A list of strings representing the names of the symbols that users want
      to excluding from being quantized.
    doc_dtype: list of strings
    normalized_default: DEFAULT None
    normalized_descp:
    - A D_STRUCTURE of D_TYPE representing the names of the symbols that users want
      to excluding from being quantized
    normalized_docdtype: D_STRUCTURE of D_TYPE
  label_names:
    default: (softmax_label,)
    descp: Label names required for creating a Module object to run forward propagation
      on the calibration dataset.
    doc_dtype: a list of strs
    normalized_default: DEFAULT BSTR
    normalized_descp:
    - Label names required for creating a Module object to run forward propagation
      on the calibration dataset
    normalized_docdtype: a D_STRUCTURE of strs
  logger:
    default: <moduleloggingfrom/work/conda_env/lib/python3.8/logging/__init__.py>
    descp: A logging object for printing information during the process of quantization.
    doc_dtype: Object
    normalized_default: REXPR work conda_env lib python3 CONSTANT_NUM logging init
      py
    normalized_descp:
    - A logging object for printing information during the process of quantization
    normalized_docdtype: ONE_WORD Object
  num_calib_examples:
    default: None
    descp: The maximum number of examples that user would like to use for calibration.
      If not provided, the whole calibration dataset will be used.
    doc_dtype: int or None
    normalized_default: DEFAULT None
    normalized_descp:
    - The maximum number of examples that user would like to use for calibration
    - If not provided, the whole calibration dataset will be used
    normalized_docdtype: D_TYPE or None
  quantize_mode:
    default: smart
    descp: The mode that quantization pass to apply. Support 'full' and 'smart'. 'full'
      means quantize all operator if possible. 'smart' means quantization pass will
      smartly choice which operator should be quantized.
    doc_dtype: str
    normalized_default: DEFAULT DF_STR
    normalized_descp:
    - The mode that quantization pass to apply
    - Support QSTR
    - QSTR means quantize all operator if possible
    - QSTR means quantization pass will smartly choice which operator should be quantized
    normalized_docdtype: ONE_WORD D_TYPE
  quantized_dtype:
    default: int8
    descp: The quantized destination type for input data. Currently support 'int8',
      'uint8' and 'auto'. 'auto' means automatically select output type according
      to calibration result. Default value is 'int8'.
    doc_dtype: str
    normalized_default: DEFAULT D_TYPE
    normalized_descp:
    - The quantized destination type for input data
    - Currently support QSTR
    - QSTR means automatically select output type according to calibration result
    - Default value is QSTR
    normalized_docdtype: ONE_WORD D_TYPE
  sym:
    descp: Defines the structure of a neural network for FP32 data types.
    doc_dtype: str or Symbol
    normalized_descp:
    - Defines the structure of a neural network for D_TYPE data types
    normalized_docdtype: D_TYPE or Symbol
inputs:
  optional:
  - data_names
  - label_names
  - ctx
  - excluded_sym_names
  - excluded_op_names
  - calib_mode
  - calib_data
  - num_calib_examples
  - quantized_dtype
  - quantize_mode
  - logger
  required:
  - sym
  - arg_params
  - aux_params
link: https://mxnet.apache.org/versions/1.6/api/python/docs/api/contrib/quantization/index.html#mxnet.contrib.quantization.quantize_model
package: mxnet
target: quantize_model
title: mxnet.contrib.quantization.quantize_model
version: 1.6.0
