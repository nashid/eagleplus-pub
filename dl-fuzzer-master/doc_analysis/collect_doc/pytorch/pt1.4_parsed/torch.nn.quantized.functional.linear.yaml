constraints:
  bias:
    default: None
    descp: None or fp32 bias of type torch.float
    doc_dtype:
    - Tensor
  input:
    descp: Quantized input of type torch.quint8
    doc_dtype:
    - Tensor
  scale:
    default: None
    descp: output scale. If None, derived from the input scale
    doc_dtype:
    - double
  weight:
    descp: Quantized weight of type torch.qint8
    doc_dtype:
    - Tensor
  zero_point:
    default: None
    descp: output zero point. If None, derived from the input zero_point
    doc_dtype:
    - python:long
inputs:
  optional:
  - bias
  - scale
  - zero_point
  required:
  - input
  - weight
link: https://pytorch.org/docs/1.4.0/quantization.html#torch.nn.quantized.functional.linear
package: torch
target: linear
title: torch.nn.quantized.functional.linear
version: 1.4.0
