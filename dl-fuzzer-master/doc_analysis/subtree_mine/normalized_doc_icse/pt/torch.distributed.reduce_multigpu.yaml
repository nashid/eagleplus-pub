constraints:
  async_op:
    default: 'False'
    descp: Whether this op should be an async op
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - Whether this PARAM should be an async PARAM
    normalized_docdtype: D_TYPE optional
  dst:
    descp: Destination rank
    doc_dtype: int
    normalized_descp:
    - Destination rank
    normalized_docdtype: ONE_WORD D_TYPE
  dst_tensor:
    default: '0'
    descp: Destination tensor rank within `tensor_list`
    doc_dtype: int, optional
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - Destination D_STRUCTURE rank within PARAM
    normalized_docdtype: D_TYPE optional
  group:
    default: <objectobject>
    descp: The process group to work on
    doc_dtype: ProcessGroup, optional
    normalized_default: DEFAULT REXPR
    normalized_descp:
    - The process group to work on
    normalized_docdtype: ProcessGroup optional
  op:
    default: ReduceOp.SUM
    descp: One of the values from `torch.distributed.ReduceOp` enum.  Specifies an
      operation used for element-wise reductions.
    doc_dtype: optional
    normalized_default: ReduceOp SUM
    normalized_descp:
    - One of the values from torch distributed ReduceOp enum
    - Specifies an operation used for element wise reductions
    normalized_docdtype: ONE_WORD optional
  tensor_list:
    descp: Input and output GPU tensors of the collective. The function operates in-place.
      You also need to make sure that `len(tensor_list)` is the same for all the distributed
      processes calling this function.
    doc_dtype: List[Tensor]
    normalized_descp:
    - Input and output GPU D_STRUCTURE of the collective
    - The function operates in place
    - You also need to make sure that len BSTR is the same for all the distributed
      processes calling this function
    normalized_docdtype: D_STRUCTURE BSTR
inputs:
  optional:
  - op
  - group
  - async_op
  - dst_tensor
  required:
  - tensor_list
  - dst
link: https://pytorch.org/docs/stable/distributed.html#torch.distributed.reduce_multigpu
package: torch
target: reduce_multigpu
title: torch.distributed.reduce_multigpu
version: 1.5.0
