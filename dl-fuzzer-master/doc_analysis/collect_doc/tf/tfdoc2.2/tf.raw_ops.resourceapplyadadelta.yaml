constraints:
  accum:
    descp: A Tensor of type resource. Should be from a Variable().
  accum_update:
    descp: A Tensor of type resource. Should be from a Variable().
  epsilon:
    descp: A Tensor. Must have the same type as lr. Constant factor. Must be a scalar.
  grad:
    descp: A Tensor. Must have the same type as lr. The gradient.
  lr:
    descp: 'A Tensor. Must be one of the following types: float32, float64, int32,
      uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16, uint16,
      complex128, half, uint32, uint64. Scaling factor. Must be a scalar.'
  name:
    default: None
    descp: A name for the operation (optional).
  rho:
    descp: A Tensor. Must have the same type as lr. Decay factor. Must be a scalar.
  use_locking:
    default: 'False'
    descp: An optional bool. Defaults to False. If True, updating of the var, accum
      and update_accum tensors will be protected by a lock; otherwise the behavior
      is undefined, but may exhibit less contention.
  var:
    descp: A Tensor of type resource. Should be from a Variable().
inputs:
  optional:
  - use_locking
  - name
  required:
  - var
  - accum
  - accum_update
  - lr
  - rho
  - epsilon
  - grad
link: https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/raw_ops/ResourceApplyAdadelta
outputs:
- The created Operation.
package: tensorflow
target: ResourceApplyAdadelta
title: tf.raw_ops.ResourceApplyAdadelta
version: 2.2.0
