constraints:
  async_op:
    default: 'False'
    descp: Whether this op should be an async op
    doc_dtype: bool, optional
    normalized_default: DEFAULT CONSTANT_BOOL
    normalized_descp:
    - Whether this op should be an async op
    normalized_docdtype: D_TYPE, optional
  group:
    default: <objectobject>
    descp: The process group to work on
    doc_dtype: ProcessGroup, optional
    normalized_default: DEFAULT REXPR
    normalized_descp:
    - The process group to work on
    normalized_docdtype: ProcessGroup, optional
  src:
    descp: Source rank.
    doc_dtype: int
    normalized_descp:
    - Source rank
    normalized_docdtype: ONE_WORD D_TYPE
  src_tensor:
    default: '0'
    descp: Source tensor rank within `tensor_list`
    doc_dtype: int, optional
    normalized_default: DEFAULT CONSTANT_NUM
    normalized_descp:
    - Source D_STRUCTURE rank within PARAM
    normalized_docdtype: D_TYPE, optional
  tensor_list:
    descp: Tensors that participate in the collective operation. If `src` is the rank,
      then the specified `src_tensor` element of `tensor_list` (`tensor_list[src_tensor]`)
      will be broadcast to all other tensors (on different GPUs) in the src process
      and all tensors in `tensor_list` of other non-src processes. You also need to
      make sure that `len(tensor_list)` is the same for all the distributed processes
      calling this function.
    doc_dtype: List[Tensor]
    normalized_descp:
    - D_STRUCTURE that participate in the collective operation
    - If PARAM is the rank, then the specified PARAM element of QSTR tensor_list BSTR
      will be broadcast to all other D_STRUCTURE BSTR in the PARAM process and all
      D_STRUCTURE in QSTR of other non PARAM processes
    - You also need to make sure that len BSTR is the same for all the distributed
      processes calling this function
    normalized_docdtype: D_STRUCTURE BSTR
inputs:
  optional:
  - group
  - async_op
  - src_tensor
  required:
  - tensor_list
  - src
link: https://pytorch.org/docs/stable/distributed.html#torch.distributed.broadcast_multigpu
package: torch
target: broadcast_multigpu
title: torch.distributed.broadcast_multigpu
version: 1.5.0
