constraints:
  '**kwargs':
    descp: ''
    normalized_descp: []
  '*args':
    descp: ''
    normalized_descp: []
  batch_first:
    default: None
    descp: 'If `True`, then the input and output tensors are provided as (batch, seq,
      feature). Default: `False`'
    normalized_default: DEFAULT None
    normalized_descp:
    - If CONSTANT_BOOL, then the input and output D_STRUCTURE are provided as BSTR
    - Default CONSTANT_BOOL
  bias:
    default: None
    descp: 'If `False`, then the layer does not use bias weights b_ih and b_hh. Default:
      `True`'
    normalized_default: DEFAULT None
    normalized_descp:
    - If CONSTANT_BOOL, then the layer does not use bias weights b_ih and b_hh
    - Default CONSTANT_BOOL
  bidirectional:
    default: None
    descp: 'If `True`, becomes a bidirectional GRU. Default: `False`'
    normalized_default: DEFAULT None
    normalized_descp:
    - If CONSTANT_BOOL, becomes a bidirectional GRU
    - Default CONSTANT_BOOL
  dropout:
    default: None
    descp: 'If non-zero, introduces a Dropout layer on the outputs of each GRU layer
      except the last layer, with dropout probability equal to `dropout`. Default:
      0'
    normalized_default: DEFAULT None
    normalized_descp:
    - If non zero, introduces a Dropout layer on the outputs of each GRU layer except
      the last layer, with dropout probability equal to QSTR
    - Default CONSTANT_NUM
  hidden_size:
    default: None
    descp: The number of features in the hidden state h
    normalized_default: DEFAULT None
    normalized_descp:
    - The number of features in the hidden state h
  input_size:
    default: None
    descp: The number of expected features in the input x
    normalized_default: DEFAULT None
    normalized_descp:
    - The number of expected features in the input x
  num_layers:
    default: None
    descp: 'Number of recurrent layers. E.g., setting `num_layers=2` would mean stacking
      two GRUs together to form a stacked GRU, with the second GRU taking in outputs
      of the first GRU and computing the final results. Default: 1'
    normalized_default: DEFAULT None
    normalized_descp:
    - Number of recurrent layers
    - E g , setting num_layers CONSTANT_NUM would mean stacking two GRUs together
      to form a stacked GRU, with the second GRU taking in outputs of the first GRU
      and computing the final results
    - Default CONSTANT_NUM
inputs:
  optional:
  - input_size
  - hidden_size
  - num_layers
  - bias
  - batch_first
  - dropout
  - bidirectional
  required:
  - '*args'
  - '**kwargs'
link: https://pytorch.org/docs/stable/nn.html#torch.nn.GRU
package: torch
target: GRU
title: torch.nn.GRU
version: 1.5.0
