constraints:
  beta1:
    descp: A Tensor. Must have the same type as var. Momentum factor. Must be a scalar.
  beta1_power:
    descp: A Tensor. Must have the same type as var. Must be a scalar.
  beta2:
    descp: A Tensor. Must have the same type as var. Momentum factor. Must be a scalar.
  beta2_power:
    descp: A Tensor. Must have the same type as var. Must be a scalar.
  epsilon:
    descp: A Tensor. Must have the same type as var. Ridge term. Must be a scalar.
  grad:
    descp: A Tensor. Must have the same type as var. The gradient.
  lr:
    descp: A Tensor. Must have the same type as var. Scaling factor. Must be a scalar.
  m:
    descp: A mutable Tensor. Must have the same type as var. Should be from a Variable().
  name:
    default: None
    descp: A name for the operation (optional).
  use_locking:
    default: 'False'
    descp: An optional bool. Defaults to False. If True, updating of the var, m, and
      v tensors will be protected by a lock; otherwise the behavior is undefined,
      but may exhibit less contention.
  use_nesterov:
    default: 'False'
    descp: An optional bool. Defaults to False. If True, uses the nesterov update.
  v:
    descp: A mutable Tensor. Must have the same type as var. Should be from a Variable().
  var:
    descp: 'A mutable Tensor. Must be one of the following types: float32, float64,
      int32, uint8, int16, int8, complex64, int64, qint8, quint8, qint32, bfloat16,
      uint16, complex128, half, uint32, uint64. Should be from a Variable().'
inputs:
  optional:
  - use_locking
  - use_nesterov
  - name
  required:
  - var
  - m
  - v
  - beta1_power
  - beta2_power
  - lr
  - beta1
  - beta2
  - epsilon
  - grad
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/raw_ops/ApplyAdam
outputs:
- A mutable Tensor. Has the same type as var.
package: tensorflow
target: ApplyAdam
title: tf.raw_ops.ApplyAdam
version: 2.3.0
