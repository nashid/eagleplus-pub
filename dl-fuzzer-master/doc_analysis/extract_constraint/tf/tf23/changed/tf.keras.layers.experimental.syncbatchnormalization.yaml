constraints:
  '**kwargs':
    default: null
    descp: ''
  adjustment:
    default: None
    descp: ''
  axis:
    default: '-1'
    descp: Integer, the axis that should be normalized (typically the features axis).
      For instance, after a Conv2D layer with data_format="channels_first", set axis=1
      in BatchNormalization.
    dtype:
    - int
    ndim:
    - '0'
  beta_constraint:
    default: None
    descp: Optional constraint for the beta weight.
  beta_initializer:
    default: zeros
    descp: Initializer for the beta weight.
    dtype:
    - tf.string
  beta_regularizer:
    default: None
    descp: Optional regularizer for the beta weight.
  center:
    default: 'True'
    descp: If True, add offset of beta to normalized tensor. If False, beta is ignored.
    dtype:
    - tf.bool
    ndim:
    - '0'
  epsilon:
    default: '0.001'
    descp: Small float added to variance to avoid dividing by zero.
    dtype:
    - float
    ndim:
    - '0'
  gamma_constraint:
    default: None
    descp: Optional constraint for the gamma weight.
  gamma_initializer:
    default: ones
    descp: Initializer for the gamma weight.
    dtype:
    - tf.string
  gamma_regularizer:
    default: None
    descp: Optional regularizer for the gamma weight.
  momentum:
    default: '0.99'
    descp: Momentum for the moving average.
    dtype:
    - float
    ndim:
    - '0'
  moving_mean_initializer:
    default: zeros
    descp: Initializer for the moving mean.
    dtype:
    - tf.string
  moving_variance_initializer:
    default: ones
    descp: Initializer for the moving variance.
    dtype:
    - tf.string
  name:
    default: None
    descp: ''
    dtype:
    - tf.string
    ndim:
    - '0'
  renorm:
    default: 'False'
    descp: Whether to use Batch Renormalization. This adds extra variables during
      training. The inference is the same for either value of this parameter.
    dtype:
    - tf.bool
    ndim:
    - '0'
  renorm_clipping:
    default: None
    descp: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to scalar Tensors
      used to clip the renorm correction. The correction (r, d) is used as corrected_value
      = normalized_value * r + d, with r clipped to [rmin, rmax], and d to [-dmax,
      dmax]. Missing rmax, rmin, dmax are set to inf, 0, inf, respectively.
    dtype:
    - numeric
    ndim:
    - '0'
    structure:
    - dict
    tensor_t:
    - tf.tensor
  renorm_momentum:
    default: '0.99'
    descp: Momentum used to update the moving means and standard deviations with renorm.
      Unlike momentum, this affects training and should be neither too small (which
      would add noise) nor too large (which would give stale estimates). Note that
      momentum is still applied to get the means and variances for inference.
    dtype:
    - float
    ndim:
    - '0'
  scale:
    default: 'True'
    descp: If True, multiply by gamma. If False, gamma is not used. When the next
      layer is linear (also e.g. nn.relu), this can be disabled since the scaling
      will be done by the next layer.
    dtype:
    - tf.bool
    ndim:
    - '0'
  trainable:
    default: 'True'
    descp: Boolean, if True the variables will be marked as trainable.
    dtype:
    - tf.bool
    ndim:
    - '0'
inputs:
  optional:
  - axis
  - momentum
  - epsilon
  - center
  - scale
  - beta_initializer
  - gamma_initializer
  - moving_mean_initializer
  - moving_variance_initializer
  - beta_regularizer
  - gamma_regularizer
  - beta_constraint
  - gamma_constraint
  - renorm
  - renorm_clipping
  - renorm_momentum
  - trainable
  - adjustment
  - name
  - '**kwargs'
  required: []
link: https://www.tensorflow.org/versions/r2.3/api_docs/python/tf/keras/layers/experimental/SyncBatchNormalization
package: tensorflow
target: SyncBatchNormalization
title: tf.keras.layers.experimental.SyncBatchNormalization
version: 2.3.0
